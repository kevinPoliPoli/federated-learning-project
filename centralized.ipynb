{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinPoliPoli/federated-learning-project/blob/main/centralized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC-7OoJPC-BY"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## <strong>Dependencies and imports<strong/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b505e30-d78f-49cb-ec72-3552136e2c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.10/dist-packages (9.0.0.post1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install 'torch'\n",
        "!pip3 install 'torchvision'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "\n",
        "!pip3 install torchinfo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from statistics import mean \n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV1bmcwNDCq8"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## <strong>Hyper-parameters and transformations<strong/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 100    # 101: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.1                # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 0.0001   # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 160   # Total number of training epochs (iterations over dataset)\n",
        "\n",
        "STEP_SIZE = 20      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 5\n",
        "\n",
        "AUG_PROB = 0.5   # the probability with witch each image is transformed at training time during each epoch\n",
        "AUG_TYPE = None      # define the type  of augmentation pipeline \n",
        "                     # None for no data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing and Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "outputs": [],
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([ transforms.ToTensor()   # Remember this when applying different transformations, otherwise you get an error\n",
        "])\n",
        "\n",
        "#normalizeTest = transforms.Normalize((0.5089, 0.4874, 0.4419), (0.2683, 0.2574, 0.2771))\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([ transforms.ToTensor(),   # Remember this when applying different transformations, otherwise you get an error\n",
        "#normalizeTest\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHGdmJjKOpjR"
      },
      "source": [
        "**FData augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IfY0flp3riZL"
      },
      "outputs": [],
      "source": [
        "rc_t = transforms.RandomCrop(32, padding=4)\n",
        "hflip_t = transforms.RandomHorizontalFlip(p = 1)\n",
        "normalizer = transforms.Normalize(mean = (0.5071, 0.4867, 0.4408), std= (0.2675, 0.2565, 0.2761))\n",
        "\n",
        "aug_transformation = transforms.Compose([hflip_t, rc_t])\n",
        "\n",
        "aug_pipeline = transforms.Compose([   transforms.ToPILImage(),\n",
        "                                      transforms.RandomApply([aug_transformation], p = AUG_PROB),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalizer\n",
        "                                  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b423dc-fb4c-406c-c0ed-f93603bfdc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "-----------------------------------------\n",
            "CIFAR100 TRAIN Dataset: 50000\n",
            "CIFAR100 TEST Dataset: 10000\n",
            "-----------------------------------------\n",
            "Train Dataset: 45000\n",
            "Valid Dataset: 5000\n",
            "Test Dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "TRAIN_DATASET = CIFAR100(root=\"data\", train=True, download=True, transform=train_transform)\n",
        "TEST_DATASET = CIFAR100(root=\"data\", train=False, download=True, transform=eval_transform)\n",
        "\n",
        "train_set, val_set, _ = random_split(TRAIN_DATASET, [45000, 5000, 50000-len(TRAIN_DATASET)])\n",
        "\n",
        "# Check dataset sizes\n",
        "print('-----------------------------------------')\n",
        "print('CIFAR100 TRAIN Dataset: {}'.format(len(TRAIN_DATASET)))\n",
        "print('CIFAR100 TEST Dataset: {}'.format(len(TEST_DATASET)))\n",
        "print('-----------------------------------------')\n",
        "print('Train Dataset: {}'.format(len(train_set)))\n",
        "print('Valid Dataset: {}'.format(len(val_set)))\n",
        "print('Test Dataset: {}'.format(len(TEST_DATASET)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VriRw8SI1nle",
        "outputId": "01478573-28de-4b02-89f2-3bdf2535782e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "351\n",
            "40\n",
            "79\n"
          ]
        }
      ],
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_dataloader = DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-EP-roAyJNz"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## <strong>ResNet definition<strong/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Res net 20"
      ],
      "metadata": {
        "id": "6SHeuGsVj9Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch.nn as nn\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.gn1 = nn.GroupNorm(2, planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.gn2 = nn.GroupNorm(2, planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.gn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.gn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=100):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(3, 16)\n",
        "        self.gn1 = nn.GroupNorm(2, 16)  \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.GroupNorm(2, planes * block.expansion),  \n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.gn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    layers: List[int],\n",
        ") -> CifarResNet:\n",
        "    model = CifarResNet(BasicBlock, layers)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "bv_iNiRZj8xG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKp81ptPknY"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## <strong>Training<strong/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9sjq00G94tSc",
        "outputId": "c698b49e-c0bb-406e-8b74-41ccf72da13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "CifarResNet                              [128, 100]                --\n",
            "├─Conv2d: 1-1                            [128, 16, 32, 32]         432\n",
            "├─GroupNorm: 1-2                         [128, 16, 32, 32]         32\n",
            "├─ReLU: 1-3                              [128, 16, 32, 32]         --\n",
            "├─Sequential: 1-4                        [128, 16, 32, 32]         --\n",
            "│    └─BasicBlock: 2-1                   [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-1                  [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-2               [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-3                    [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-4                  [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-5               [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-6                    [128, 16, 32, 32]         --\n",
            "│    └─BasicBlock: 2-2                   [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-7                  [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-8               [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-9                    [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-10                 [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-11              [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-12                   [128, 16, 32, 32]         --\n",
            "│    └─BasicBlock: 2-3                   [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-13                 [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-14              [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-15                   [128, 16, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-16                 [128, 16, 32, 32]         2,304\n",
            "│    │    └─GroupNorm: 3-17              [128, 16, 32, 32]         32\n",
            "│    │    └─ReLU: 3-18                   [128, 16, 32, 32]         --\n",
            "├─Sequential: 1-5                        [128, 32, 16, 16]         --\n",
            "│    └─BasicBlock: 2-4                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-19                 [128, 32, 16, 16]         4,608\n",
            "│    │    └─GroupNorm: 3-20              [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-21                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-22                 [128, 32, 16, 16]         9,216\n",
            "│    │    └─GroupNorm: 3-23              [128, 32, 16, 16]         64\n",
            "│    │    └─Sequential: 3-24             [128, 32, 16, 16]         --\n",
            "│    │    │    └─Conv2d: 4-1             [128, 32, 16, 16]         512\n",
            "│    │    │    └─GroupNorm: 4-2          [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-25                   [128, 32, 16, 16]         --\n",
            "│    └─BasicBlock: 2-5                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-26                 [128, 32, 16, 16]         9,216\n",
            "│    │    └─GroupNorm: 3-27              [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-28                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-29                 [128, 32, 16, 16]         9,216\n",
            "│    │    └─GroupNorm: 3-30              [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-31                   [128, 32, 16, 16]         --\n",
            "│    └─BasicBlock: 2-6                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-32                 [128, 32, 16, 16]         9,216\n",
            "│    │    └─GroupNorm: 3-33              [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-34                   [128, 32, 16, 16]         --\n",
            "│    │    └─Conv2d: 3-35                 [128, 32, 16, 16]         9,216\n",
            "│    │    └─GroupNorm: 3-36              [128, 32, 16, 16]         64\n",
            "│    │    └─ReLU: 3-37                   [128, 32, 16, 16]         --\n",
            "├─Sequential: 1-6                        [128, 64, 8, 8]           --\n",
            "│    └─BasicBlock: 2-7                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-38                 [128, 64, 8, 8]           18,432\n",
            "│    │    └─GroupNorm: 3-39              [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-40                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-41                 [128, 64, 8, 8]           36,864\n",
            "│    │    └─GroupNorm: 3-42              [128, 64, 8, 8]           128\n",
            "│    │    └─Sequential: 3-43             [128, 64, 8, 8]           --\n",
            "│    │    │    └─Conv2d: 4-3             [128, 64, 8, 8]           2,048\n",
            "│    │    │    └─GroupNorm: 4-4          [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-44                   [128, 64, 8, 8]           --\n",
            "│    └─BasicBlock: 2-8                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-45                 [128, 64, 8, 8]           36,864\n",
            "│    │    └─GroupNorm: 3-46              [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-47                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-48                 [128, 64, 8, 8]           36,864\n",
            "│    │    └─GroupNorm: 3-49              [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-50                   [128, 64, 8, 8]           --\n",
            "│    └─BasicBlock: 2-9                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-51                 [128, 64, 8, 8]           36,864\n",
            "│    │    └─GroupNorm: 3-52              [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-53                   [128, 64, 8, 8]           --\n",
            "│    │    └─Conv2d: 3-54                 [128, 64, 8, 8]           36,864\n",
            "│    │    └─GroupNorm: 3-55              [128, 64, 8, 8]           128\n",
            "│    │    └─ReLU: 3-56                   [128, 64, 8, 8]           --\n",
            "├─AdaptiveAvgPool2d: 1-7                 [128, 64, 1, 1]           --\n",
            "├─Linear: 1-8                            [128, 100]                6,500\n",
            "==========================================================================================\n",
            "Total params: 278,324\n",
            "Trainable params: 278,324\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 5.23\n",
            "==========================================================================================\n",
            "Input size (MB): 1.57\n",
            "Forward/backward pass size (MB): 411.14\n",
            "Params size (MB): 1.11\n",
            "Estimated Total Size (MB): 413.83\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "#net = ResNet20(100).cuda(); # first implementation\n",
        "net = _resnet([3]*3) # second implementation\n",
        "\n",
        "net_summary = summary(net, input_size=(BATCH_SIZE,3,32,32), depth=4) \n",
        "print(str(net_summary)) #show the model architecture\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "parameters_to_optimize = net.parameters()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "T_max = NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0.00000015)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, STEP_SIZE, eta_min=0.000015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EO3HV5pqJg1o"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(net, dataloader, last_accuracy, update, print_tqdm = True):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "    running_corrects = 0\n",
        "    iterable = tqdm(dataloader) if print_tqdm else dataloader\n",
        "    losses = []\n",
        "    for images, labels in iterable: \n",
        "      norm_images = []\n",
        "      for image in images:\n",
        "        norm_image = normalizer(image)\n",
        "        norm_images.append(norm_image)\n",
        "      norm_images = torch.stack(norm_images)  \n",
        "      norm_images = norm_images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      # Forward Pass\n",
        "      outputs = net(norm_images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "    if update is True:\n",
        "      if accuracy > last_accuracy:\n",
        "        torch.save(net.state_dict(), f\"{net.__class__.__name__}-gn-cifar100.pth\")\n",
        "        last_accuracy = accuracy\n",
        "        print(\"model updated\")\n",
        "      else:\n",
        "        print(\"model not updated\")\n",
        "\n",
        "  return accuracy, mean(losses), last_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluateTest(net, dataloader, print_tqdm = True):\n",
        "  normalizeTest = transforms.Normalize((0.5089, 0.4874, 0.4419), (0.2683, 0.2574, 0.2771))\n",
        "  net.load_state_dict(torch.load(f\"{net.__class__.__name__}-gn-cifar100.pth\"))\n",
        "  with torch.no_grad():\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "    running_corrects = 0\n",
        "    iterable = tqdm(dataloader) if print_tqdm else dataloader\n",
        "    losses = []\n",
        "    for images, labels in iterable: \n",
        "      norm_images = []\n",
        "      for image in images:\n",
        "        norm_image = normalizeTest(image)\n",
        "        norm_images.append(norm_image)\n",
        "      norm_images = torch.stack(norm_images)  \n",
        "      norm_images = norm_images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      # Forward Pass\n",
        "      outputs = net(norm_images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "\n",
        "    \n",
        "\n",
        "  return accuracy, mean(losses)"
      ],
      "metadata": {
        "id": "LZPVP1HMeacW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "outputId": "a679269b-c816-400f-ef3a-b2dbf9e76cab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Step 31855, Loss 1.0524754524230957\n",
            "Step 31860, Loss 0.7011505365371704\n",
            "Step 31865, Loss 0.9448589086532593\n",
            "Step 31870, Loss 0.7264536619186401\n",
            "Step 31875, Loss 0.942328929901123\n",
            "Step 31880, Loss 0.9760012626647949\n",
            "Step 31885, Loss 1.089676856994629\n",
            "Step 31890, Loss 1.090157389640808\n",
            "Step 31895, Loss 0.8627643585205078\n",
            "Step 31900, Loss 1.239313006401062\n",
            "Step 31905, Loss 1.0593458414077759\n",
            "Step 31910, Loss 0.8231433033943176\n",
            "Step 31915, Loss 1.0023901462554932\n",
            "Step 31920, Loss 0.8384928703308105\n",
            "Step 31925, Loss 1.2001214027404785\n",
            "Step 31930, Loss 1.0109575986862183\n",
            "Step 31935, Loss 1.0297131538391113\n",
            "Step 31940, Loss 0.8253280520439148\n",
            "model updated\n",
            "Starting epoch 92/160, LR = [0.03928463341993392]\n",
            "Step 31945, Loss 0.8404992818832397\n",
            "Step 31950, Loss 1.04150390625\n",
            "Step 31955, Loss 0.8770989775657654\n",
            "Step 31960, Loss 0.7692509889602661\n",
            "Step 31965, Loss 0.9814978241920471\n",
            "Step 31970, Loss 0.7774585485458374\n",
            "Step 31975, Loss 1.0948759317398071\n",
            "Step 31980, Loss 0.880570113658905\n",
            "Step 31985, Loss 0.9561447501182556\n",
            "Step 31990, Loss 0.8005402088165283\n",
            "Step 31995, Loss 1.0845789909362793\n",
            "Step 32000, Loss 0.8203397393226624\n",
            "Step 32005, Loss 0.8271054625511169\n",
            "Step 32010, Loss 0.9284363389015198\n",
            "Step 32015, Loss 0.9329492449760437\n",
            "Step 32020, Loss 0.7024936676025391\n",
            "Step 32025, Loss 0.9087745547294617\n",
            "Step 32030, Loss 1.0250400304794312\n",
            "Step 32035, Loss 0.9383936524391174\n",
            "Step 32040, Loss 1.0338106155395508\n",
            "Step 32045, Loss 0.8768868446350098\n",
            "Step 32050, Loss 0.9588285684585571\n",
            "Step 32055, Loss 1.0876268148422241\n",
            "Step 32060, Loss 0.7041555047035217\n",
            "Step 32065, Loss 1.2871228456497192\n",
            "Step 32070, Loss 1.0128519535064697\n",
            "Step 32075, Loss 0.930701732635498\n",
            "Step 32080, Loss 0.9242408871650696\n",
            "Step 32085, Loss 0.7994371056556702\n",
            "Step 32090, Loss 0.6130834817886353\n",
            "Step 32095, Loss 0.9345793724060059\n",
            "Step 32100, Loss 0.8222346305847168\n",
            "Step 32105, Loss 1.0418915748596191\n",
            "Step 32110, Loss 0.9003828167915344\n",
            "Step 32115, Loss 0.988917350769043\n",
            "Step 32120, Loss 0.8051815629005432\n",
            "Step 32125, Loss 0.7983933687210083\n",
            "Step 32130, Loss 0.7915379405021667\n",
            "Step 32135, Loss 1.026348352432251\n",
            "Step 32140, Loss 0.9877937436103821\n",
            "Step 32145, Loss 0.7886275053024292\n",
            "Step 32150, Loss 0.8565844297409058\n",
            "Step 32155, Loss 0.8066380023956299\n",
            "Step 32160, Loss 0.8358421325683594\n",
            "Step 32165, Loss 0.8387818932533264\n",
            "Step 32170, Loss 1.137163519859314\n",
            "Step 32175, Loss 0.9253325462341309\n",
            "Step 32180, Loss 0.8576498627662659\n",
            "Step 32185, Loss 0.7663021087646484\n",
            "Step 32190, Loss 0.9576972126960754\n",
            "Step 32195, Loss 0.8001062870025635\n",
            "Step 32200, Loss 1.1139289140701294\n",
            "Step 32205, Loss 1.0690768957138062\n",
            "Step 32210, Loss 0.9531964063644409\n",
            "Step 32215, Loss 0.9937184453010559\n",
            "Step 32220, Loss 0.9296964406967163\n",
            "Step 32225, Loss 0.9491071105003357\n",
            "Step 32230, Loss 0.894328773021698\n",
            "Step 32235, Loss 0.9359672665596008\n",
            "Step 32240, Loss 0.8029084205627441\n",
            "Step 32245, Loss 0.9849169850349426\n",
            "Step 32250, Loss 0.844848096370697\n",
            "Step 32255, Loss 0.8002766966819763\n",
            "Step 32260, Loss 1.082643985748291\n",
            "Step 32265, Loss 0.8364275693893433\n",
            "Step 32270, Loss 0.9885363578796387\n",
            "Step 32275, Loss 0.7570186257362366\n",
            "Step 32280, Loss 1.0712246894836426\n",
            "Step 32285, Loss 0.7449550032615662\n",
            "Step 32290, Loss 1.0737806558609009\n",
            "model not updated\n",
            "Starting epoch 93/160, LR = [0.038327824315606995]\n",
            "Step 32295, Loss 0.7264243960380554\n",
            "Step 32300, Loss 0.8810833692550659\n",
            "Step 32305, Loss 0.908441424369812\n",
            "Step 32310, Loss 0.6084345579147339\n",
            "Step 32315, Loss 0.8076431751251221\n",
            "Step 32320, Loss 0.8639001250267029\n",
            "Step 32325, Loss 0.7783428430557251\n",
            "Step 32330, Loss 0.8453620672225952\n",
            "Step 32335, Loss 0.867009699344635\n",
            "Step 32340, Loss 0.9011847972869873\n",
            "Step 32345, Loss 0.7951981425285339\n",
            "Step 32350, Loss 0.7478979825973511\n",
            "Step 32355, Loss 0.8811644911766052\n",
            "Step 32360, Loss 0.8143348097801208\n",
            "Step 32365, Loss 0.7031207084655762\n",
            "Step 32370, Loss 1.1569108963012695\n",
            "Step 32375, Loss 0.6054199934005737\n",
            "Step 32380, Loss 0.7732193470001221\n",
            "Step 32385, Loss 0.8031977415084839\n",
            "Step 32390, Loss 0.6743228435516357\n",
            "Step 32395, Loss 0.8051797151565552\n",
            "Step 32400, Loss 0.732648491859436\n",
            "Step 32405, Loss 0.8973428010940552\n",
            "Step 32410, Loss 0.9247493743896484\n",
            "Step 32415, Loss 0.9847801327705383\n",
            "Step 32420, Loss 0.9106607437133789\n",
            "Step 32425, Loss 0.9136322736740112\n",
            "Step 32430, Loss 0.9086428284645081\n",
            "Step 32435, Loss 0.6683330535888672\n",
            "Step 32440, Loss 0.9042758941650391\n",
            "Step 32445, Loss 1.0912305116653442\n",
            "Step 32450, Loss 0.9883806109428406\n",
            "Step 32455, Loss 0.9096771478652954\n",
            "Step 32460, Loss 0.895020067691803\n",
            "Step 32465, Loss 0.9082730412483215\n",
            "Step 32470, Loss 1.01766037940979\n",
            "Step 32475, Loss 0.981490433216095\n",
            "Step 32480, Loss 0.9169687628746033\n",
            "Step 32485, Loss 0.8121894598007202\n",
            "Step 32490, Loss 0.752163827419281\n",
            "Step 32495, Loss 0.8697940707206726\n",
            "Step 32500, Loss 1.0141901969909668\n",
            "Step 32505, Loss 0.7497774958610535\n",
            "Step 32510, Loss 1.0176411867141724\n",
            "Step 32515, Loss 0.9072816371917725\n",
            "Step 32520, Loss 0.9286332130432129\n",
            "Step 32525, Loss 1.0894412994384766\n",
            "Step 32530, Loss 0.6971802115440369\n",
            "Step 32535, Loss 0.7537081241607666\n",
            "Step 32540, Loss 0.9260075688362122\n",
            "Step 32545, Loss 0.9973281025886536\n",
            "Step 32550, Loss 0.9303337335586548\n",
            "Step 32555, Loss 1.1373283863067627\n",
            "Step 32560, Loss 0.8168076872825623\n",
            "Step 32565, Loss 1.0409051179885864\n",
            "Step 32570, Loss 0.8849192261695862\n",
            "Step 32575, Loss 1.152498483657837\n",
            "Step 32580, Loss 0.8116463422775269\n",
            "Step 32585, Loss 0.8376674652099609\n",
            "Step 32590, Loss 0.7730458378791809\n",
            "Step 32595, Loss 0.9598042368888855\n",
            "Step 32600, Loss 0.8361402153968811\n",
            "Step 32605, Loss 0.8231071829795837\n",
            "Step 32610, Loss 0.7792664170265198\n",
            "Step 32615, Loss 0.857014536857605\n",
            "Step 32620, Loss 0.9064949750900269\n",
            "Step 32625, Loss 1.037926435470581\n",
            "Step 32630, Loss 1.0336472988128662\n",
            "Step 32635, Loss 1.0057461261749268\n",
            "Step 32640, Loss 0.7016106843948364\n",
            "model updated\n",
            "Starting epoch 94/160, LR = [0.03737551508611036]\n",
            "Step 32645, Loss 0.808415949344635\n",
            "Step 32650, Loss 0.7193976640701294\n",
            "Step 32655, Loss 0.7734909057617188\n",
            "Step 32660, Loss 0.9319620132446289\n",
            "Step 32665, Loss 0.9277900457382202\n",
            "Step 32670, Loss 0.9229229092597961\n",
            "Step 32675, Loss 0.9261428117752075\n",
            "Step 32680, Loss 0.9702289700508118\n",
            "Step 32685, Loss 1.11819589138031\n",
            "Step 32690, Loss 0.9045353531837463\n",
            "Step 32695, Loss 0.8514871597290039\n",
            "Step 32700, Loss 0.7957853674888611\n",
            "Step 32705, Loss 0.9365102648735046\n",
            "Step 32710, Loss 0.9291922450065613\n",
            "Step 32715, Loss 0.8532420992851257\n",
            "Step 32720, Loss 0.9093155860900879\n",
            "Step 32725, Loss 0.9660512208938599\n",
            "Step 32730, Loss 0.7449066042900085\n",
            "Step 32735, Loss 0.9254713654518127\n",
            "Step 32740, Loss 0.7626630663871765\n",
            "Step 32745, Loss 1.0011557340621948\n",
            "Step 32750, Loss 0.5533815622329712\n",
            "Step 32755, Loss 0.7831341028213501\n",
            "Step 32760, Loss 0.7953703999519348\n",
            "Step 32765, Loss 0.7664818167686462\n",
            "Step 32770, Loss 0.78244948387146\n",
            "Step 32775, Loss 0.6800003051757812\n",
            "Step 32780, Loss 0.931037962436676\n",
            "Step 32785, Loss 0.8946710824966431\n",
            "Step 32790, Loss 0.7331992387771606\n",
            "Step 32795, Loss 0.901462733745575\n",
            "Step 32800, Loss 1.023008942604065\n",
            "Step 32805, Loss 0.8464857339859009\n",
            "Step 32810, Loss 0.9932185411453247\n",
            "Step 32815, Loss 1.3183127641677856\n",
            "Step 32820, Loss 0.8827210068702698\n",
            "Step 32825, Loss 0.9703433513641357\n",
            "Step 32830, Loss 0.8352912664413452\n",
            "Step 32835, Loss 0.8932223916053772\n",
            "Step 32840, Loss 0.899367094039917\n",
            "Step 32845, Loss 0.8920375108718872\n",
            "Step 32850, Loss 0.9191054701805115\n",
            "Step 32855, Loss 0.7217064499855042\n",
            "Step 32860, Loss 0.938420295715332\n",
            "Step 32865, Loss 0.8325235247612\n",
            "Step 32870, Loss 1.08800208568573\n",
            "Step 32875, Loss 0.9105029702186584\n",
            "Step 32880, Loss 1.0139858722686768\n",
            "Step 32885, Loss 1.100677490234375\n",
            "Step 32890, Loss 1.0399017333984375\n",
            "Step 32895, Loss 0.9382426142692566\n",
            "Step 32900, Loss 0.8161929845809937\n",
            "Step 32905, Loss 0.9906888604164124\n",
            "Step 32910, Loss 1.0708938837051392\n",
            "Step 32915, Loss 0.8939633965492249\n",
            "Step 32920, Loss 1.0439050197601318\n",
            "Step 32925, Loss 0.7639496326446533\n",
            "Step 32930, Loss 0.83868008852005\n",
            "Step 32935, Loss 0.8331738114356995\n",
            "Step 32940, Loss 1.1918221712112427\n",
            "Step 32945, Loss 0.7940592765808105\n",
            "Step 32950, Loss 0.8628508448600769\n",
            "Step 32955, Loss 0.714617908000946\n",
            "Step 32960, Loss 1.0379267930984497\n",
            "Step 32965, Loss 0.6741154193878174\n",
            "Step 32970, Loss 0.7949740290641785\n",
            "Step 32975, Loss 0.9535911083221436\n",
            "Step 32980, Loss 0.8222646117210388\n",
            "Step 32985, Loss 0.8778706789016724\n",
            "Step 32990, Loss 0.872586190700531\n",
            "model not updated\n",
            "Starting epoch 95/160, LR = [0.03642807286477999]\n",
            "Step 32995, Loss 0.6248695254325867\n",
            "Step 33000, Loss 0.7886189222335815\n",
            "Step 33005, Loss 0.6879040598869324\n",
            "Step 33010, Loss 0.7265595197677612\n",
            "Step 33015, Loss 0.9163505434989929\n",
            "Step 33020, Loss 0.8759974241256714\n",
            "Step 33025, Loss 0.557191014289856\n",
            "Step 33030, Loss 0.9418049454689026\n",
            "Step 33035, Loss 0.8032810688018799\n",
            "Step 33040, Loss 0.8038646578788757\n",
            "Step 33045, Loss 0.7784864902496338\n",
            "Step 33050, Loss 0.7490954995155334\n",
            "Step 33055, Loss 0.8105522394180298\n",
            "Step 33060, Loss 0.7633006572723389\n",
            "Step 33065, Loss 0.9032006859779358\n",
            "Step 33070, Loss 0.8838290572166443\n",
            "Step 33075, Loss 0.8047229051589966\n",
            "Step 33080, Loss 0.8925096392631531\n",
            "Step 33085, Loss 0.8462716937065125\n",
            "Step 33090, Loss 0.7680221199989319\n",
            "Step 33095, Loss 0.917319118976593\n",
            "Step 33100, Loss 0.8748677372932434\n",
            "Step 33105, Loss 0.8442615270614624\n",
            "Step 33110, Loss 0.9364418387413025\n",
            "Step 33115, Loss 0.8146790266036987\n",
            "Step 33120, Loss 0.929902970790863\n",
            "Step 33125, Loss 0.9023635387420654\n",
            "Step 33130, Loss 0.809110701084137\n",
            "Step 33135, Loss 1.0969291925430298\n",
            "Step 33140, Loss 0.7471307516098022\n",
            "Step 33145, Loss 0.9834033250808716\n",
            "Step 33150, Loss 1.1047894954681396\n",
            "Step 33155, Loss 0.9242636561393738\n",
            "Step 33160, Loss 0.8899078965187073\n",
            "Step 33165, Loss 1.0420386791229248\n",
            "Step 33170, Loss 0.9047472476959229\n",
            "Step 33175, Loss 0.8058298826217651\n",
            "Step 33180, Loss 0.931871771812439\n",
            "Step 33185, Loss 0.7962837219238281\n",
            "Step 33190, Loss 0.9278278946876526\n",
            "Step 33195, Loss 0.676081120967865\n",
            "Step 33200, Loss 0.7461555600166321\n",
            "Step 33205, Loss 1.0675787925720215\n",
            "Step 33210, Loss 0.8620616793632507\n",
            "Step 33215, Loss 1.0176962614059448\n",
            "Step 33220, Loss 1.1992676258087158\n",
            "Step 33225, Loss 0.8634170889854431\n",
            "Step 33230, Loss 0.9718749523162842\n",
            "Step 33235, Loss 1.0326533317565918\n",
            "Step 33240, Loss 0.9519309997558594\n",
            "Step 33245, Loss 0.7570741176605225\n",
            "Step 33250, Loss 1.028128743171692\n",
            "Step 33255, Loss 0.887515127658844\n",
            "Step 33260, Loss 0.9546708464622498\n",
            "Step 33265, Loss 0.992995023727417\n",
            "Step 33270, Loss 0.7872241735458374\n",
            "Step 33275, Loss 0.8231751918792725\n",
            "Step 33280, Loss 0.8052898645401001\n",
            "Step 33285, Loss 1.0242764949798584\n",
            "Step 33290, Loss 1.0856969356536865\n",
            "Step 33295, Loss 0.8946157097816467\n",
            "Step 33300, Loss 0.9573749899864197\n",
            "Step 33305, Loss 0.9496109485626221\n",
            "Step 33310, Loss 1.0376828908920288\n",
            "Step 33315, Loss 0.7911671996116638\n",
            "Step 33320, Loss 1.0690152645111084\n",
            "Step 33325, Loss 0.9333660006523132\n",
            "Step 33330, Loss 1.180145263671875\n",
            "Step 33335, Loss 1.120713233947754\n",
            "Step 33340, Loss 0.8564636707305908\n",
            "model updated\n",
            "Starting epoch 96/160, LR = [0.035485862908627645]\n",
            "Step 33345, Loss 0.7140029072761536\n",
            "Step 33350, Loss 0.9325478076934814\n",
            "Step 33355, Loss 0.8566606640815735\n",
            "Step 33360, Loss 0.7745600342750549\n",
            "Step 33365, Loss 0.8788210153579712\n",
            "Step 33370, Loss 0.6445416808128357\n",
            "Step 33375, Loss 0.8197877407073975\n",
            "Step 33380, Loss 0.8233957290649414\n",
            "Step 33385, Loss 0.9332961440086365\n",
            "Step 33390, Loss 0.7490096092224121\n",
            "Step 33395, Loss 0.7718978524208069\n",
            "Step 33400, Loss 0.9469146728515625\n",
            "Step 33405, Loss 0.8765364289283752\n",
            "Step 33410, Loss 0.8393113017082214\n",
            "Step 33415, Loss 0.8822829127311707\n",
            "Step 33420, Loss 0.8138130903244019\n",
            "Step 33425, Loss 0.8338043689727783\n",
            "Step 33430, Loss 0.8034877181053162\n",
            "Step 33435, Loss 0.820841372013092\n",
            "Step 33440, Loss 0.976544976234436\n",
            "Step 33445, Loss 0.8895550966262817\n",
            "Step 33450, Loss 1.0242762565612793\n",
            "Step 33455, Loss 0.8449260592460632\n",
            "Step 33460, Loss 1.0193560123443604\n",
            "Step 33465, Loss 0.7632586359977722\n",
            "Step 33470, Loss 0.9173838496208191\n",
            "Step 33475, Loss 0.9242386817932129\n",
            "Step 33480, Loss 0.5230761766433716\n",
            "Step 33485, Loss 0.8428071737289429\n",
            "Step 33490, Loss 0.8293076157569885\n",
            "Step 33495, Loss 1.058687686920166\n",
            "Step 33500, Loss 0.884162187576294\n",
            "Step 33505, Loss 0.7437278032302856\n",
            "Step 33510, Loss 0.7962512969970703\n",
            "Step 33515, Loss 0.9111025333404541\n",
            "Step 33520, Loss 0.7560675144195557\n",
            "Step 33525, Loss 0.9572193026542664\n",
            "Step 33530, Loss 0.9027705192565918\n",
            "Step 33535, Loss 0.8216089606285095\n",
            "Step 33540, Loss 0.8450568318367004\n",
            "Step 33545, Loss 0.8601338267326355\n",
            "Step 33550, Loss 1.1025351285934448\n",
            "Step 33555, Loss 0.7947714924812317\n",
            "Step 33560, Loss 0.8374097347259521\n",
            "Step 33565, Loss 0.7883411645889282\n",
            "Step 33570, Loss 0.7509374618530273\n",
            "Step 33575, Loss 0.9577116370201111\n",
            "Step 33580, Loss 0.8308207988739014\n",
            "Step 33585, Loss 1.011422872543335\n",
            "Step 33590, Loss 0.950157880783081\n",
            "Step 33595, Loss 0.8243734240531921\n",
            "Step 33600, Loss 0.8059077858924866\n",
            "Step 33605, Loss 0.7294660806655884\n",
            "Step 33610, Loss 1.1060770750045776\n",
            "Step 33615, Loss 0.8053314685821533\n",
            "Step 33620, Loss 0.828981876373291\n",
            "Step 33625, Loss 0.674018144607544\n",
            "Step 33630, Loss 0.8328694105148315\n",
            "Step 33635, Loss 0.7041471600532532\n",
            "Step 33640, Loss 0.9256945252418518\n",
            "Step 33645, Loss 0.9571337103843689\n",
            "Step 33650, Loss 0.9412655234336853\n",
            "Step 33655, Loss 0.8039231896400452\n",
            "Step 33660, Loss 0.873785138130188\n",
            "Step 33665, Loss 0.7554903030395508\n",
            "Step 33670, Loss 0.82755446434021\n",
            "Step 33675, Loss 0.9326074123382568\n",
            "Step 33680, Loss 0.9301919937133789\n",
            "Step 33685, Loss 0.8289467692375183\n",
            "Step 33690, Loss 1.0876497030258179\n",
            "Step 33695, Loss 0.8461431264877319\n",
            "model updated\n",
            "Starting epoch 97/160, LR = [0.03454924845752718]\n",
            "Step 33700, Loss 0.6505218148231506\n",
            "Step 33705, Loss 0.8179696202278137\n",
            "Step 33710, Loss 0.6585109829902649\n",
            "Step 33715, Loss 0.8358498215675354\n",
            "Step 33720, Loss 0.7545843124389648\n",
            "Step 33725, Loss 0.8511842489242554\n",
            "Step 33730, Loss 0.8932749032974243\n",
            "Step 33735, Loss 0.6898770332336426\n",
            "Step 33740, Loss 0.6803820133209229\n",
            "Step 33745, Loss 0.813513457775116\n",
            "Step 33750, Loss 0.7797976136207581\n",
            "Step 33755, Loss 0.9093790054321289\n",
            "Step 33760, Loss 0.7114391326904297\n",
            "Step 33765, Loss 0.7612922191619873\n",
            "Step 33770, Loss 0.9785573482513428\n",
            "Step 33775, Loss 0.8379431366920471\n",
            "Step 33780, Loss 0.8850635290145874\n",
            "Step 33785, Loss 0.7912726402282715\n",
            "Step 33790, Loss 0.9856034517288208\n",
            "Step 33795, Loss 0.8104009628295898\n",
            "Step 33800, Loss 0.6940553188323975\n",
            "Step 33805, Loss 0.7646321058273315\n",
            "Step 33810, Loss 0.80661541223526\n",
            "Step 33815, Loss 0.7999834418296814\n",
            "Step 33820, Loss 1.0485674142837524\n",
            "Step 33825, Loss 0.6955248117446899\n",
            "Step 33830, Loss 1.050264835357666\n",
            "Step 33835, Loss 1.0638870000839233\n",
            "Step 33840, Loss 0.8886798024177551\n",
            "Step 33845, Loss 0.745012104511261\n",
            "Step 33850, Loss 0.824396014213562\n",
            "Step 33855, Loss 0.891354501247406\n",
            "Step 33860, Loss 0.824737012386322\n",
            "Step 33865, Loss 0.6922194361686707\n",
            "Step 33870, Loss 0.7815112471580505\n",
            "Step 33875, Loss 0.939937949180603\n",
            "Step 33880, Loss 0.8843215107917786\n",
            "Step 33885, Loss 0.8075424432754517\n",
            "Step 33890, Loss 0.7445386648178101\n",
            "Step 33895, Loss 0.9919636249542236\n",
            "Step 33900, Loss 0.8360200524330139\n",
            "Step 33905, Loss 0.9235587120056152\n",
            "Step 33910, Loss 0.8357308506965637\n",
            "Step 33915, Loss 0.7764177322387695\n",
            "Step 33920, Loss 1.063767433166504\n",
            "Step 33925, Loss 0.7153018116950989\n",
            "Step 33930, Loss 0.8808919787406921\n",
            "Step 33935, Loss 0.8320581316947937\n",
            "Step 33940, Loss 0.8582090735435486\n",
            "Step 33945, Loss 0.8580517172813416\n",
            "Step 33950, Loss 0.8935328722000122\n",
            "Step 33955, Loss 0.9835262298583984\n",
            "Step 33960, Loss 0.9243426322937012\n",
            "Step 33965, Loss 0.8719682097434998\n",
            "Step 33970, Loss 0.7421912550926208\n",
            "Step 33975, Loss 0.7662667632102966\n",
            "Step 33980, Loss 0.7596508860588074\n",
            "Step 33985, Loss 0.6717615127563477\n",
            "Step 33990, Loss 0.841443657875061\n",
            "Step 33995, Loss 0.9889485836029053\n",
            "Step 34000, Loss 0.8172715902328491\n",
            "Step 34005, Loss 0.8770836591720581\n",
            "Step 34010, Loss 0.8306369185447693\n",
            "Step 34015, Loss 0.7963234186172485\n",
            "Step 34020, Loss 1.0087182521820068\n",
            "Step 34025, Loss 0.8918622136116028\n",
            "Step 34030, Loss 0.7714212536811829\n",
            "Step 34035, Loss 0.9267415404319763\n",
            "Step 34040, Loss 0.74064040184021\n",
            "Step 34045, Loss 1.104807734489441\n",
            "model not updated\n",
            "Starting epoch 98/160, LR = [0.03361859059417877]\n",
            "Step 34050, Loss 0.7182964086532593\n",
            "Step 34055, Loss 0.8804343938827515\n",
            "Step 34060, Loss 0.6267234086990356\n",
            "Step 34065, Loss 0.9370689392089844\n",
            "Step 34070, Loss 0.799017071723938\n",
            "Step 34075, Loss 0.6973366737365723\n",
            "Step 34080, Loss 0.7078869938850403\n",
            "Step 34085, Loss 0.6308796405792236\n",
            "Step 34090, Loss 0.7529621124267578\n",
            "Step 34095, Loss 0.7152317762374878\n",
            "Step 34100, Loss 0.7141479253768921\n",
            "Step 34105, Loss 0.7289982438087463\n",
            "Step 34110, Loss 0.6476374864578247\n",
            "Step 34115, Loss 0.6843149065971375\n",
            "Step 34120, Loss 0.5638809204101562\n",
            "Step 34125, Loss 0.9255450367927551\n",
            "Step 34130, Loss 0.6806820631027222\n",
            "Step 34135, Loss 0.5888292789459229\n",
            "Step 34140, Loss 0.7806962132453918\n",
            "Step 34145, Loss 0.9550593495368958\n",
            "Step 34150, Loss 0.847165584564209\n",
            "Step 34155, Loss 0.7245569825172424\n",
            "Step 34160, Loss 0.7107684016227722\n",
            "Step 34165, Loss 0.7982720732688904\n",
            "Step 34170, Loss 0.7776687145233154\n",
            "Step 34175, Loss 0.9806051254272461\n",
            "Step 34180, Loss 0.9857807159423828\n",
            "Step 34185, Loss 0.8588743209838867\n",
            "Step 34190, Loss 0.8101633191108704\n",
            "Step 34195, Loss 0.7809401154518127\n",
            "Step 34200, Loss 0.823687732219696\n",
            "Step 34205, Loss 0.8771894574165344\n",
            "Step 34210, Loss 0.9586631059646606\n",
            "Step 34215, Loss 0.5979198217391968\n",
            "Step 34220, Loss 0.7601327896118164\n",
            "Step 34225, Loss 0.8062518239021301\n",
            "Step 34230, Loss 0.7218730449676514\n",
            "Step 34235, Loss 0.7672473192214966\n",
            "Step 34240, Loss 0.9045889377593994\n",
            "Step 34245, Loss 0.8529411554336548\n",
            "Step 34250, Loss 0.9068459868431091\n",
            "Step 34255, Loss 0.820389449596405\n",
            "Step 34260, Loss 0.843595564365387\n",
            "Step 34265, Loss 0.9042253494262695\n",
            "Step 34270, Loss 0.7148897647857666\n",
            "Step 34275, Loss 1.1093658208847046\n",
            "Step 34280, Loss 0.9121842384338379\n",
            "Step 34285, Loss 1.113289475440979\n",
            "Step 34290, Loss 0.9429771304130554\n",
            "Step 34295, Loss 0.8877320885658264\n",
            "Step 34300, Loss 0.9022617936134338\n",
            "Step 34305, Loss 1.0600757598876953\n",
            "Step 34310, Loss 0.7960271835327148\n",
            "Step 34315, Loss 0.7612707018852234\n",
            "Step 34320, Loss 0.797900915145874\n",
            "Step 34325, Loss 0.8025043606758118\n",
            "Step 34330, Loss 0.8683345913887024\n",
            "Step 34335, Loss 0.96429443359375\n",
            "Step 34340, Loss 0.9648225903511047\n",
            "Step 34345, Loss 0.8679336309432983\n",
            "Step 34350, Loss 0.8641723990440369\n",
            "Step 34355, Loss 0.7267562747001648\n",
            "Step 34360, Loss 0.9450177550315857\n",
            "Step 34365, Loss 0.8669971227645874\n",
            "Step 34370, Loss 0.934851884841919\n",
            "Step 34375, Loss 0.8528004884719849\n",
            "Step 34380, Loss 0.8848021030426025\n",
            "Step 34385, Loss 1.0860302448272705\n",
            "Step 34390, Loss 1.039596676826477\n",
            "Step 34395, Loss 0.9312469959259033\n",
            "model not updated\n",
            "Starting epoch 99/160, LR = [0.03269424810490462]\n",
            "Step 34400, Loss 0.7857226133346558\n",
            "Step 34405, Loss 0.8616182208061218\n",
            "Step 34410, Loss 0.736051082611084\n",
            "Step 34415, Loss 0.7022696137428284\n",
            "Step 34420, Loss 0.8140827417373657\n",
            "Step 34425, Loss 0.6530731320381165\n",
            "Step 34430, Loss 0.6103569865226746\n",
            "Step 34435, Loss 0.7416127324104309\n",
            "Step 34440, Loss 0.9190280437469482\n",
            "Step 34445, Loss 0.7039760947227478\n",
            "Step 34450, Loss 0.704390287399292\n",
            "Step 34455, Loss 0.7171024084091187\n",
            "Step 34460, Loss 0.6668294072151184\n",
            "Step 34465, Loss 0.7292394042015076\n",
            "Step 34470, Loss 0.8058745861053467\n",
            "Step 34475, Loss 0.7517461776733398\n",
            "Step 34480, Loss 0.7845870852470398\n",
            "Step 34485, Loss 0.7335493564605713\n",
            "Step 34490, Loss 0.721741795539856\n",
            "Step 34495, Loss 0.6214132308959961\n",
            "Step 34500, Loss 0.8314580917358398\n",
            "Step 34505, Loss 0.6453829407691956\n",
            "Step 34510, Loss 0.7565486431121826\n",
            "Step 34515, Loss 0.7534139156341553\n",
            "Step 34520, Loss 0.7028478980064392\n",
            "Step 34525, Loss 0.7968182563781738\n",
            "Step 34530, Loss 0.7440764307975769\n",
            "Step 34535, Loss 0.8339637517929077\n",
            "Step 34540, Loss 0.7567744851112366\n",
            "Step 34545, Loss 0.645677387714386\n",
            "Step 34550, Loss 0.6689308881759644\n",
            "Step 34555, Loss 1.0785356760025024\n",
            "Step 34560, Loss 0.9040184020996094\n",
            "Step 34565, Loss 0.8277247548103333\n",
            "Step 34570, Loss 0.9908576011657715\n",
            "Step 34575, Loss 0.7566980123519897\n",
            "Step 34580, Loss 0.9998756051063538\n",
            "Step 34585, Loss 0.8295168876647949\n",
            "Step 34590, Loss 0.8083518743515015\n",
            "Step 34595, Loss 0.9275802969932556\n",
            "Step 34600, Loss 0.836086094379425\n",
            "Step 34605, Loss 0.9118533134460449\n",
            "Step 34610, Loss 0.7813808917999268\n",
            "Step 34615, Loss 0.8132153749465942\n",
            "Step 34620, Loss 0.8014974594116211\n",
            "Step 34625, Loss 1.0612691640853882\n",
            "Step 34630, Loss 0.6476162672042847\n",
            "Step 34635, Loss 0.9095489978790283\n",
            "Step 34640, Loss 0.9139569997787476\n",
            "Step 34645, Loss 0.8256380558013916\n",
            "Step 34650, Loss 0.8326265215873718\n",
            "Step 34655, Loss 0.8578842282295227\n",
            "Step 34660, Loss 0.8458960056304932\n",
            "Step 34665, Loss 1.0240739583969116\n",
            "Step 34670, Loss 0.7622014880180359\n",
            "Step 34675, Loss 0.6975464224815369\n",
            "Step 34680, Loss 0.7769017219543457\n",
            "Step 34685, Loss 0.7082558870315552\n",
            "Step 34690, Loss 0.7246862649917603\n",
            "Step 34695, Loss 1.0704528093338013\n",
            "Step 34700, Loss 0.871489942073822\n",
            "Step 34705, Loss 0.9358521699905396\n",
            "Step 34710, Loss 0.9220510125160217\n",
            "Step 34715, Loss 0.6579834222793579\n",
            "Step 34720, Loss 0.8546360731124878\n",
            "Step 34725, Loss 0.9825485348701477\n",
            "Step 34730, Loss 0.8733253479003906\n",
            "Step 34735, Loss 0.9184360504150391\n",
            "Step 34740, Loss 0.8798952698707581\n",
            "Step 34745, Loss 1.0200448036193848\n",
            "model not updated\n",
            "Starting epoch 100/160, LR = [0.031776577341329985]\n",
            "Step 34750, Loss 0.6983267664909363\n",
            "Step 34755, Loss 0.8368892669677734\n",
            "Step 34760, Loss 0.76108717918396\n",
            "Step 34765, Loss 0.8068262338638306\n",
            "Step 34770, Loss 0.7070643901824951\n",
            "Step 34775, Loss 0.6885383129119873\n",
            "Step 34780, Loss 0.7817077040672302\n",
            "Step 34785, Loss 0.6691683530807495\n",
            "Step 34790, Loss 0.5637114644050598\n",
            "Step 34795, Loss 0.8606523871421814\n",
            "Step 34800, Loss 0.7176973223686218\n",
            "Step 34805, Loss 0.8278090953826904\n",
            "Step 34810, Loss 0.6264596581459045\n",
            "Step 34815, Loss 0.8155729174613953\n",
            "Step 34820, Loss 0.9392090439796448\n",
            "Step 34825, Loss 0.7378756999969482\n",
            "Step 34830, Loss 0.6651937961578369\n",
            "Step 34835, Loss 0.7280774116516113\n",
            "Step 34840, Loss 0.7380019426345825\n",
            "Step 34845, Loss 0.8500968217849731\n",
            "Step 34850, Loss 0.7841327786445618\n",
            "Step 34855, Loss 0.9718408584594727\n",
            "Step 34860, Loss 0.8455032706260681\n",
            "Step 34865, Loss 0.934857964515686\n",
            "Step 34870, Loss 0.6841273307800293\n",
            "Step 34875, Loss 0.9614276885986328\n",
            "Step 34880, Loss 0.9385762810707092\n",
            "Step 34885, Loss 0.8468664288520813\n",
            "Step 34890, Loss 0.7882041335105896\n",
            "Step 34895, Loss 0.660021960735321\n",
            "Step 34900, Loss 1.0207340717315674\n",
            "Step 34905, Loss 0.8904603123664856\n",
            "Step 34910, Loss 0.7567782998085022\n",
            "Step 34915, Loss 0.67083740234375\n",
            "Step 34920, Loss 0.7872733473777771\n",
            "Step 34925, Loss 0.861312985420227\n",
            "Step 34930, Loss 0.8284423351287842\n",
            "Step 34935, Loss 0.70848149061203\n",
            "Step 34940, Loss 0.8913605809211731\n",
            "Step 34945, Loss 0.8397547006607056\n",
            "Step 34950, Loss 1.1149072647094727\n",
            "Step 34955, Loss 0.6514254212379456\n",
            "Step 34960, Loss 1.006915807723999\n",
            "Step 34965, Loss 0.8390949964523315\n",
            "Step 34970, Loss 0.7444939613342285\n",
            "Step 34975, Loss 0.9424949288368225\n",
            "Step 34980, Loss 0.6558817028999329\n",
            "Step 34985, Loss 1.137891173362732\n",
            "Step 34990, Loss 0.8662944436073303\n",
            "Step 34995, Loss 0.948930561542511\n",
            "Step 35000, Loss 0.8569931387901306\n",
            "Step 35005, Loss 0.8105810284614563\n",
            "Step 35010, Loss 0.8334134221076965\n",
            "Step 35015, Loss 0.8172764182090759\n",
            "Step 35020, Loss 0.8560901880264282\n",
            "Step 35025, Loss 0.7865211367607117\n",
            "Step 35030, Loss 0.8983110785484314\n",
            "Step 35035, Loss 0.792524516582489\n",
            "Step 35040, Loss 0.7522293329238892\n",
            "Step 35045, Loss 1.0072047710418701\n",
            "Step 35050, Loss 0.77024245262146\n",
            "Step 35055, Loss 0.7862673401832581\n",
            "Step 35060, Loss 0.9804067015647888\n",
            "Step 35065, Loss 0.7014216780662537\n",
            "Step 35070, Loss 0.7700871229171753\n",
            "Step 35075, Loss 0.8453714847564697\n",
            "Step 35080, Loss 0.7561134696006775\n",
            "Step 35085, Loss 0.7253370881080627\n",
            "Step 35090, Loss 0.6999608874320984\n",
            "Step 35095, Loss 0.7620527148246765\n",
            "model not updated\n",
            "Starting epoch 101/160, LR = [0.030865932083002923]\n",
            "Step 35100, Loss 0.9448661208152771\n",
            "Step 35105, Loss 0.659074068069458\n",
            "Step 35110, Loss 0.8765931129455566\n",
            "Step 35115, Loss 0.9249793887138367\n",
            "Step 35120, Loss 0.6995243430137634\n",
            "Step 35125, Loss 0.8248103857040405\n",
            "Step 35130, Loss 0.9141334891319275\n",
            "Step 35135, Loss 1.01163911819458\n",
            "Step 35140, Loss 0.8780568838119507\n",
            "Step 35145, Loss 0.6083778738975525\n",
            "Step 35150, Loss 0.828295886516571\n",
            "Step 35155, Loss 0.7590092420578003\n",
            "Step 35160, Loss 0.7399299144744873\n",
            "Step 35165, Loss 0.6708588600158691\n",
            "Step 35170, Loss 0.8368935585021973\n",
            "Step 35175, Loss 0.671215295791626\n",
            "Step 35180, Loss 0.8392848372459412\n",
            "Step 35185, Loss 0.7571834921836853\n",
            "Step 35190, Loss 0.6558284163475037\n",
            "Step 35195, Loss 0.8576206564903259\n",
            "Step 35200, Loss 0.7404099702835083\n",
            "Step 35205, Loss 0.7852610349655151\n",
            "Step 35210, Loss 0.818408727645874\n",
            "Step 35215, Loss 0.761992871761322\n",
            "Step 35220, Loss 0.7502657771110535\n",
            "Step 35225, Loss 0.7922150492668152\n",
            "Step 35230, Loss 0.621071994304657\n",
            "Step 35235, Loss 0.7061769962310791\n",
            "Step 35240, Loss 0.7020438313484192\n",
            "Step 35245, Loss 0.7263039946556091\n",
            "Step 35250, Loss 0.7564798593521118\n",
            "Step 35255, Loss 0.6747539639472961\n",
            "Step 35260, Loss 0.7482233643531799\n",
            "Step 35265, Loss 0.8855686187744141\n",
            "Step 35270, Loss 0.849696934223175\n",
            "Step 35275, Loss 0.727469801902771\n",
            "Step 35280, Loss 0.7936615347862244\n",
            "Step 35285, Loss 0.7489705085754395\n",
            "Step 35290, Loss 0.8313673138618469\n",
            "Step 35295, Loss 0.6379231214523315\n",
            "Step 35300, Loss 0.70944744348526\n",
            "Step 35305, Loss 0.8125970363616943\n",
            "Step 35310, Loss 0.7716615200042725\n",
            "Step 35315, Loss 0.7254848480224609\n",
            "Step 35320, Loss 0.8536289930343628\n",
            "Step 35325, Loss 0.7686280608177185\n",
            "Step 35330, Loss 0.801049530506134\n",
            "Step 35335, Loss 0.7084442377090454\n",
            "Step 35340, Loss 0.8503661751747131\n",
            "Step 35345, Loss 0.8475006818771362\n",
            "Step 35350, Loss 0.9200310707092285\n",
            "Step 35355, Loss 0.7581605911254883\n",
            "Step 35360, Loss 0.8761444091796875\n",
            "Step 35365, Loss 0.8333702087402344\n",
            "Step 35370, Loss 0.7225841283798218\n",
            "Step 35375, Loss 0.7988705039024353\n",
            "Step 35380, Loss 0.7758352160453796\n",
            "Step 35385, Loss 0.6884892582893372\n",
            "Step 35390, Loss 0.6347112655639648\n",
            "Step 35395, Loss 0.9034967422485352\n",
            "Step 35400, Loss 0.8435761332511902\n",
            "Step 35405, Loss 0.7110518217086792\n",
            "Step 35410, Loss 0.8865032196044922\n",
            "Step 35415, Loss 0.6443209648132324\n",
            "Step 35420, Loss 0.7074041366577148\n",
            "Step 35425, Loss 0.8362858891487122\n",
            "Step 35430, Loss 0.9375920295715332\n",
            "Step 35435, Loss 0.7526311278343201\n",
            "Step 35440, Loss 0.8130601644515991\n",
            "Step 35445, Loss 0.7630863189697266\n",
            "Step 35450, Loss 1.037548303604126\n",
            "model not updated\n",
            "Starting epoch 102/160, LR = [0.029962663401005413]\n",
            "Step 35455, Loss 0.5507339239120483\n",
            "Step 35460, Loss 0.6666288375854492\n",
            "Step 35465, Loss 0.7969294190406799\n",
            "Step 35470, Loss 0.6634770035743713\n",
            "Step 35475, Loss 0.7771799564361572\n",
            "Step 35480, Loss 0.6771475672721863\n",
            "Step 35485, Loss 0.5980846881866455\n",
            "Step 35490, Loss 0.8057407140731812\n",
            "Step 35495, Loss 0.8434324860572815\n",
            "Step 35500, Loss 0.6923825144767761\n",
            "Step 35505, Loss 0.7137874364852905\n",
            "Step 35510, Loss 0.611007034778595\n",
            "Step 35515, Loss 0.6246800422668457\n",
            "Step 35520, Loss 0.9322089552879333\n",
            "Step 35525, Loss 0.6756245493888855\n",
            "Step 35530, Loss 0.9331867694854736\n",
            "Step 35535, Loss 0.7879648804664612\n",
            "Step 35540, Loss 0.7791588306427002\n",
            "Step 35545, Loss 0.6382099986076355\n",
            "Step 35550, Loss 0.8976274132728577\n",
            "Step 35555, Loss 0.8819422721862793\n",
            "Step 35560, Loss 0.6755576729774475\n",
            "Step 35565, Loss 0.9902275800704956\n",
            "Step 35570, Loss 0.5944104194641113\n",
            "Step 35575, Loss 0.6582047343254089\n",
            "Step 35580, Loss 0.7816641330718994\n",
            "Step 35585, Loss 0.8217560648918152\n",
            "Step 35590, Loss 0.7439939975738525\n",
            "Step 35595, Loss 0.9125232100486755\n",
            "Step 35600, Loss 0.7693325281143188\n",
            "Step 35605, Loss 0.7116382122039795\n",
            "Step 35610, Loss 0.6422461867332458\n",
            "Step 35615, Loss 0.7904709577560425\n",
            "Step 35620, Loss 0.6919189095497131\n",
            "Step 35625, Loss 0.7138177752494812\n",
            "Step 35630, Loss 0.6998152136802673\n",
            "Step 35635, Loss 0.8473950624465942\n",
            "Step 35640, Loss 0.8633962273597717\n",
            "Step 35645, Loss 0.747329592704773\n",
            "Step 35650, Loss 0.8714982271194458\n",
            "Step 35655, Loss 0.8992422223091125\n",
            "Step 35660, Loss 0.8761113882064819\n",
            "Step 35665, Loss 0.5176149606704712\n",
            "Step 35670, Loss 0.8238770961761475\n",
            "Step 35675, Loss 1.006165862083435\n",
            "Step 35680, Loss 0.6934441328048706\n",
            "Step 35685, Loss 0.7673525214195251\n",
            "Step 35690, Loss 0.8066120147705078\n",
            "Step 35695, Loss 0.8953198194503784\n",
            "Step 35700, Loss 0.9893105626106262\n",
            "Step 35705, Loss 0.7256439924240112\n",
            "Step 35710, Loss 0.806674063205719\n",
            "Step 35715, Loss 0.8574897050857544\n",
            "Step 35720, Loss 0.8631013631820679\n",
            "Step 35725, Loss 0.688107430934906\n",
            "Step 35730, Loss 0.5640518665313721\n",
            "Step 35735, Loss 0.7004759311676025\n",
            "Step 35740, Loss 0.8403317332267761\n",
            "Step 35745, Loss 0.906882107257843\n",
            "Step 35750, Loss 0.8131330609321594\n",
            "Step 35755, Loss 1.0027064085006714\n",
            "Step 35760, Loss 0.7193248867988586\n",
            "Step 35765, Loss 0.7589594721794128\n",
            "Step 35770, Loss 0.7218523621559143\n",
            "Step 35775, Loss 0.7986926436424255\n",
            "Step 35780, Loss 0.8130702972412109\n",
            "Step 35785, Loss 0.9014302492141724\n",
            "Step 35790, Loss 0.7929179668426514\n",
            "Step 35795, Loss 0.7896768450737\n",
            "Step 35800, Loss 0.6518421769142151\n",
            "model not updated\n",
            "Starting epoch 103/160, LR = [0.029067119522608906]\n",
            "Step 35805, Loss 0.6620869636535645\n",
            "Step 35810, Loss 0.8983489871025085\n",
            "Step 35815, Loss 0.7510361671447754\n",
            "Step 35820, Loss 0.7370595335960388\n",
            "Step 35825, Loss 0.7498960494995117\n",
            "Step 35830, Loss 0.7632139325141907\n",
            "Step 35835, Loss 0.8696423768997192\n",
            "Step 35840, Loss 0.7900441884994507\n",
            "Step 35845, Loss 0.6791107654571533\n",
            "Step 35850, Loss 0.8104566335678101\n",
            "Step 35855, Loss 0.796140730381012\n",
            "Step 35860, Loss 0.7554840445518494\n",
            "Step 35865, Loss 0.952286422252655\n",
            "Step 35870, Loss 0.8138664364814758\n",
            "Step 35875, Loss 0.8524751663208008\n",
            "Step 35880, Loss 0.8398215174674988\n",
            "Step 35885, Loss 0.7210810780525208\n",
            "Step 35890, Loss 0.6609981060028076\n",
            "Step 35895, Loss 0.8648691177368164\n",
            "Step 35900, Loss 0.7540496587753296\n",
            "Step 35905, Loss 0.6529130339622498\n",
            "Step 35910, Loss 0.7984321117401123\n",
            "Step 35915, Loss 0.6832591891288757\n",
            "Step 35920, Loss 0.8551305532455444\n",
            "Step 35925, Loss 0.7895565032958984\n",
            "Step 35930, Loss 0.6967914700508118\n",
            "Step 35935, Loss 0.694314181804657\n",
            "Step 35940, Loss 1.0026170015335083\n",
            "Step 35945, Loss 0.6758146286010742\n",
            "Step 35950, Loss 0.8078552484512329\n",
            "Step 35955, Loss 0.892350435256958\n",
            "Step 35960, Loss 0.9278444051742554\n",
            "Step 35965, Loss 0.6960676312446594\n",
            "Step 35970, Loss 0.6814128756523132\n",
            "Step 35975, Loss 0.816363513469696\n",
            "Step 35980, Loss 0.947463870048523\n",
            "Step 35985, Loss 0.7426056861877441\n",
            "Step 35990, Loss 0.8728455901145935\n",
            "Step 35995, Loss 0.8001956939697266\n",
            "Step 36000, Loss 0.7824413776397705\n",
            "Step 36005, Loss 0.6916289329528809\n",
            "Step 36010, Loss 0.5585174560546875\n",
            "Step 36015, Loss 0.7240299582481384\n",
            "Step 36020, Loss 0.882941722869873\n",
            "Step 36025, Loss 0.8093743920326233\n",
            "Step 36030, Loss 0.5647727251052856\n",
            "Step 36035, Loss 0.6622674465179443\n",
            "Step 36040, Loss 0.9437330365180969\n",
            "Step 36045, Loss 0.8257681727409363\n",
            "Step 36050, Loss 0.7645478248596191\n",
            "Step 36055, Loss 0.712188184261322\n",
            "Step 36060, Loss 1.038748025894165\n",
            "Step 36065, Loss 0.6273765563964844\n",
            "Step 36070, Loss 0.7544986605644226\n",
            "Step 36075, Loss 0.7748205065727234\n",
            "Step 36080, Loss 0.7618194222450256\n",
            "Step 36085, Loss 0.7740503549575806\n",
            "Step 36090, Loss 0.7480306029319763\n",
            "Step 36095, Loss 0.8661064505577087\n",
            "Step 36100, Loss 0.9526959657669067\n",
            "Step 36105, Loss 0.8889833688735962\n",
            "Step 36110, Loss 0.7042917609214783\n",
            "Step 36115, Loss 0.7514969110488892\n",
            "Step 36120, Loss 0.9037485122680664\n",
            "Step 36125, Loss 0.7926218509674072\n",
            "Step 36130, Loss 0.7208997011184692\n",
            "Step 36135, Loss 0.9632502794265747\n",
            "Step 36140, Loss 0.6164574027061462\n",
            "Step 36145, Loss 1.021239995956421\n",
            "Step 36150, Loss 0.9243781566619873\n",
            "model not updated\n",
            "Starting epoch 104/160, LR = [0.028179645697025933]\n",
            "Step 36155, Loss 0.6518539190292358\n",
            "Step 36160, Loss 0.6562662124633789\n",
            "Step 36165, Loss 0.618120551109314\n",
            "Step 36170, Loss 0.5267207026481628\n",
            "Step 36175, Loss 0.6545425653457642\n",
            "Step 36180, Loss 0.9306541085243225\n",
            "Step 36185, Loss 0.8278921246528625\n",
            "Step 36190, Loss 0.8069300651550293\n",
            "Step 36195, Loss 0.5703685879707336\n",
            "Step 36200, Loss 0.6808340549468994\n",
            "Step 36205, Loss 0.9238749146461487\n",
            "Step 36210, Loss 0.6097713112831116\n",
            "Step 36215, Loss 0.6393789649009705\n",
            "Step 36220, Loss 0.8282544612884521\n",
            "Step 36225, Loss 0.7668123245239258\n",
            "Step 36230, Loss 0.7029961347579956\n",
            "Step 36235, Loss 0.698958694934845\n",
            "Step 36240, Loss 0.628327488899231\n",
            "Step 36245, Loss 0.6977860927581787\n",
            "Step 36250, Loss 0.6477988958358765\n",
            "Step 36255, Loss 0.8349152207374573\n",
            "Step 36260, Loss 0.7020922303199768\n",
            "Step 36265, Loss 0.6176166534423828\n",
            "Step 36270, Loss 0.8000370264053345\n",
            "Step 36275, Loss 0.8359432816505432\n",
            "Step 36280, Loss 0.5964866280555725\n",
            "Step 36285, Loss 0.8877986073493958\n",
            "Step 36290, Loss 0.7404950857162476\n",
            "Step 36295, Loss 0.7297922372817993\n",
            "Step 36300, Loss 0.725380539894104\n",
            "Step 36305, Loss 0.7093549370765686\n",
            "Step 36310, Loss 0.5743564367294312\n",
            "Step 36315, Loss 0.7564716339111328\n",
            "Step 36320, Loss 0.8203443884849548\n",
            "Step 36325, Loss 0.5736536979675293\n",
            "Step 36330, Loss 0.7744963765144348\n",
            "Step 36335, Loss 0.7834051847457886\n",
            "Step 36340, Loss 1.0442708730697632\n",
            "Step 36345, Loss 0.7823097109794617\n",
            "Step 36350, Loss 0.6393849849700928\n",
            "Step 36355, Loss 0.9306445717811584\n",
            "Step 36360, Loss 0.9391480088233948\n",
            "Step 36365, Loss 0.9462395310401917\n",
            "Step 36370, Loss 0.6228159070014954\n",
            "Step 36375, Loss 0.603174090385437\n",
            "Step 36380, Loss 0.9197883605957031\n",
            "Step 36385, Loss 0.5367657542228699\n",
            "Step 36390, Loss 0.6905182600021362\n",
            "Step 36395, Loss 0.6711428165435791\n",
            "Step 36400, Loss 0.7538057565689087\n",
            "Step 36405, Loss 0.9065161347389221\n",
            "Step 36410, Loss 0.7342647314071655\n",
            "Step 36415, Loss 0.7136721014976501\n",
            "Step 36420, Loss 0.8849346041679382\n",
            "Step 36425, Loss 0.7558568120002747\n",
            "Step 36430, Loss 0.7270752191543579\n",
            "Step 36435, Loss 0.7013725638389587\n",
            "Step 36440, Loss 0.6834947466850281\n",
            "Step 36445, Loss 0.7243068814277649\n",
            "Step 36450, Loss 0.8945908546447754\n",
            "Step 36455, Loss 0.8428390622138977\n",
            "Step 36460, Loss 0.7064834237098694\n",
            "Step 36465, Loss 0.8981685638427734\n",
            "Step 36470, Loss 0.7538805603981018\n",
            "Step 36475, Loss 0.9615064263343811\n",
            "Step 36480, Loss 0.8785807490348816\n",
            "Step 36485, Loss 0.4550524950027466\n",
            "Step 36490, Loss 0.8158496022224426\n",
            "Step 36495, Loss 0.9278813004493713\n",
            "Step 36500, Loss 0.6639922261238098\n",
            "model not updated\n",
            "Starting epoch 105/160, LR = [0.027300584062310125]\n",
            "Step 36505, Loss 0.7086052298545837\n",
            "Step 36510, Loss 0.492290735244751\n",
            "Step 36515, Loss 0.6630187034606934\n",
            "Step 36520, Loss 0.7725804448127747\n",
            "Step 36525, Loss 0.6519894003868103\n",
            "Step 36530, Loss 0.6268468499183655\n",
            "Step 36535, Loss 0.5888897180557251\n",
            "Step 36540, Loss 0.671393096446991\n",
            "Step 36545, Loss 0.6964362859725952\n",
            "Step 36550, Loss 0.629180371761322\n",
            "Step 36555, Loss 0.7977479100227356\n",
            "Step 36560, Loss 0.7157652974128723\n",
            "Step 36565, Loss 0.6589665412902832\n",
            "Step 36570, Loss 0.7067489624023438\n",
            "Step 36575, Loss 0.8992919921875\n",
            "Step 36580, Loss 0.7359212636947632\n",
            "Step 36585, Loss 0.5689445734024048\n",
            "Step 36590, Loss 0.7761044502258301\n",
            "Step 36595, Loss 0.7179201245307922\n",
            "Step 36600, Loss 0.8069702982902527\n",
            "Step 36605, Loss 0.6479101181030273\n",
            "Step 36610, Loss 0.6203044056892395\n",
            "Step 36615, Loss 0.705956757068634\n",
            "Step 36620, Loss 0.8376361131668091\n",
            "Step 36625, Loss 0.5956901907920837\n",
            "Step 36630, Loss 0.7894508838653564\n",
            "Step 36635, Loss 0.606745183467865\n",
            "Step 36640, Loss 0.6945067048072815\n",
            "Step 36645, Loss 0.5925515294075012\n",
            "Step 36650, Loss 0.7700408697128296\n",
            "Step 36655, Loss 0.7841079831123352\n",
            "Step 36660, Loss 0.6310641169548035\n",
            "Step 36665, Loss 0.6001293659210205\n",
            "Step 36670, Loss 0.7011969089508057\n",
            "Step 36675, Loss 0.8132672905921936\n",
            "Step 36680, Loss 0.7197192907333374\n",
            "Step 36685, Loss 0.7239891290664673\n",
            "Step 36690, Loss 0.8706780076026917\n",
            "Step 36695, Loss 0.7185204029083252\n",
            "Step 36700, Loss 0.8503627181053162\n",
            "Step 36705, Loss 0.7479361891746521\n",
            "Step 36710, Loss 1.0295510292053223\n",
            "Step 36715, Loss 0.988598108291626\n",
            "Step 36720, Loss 0.9293361902236938\n",
            "Step 36725, Loss 0.6802921891212463\n",
            "Step 36730, Loss 0.780910074710846\n",
            "Step 36735, Loss 0.6858667135238647\n",
            "Step 36740, Loss 0.7437705993652344\n",
            "Step 36745, Loss 0.9230611324310303\n",
            "Step 36750, Loss 0.9402155876159668\n",
            "Step 36755, Loss 0.6294688582420349\n",
            "Step 36760, Loss 0.9031085968017578\n",
            "Step 36765, Loss 0.8284083604812622\n",
            "Step 36770, Loss 0.8247630596160889\n",
            "Step 36775, Loss 0.8091354966163635\n",
            "Step 36780, Loss 0.7218159437179565\n",
            "Step 36785, Loss 0.6528764367103577\n",
            "Step 36790, Loss 0.7551817893981934\n",
            "Step 36795, Loss 0.6431687474250793\n",
            "Step 36800, Loss 0.769690990447998\n",
            "Step 36805, Loss 0.6847262382507324\n",
            "Step 36810, Loss 0.7822858095169067\n",
            "Step 36815, Loss 0.7436663508415222\n",
            "Step 36820, Loss 0.7747055888175964\n",
            "Step 36825, Loss 0.7078639268875122\n",
            "Step 36830, Loss 0.8632393479347229\n",
            "Step 36835, Loss 0.7334902286529541\n",
            "Step 36840, Loss 0.966362714767456\n",
            "Step 36845, Loss 0.6667513251304626\n",
            "Step 36850, Loss 0.6628714203834534\n",
            "model not updated\n",
            "Starting epoch 106/160, LR = [0.026430273513455362]\n",
            "Step 36855, Loss 0.7155234813690186\n",
            "Step 36860, Loss 0.7334187030792236\n",
            "Step 36865, Loss 0.562228262424469\n",
            "Step 36870, Loss 0.7990759611129761\n",
            "Step 36875, Loss 0.6306716203689575\n",
            "Step 36880, Loss 0.6628298163414001\n",
            "Step 36885, Loss 0.6995232105255127\n",
            "Step 36890, Loss 0.880437970161438\n",
            "Step 36895, Loss 0.7443585395812988\n",
            "Step 36900, Loss 0.637170672416687\n",
            "Step 36905, Loss 0.6525499224662781\n",
            "Step 36910, Loss 0.7460936307907104\n",
            "Step 36915, Loss 0.782491147518158\n",
            "Step 36920, Loss 0.5096771717071533\n",
            "Step 36925, Loss 0.6966462135314941\n",
            "Step 36930, Loss 0.7234674692153931\n",
            "Step 36935, Loss 0.6857848763465881\n",
            "Step 36940, Loss 0.7161064147949219\n",
            "Step 36945, Loss 0.664291501045227\n",
            "Step 36950, Loss 0.6826301217079163\n",
            "Step 36955, Loss 0.6353654861450195\n",
            "Step 36960, Loss 0.5921874642372131\n",
            "Step 36965, Loss 0.8560453653335571\n",
            "Step 36970, Loss 0.5407373905181885\n",
            "Step 36975, Loss 0.608279824256897\n",
            "Step 36980, Loss 0.6593303084373474\n",
            "Step 36985, Loss 0.7524042725563049\n",
            "Step 36990, Loss 0.8341845870018005\n",
            "Step 36995, Loss 0.7294749021530151\n",
            "Step 37000, Loss 0.8508667945861816\n",
            "Step 37005, Loss 0.6765984892845154\n",
            "Step 37010, Loss 0.8201521635055542\n",
            "Step 37015, Loss 0.5458714365959167\n",
            "Step 37020, Loss 0.8237513899803162\n",
            "Step 37025, Loss 0.7910794019699097\n",
            "Step 37030, Loss 0.6608288884162903\n",
            "Step 37035, Loss 0.819430410861969\n",
            "Step 37040, Loss 0.9048628211021423\n",
            "Step 37045, Loss 0.8512001633644104\n",
            "Step 37050, Loss 0.7946916222572327\n",
            "Step 37055, Loss 0.6548164486885071\n",
            "Step 37060, Loss 0.7968496680259705\n",
            "Step 37065, Loss 0.7682122588157654\n",
            "Step 37070, Loss 0.6063848733901978\n",
            "Step 37075, Loss 0.9188111424446106\n",
            "Step 37080, Loss 0.915596604347229\n",
            "Step 37085, Loss 0.717318594455719\n",
            "Step 37090, Loss 0.8252094984054565\n",
            "Step 37095, Loss 0.9068351984024048\n",
            "Step 37100, Loss 0.7957666516304016\n",
            "Step 37105, Loss 0.6455637812614441\n",
            "Step 37110, Loss 0.6831738948822021\n",
            "Step 37115, Loss 0.8157774806022644\n",
            "Step 37120, Loss 0.7923822999000549\n",
            "Step 37125, Loss 0.7390103340148926\n",
            "Step 37130, Loss 0.8217307329177856\n",
            "Step 37135, Loss 0.9002500176429749\n",
            "Step 37140, Loss 0.8064923286437988\n",
            "Step 37145, Loss 0.8918638825416565\n",
            "Step 37150, Loss 0.7063891291618347\n",
            "Step 37155, Loss 0.8544933199882507\n",
            "Step 37160, Loss 0.6636176705360413\n",
            "Step 37165, Loss 0.6205909848213196\n",
            "Step 37170, Loss 0.8830351829528809\n",
            "Step 37175, Loss 0.5952095985412598\n",
            "Step 37180, Loss 0.645767867565155\n",
            "Step 37185, Loss 0.7872308492660522\n",
            "Step 37190, Loss 0.8082471489906311\n",
            "Step 37195, Loss 0.9130299687385559\n",
            "Step 37200, Loss 0.9661754965782166\n",
            "Step 37205, Loss 0.8053725361824036\n",
            "model not updated\n",
            "Starting epoch 107/160, LR = [0.025569049571745342]\n",
            "Step 37210, Loss 0.7204447984695435\n",
            "Step 37215, Loss 0.7735204696655273\n",
            "Step 37220, Loss 0.7489196062088013\n",
            "Step 37225, Loss 0.6536446213722229\n",
            "Step 37230, Loss 0.6844172477722168\n",
            "Step 37235, Loss 0.7515260577201843\n",
            "Step 37240, Loss 0.5634557604789734\n",
            "Step 37245, Loss 0.7382211089134216\n",
            "Step 37250, Loss 0.7334114909172058\n",
            "Step 37255, Loss 0.5987095832824707\n",
            "Step 37260, Loss 0.7776867747306824\n",
            "Step 37265, Loss 0.8723368048667908\n",
            "Step 37270, Loss 0.7718597650527954\n",
            "Step 37275, Loss 0.6276258826255798\n",
            "Step 37280, Loss 0.6300163865089417\n",
            "Step 37285, Loss 0.6101731657981873\n",
            "Step 37290, Loss 0.6643291115760803\n",
            "Step 37295, Loss 0.6766262054443359\n",
            "Step 37300, Loss 0.6563018560409546\n",
            "Step 37305, Loss 0.6567973494529724\n",
            "Step 37310, Loss 0.5581249594688416\n",
            "Step 37315, Loss 0.8319738507270813\n",
            "Step 37320, Loss 0.7293144464492798\n",
            "Step 37325, Loss 0.7977132797241211\n",
            "Step 37330, Loss 0.6567155718803406\n",
            "Step 37335, Loss 0.7137818932533264\n",
            "Step 37340, Loss 0.620864748954773\n",
            "Step 37345, Loss 0.7862343192100525\n",
            "Step 37350, Loss 0.6370917558670044\n",
            "Step 37355, Loss 0.7854897379875183\n",
            "Step 37360, Loss 0.6280349493026733\n",
            "Step 37365, Loss 0.669725239276886\n",
            "Step 37370, Loss 0.6863930225372314\n",
            "Step 37375, Loss 0.7660526633262634\n",
            "Step 37380, Loss 0.5519956350326538\n",
            "Step 37385, Loss 0.6792693138122559\n",
            "Step 37390, Loss 0.7014481425285339\n",
            "Step 37395, Loss 0.8639639616012573\n",
            "Step 37400, Loss 0.6933603286743164\n",
            "Step 37405, Loss 0.4969502389431\n",
            "Step 37410, Loss 0.5936224460601807\n",
            "Step 37415, Loss 0.8880079388618469\n",
            "Step 37420, Loss 0.6818650364875793\n",
            "Step 37425, Loss 0.8276641368865967\n",
            "Step 37430, Loss 0.6268262267112732\n",
            "Step 37435, Loss 0.8960049748420715\n",
            "Step 37440, Loss 0.6653414368629456\n",
            "Step 37445, Loss 0.7616105675697327\n",
            "Step 37450, Loss 0.7733016610145569\n",
            "Step 37455, Loss 0.6877113580703735\n",
            "Step 37460, Loss 0.929454505443573\n",
            "Step 37465, Loss 0.7819706797599792\n",
            "Step 37470, Loss 0.8547495603561401\n",
            "Step 37475, Loss 0.7120822072029114\n",
            "Step 37480, Loss 0.5375849604606628\n",
            "Step 37485, Loss 0.695212185382843\n",
            "Step 37490, Loss 0.7690327167510986\n",
            "Step 37495, Loss 0.6437470316886902\n",
            "Step 37500, Loss 0.7250427007675171\n",
            "Step 37505, Loss 0.8054067492485046\n",
            "Step 37510, Loss 0.8771779537200928\n",
            "Step 37515, Loss 0.7893456816673279\n",
            "Step 37520, Loss 0.7719011306762695\n",
            "Step 37525, Loss 0.7640034556388855\n",
            "Step 37530, Loss 0.6893894672393799\n",
            "Step 37535, Loss 0.7659721970558167\n",
            "Step 37540, Loss 0.8936328291893005\n",
            "Step 37545, Loss 0.8429153561592102\n",
            "Step 37550, Loss 0.7918917536735535\n",
            "Step 37555, Loss 0.9198961853981018\n",
            "model not updated\n",
            "Starting epoch 108/160, LR = [0.024717244255403763]\n",
            "Step 37560, Loss 0.7034066915512085\n",
            "Step 37565, Loss 0.6294348239898682\n",
            "Step 37570, Loss 0.840634286403656\n",
            "Step 37575, Loss 0.8013288974761963\n",
            "Step 37580, Loss 0.6544147729873657\n",
            "Step 37585, Loss 0.7966755032539368\n",
            "Step 37590, Loss 0.7136702537536621\n",
            "Step 37595, Loss 0.63172847032547\n",
            "Step 37600, Loss 0.6525576710700989\n",
            "Step 37605, Loss 0.6779595017433167\n",
            "Step 37610, Loss 0.6124271750450134\n",
            "Step 37615, Loss 0.47631514072418213\n",
            "Step 37620, Loss 0.5215287804603577\n",
            "Step 37625, Loss 0.6389374732971191\n",
            "Step 37630, Loss 0.6230548024177551\n",
            "Step 37635, Loss 0.5409659743309021\n",
            "Step 37640, Loss 0.517213761806488\n",
            "Step 37645, Loss 0.580288290977478\n",
            "Step 37650, Loss 0.6431410312652588\n",
            "Step 37655, Loss 0.7843337655067444\n",
            "Step 37660, Loss 0.7322538495063782\n",
            "Step 37665, Loss 0.6915655136108398\n",
            "Step 37670, Loss 0.6999703049659729\n",
            "Step 37675, Loss 0.9231586456298828\n",
            "Step 37680, Loss 0.8976265788078308\n",
            "Step 37685, Loss 0.6760468482971191\n",
            "Step 37690, Loss 0.6034572720527649\n",
            "Step 37695, Loss 0.8300678730010986\n",
            "Step 37700, Loss 0.8090724349021912\n",
            "Step 37705, Loss 0.724114716053009\n",
            "Step 37710, Loss 0.9164400696754456\n",
            "Step 37715, Loss 0.6036810874938965\n",
            "Step 37720, Loss 0.6164074540138245\n",
            "Step 37725, Loss 0.7118160128593445\n",
            "Step 37730, Loss 0.7324346303939819\n",
            "Step 37735, Loss 0.7582952380180359\n",
            "Step 37740, Loss 0.7139376401901245\n",
            "Step 37745, Loss 0.6025286912918091\n",
            "Step 37750, Loss 0.8499698042869568\n",
            "Step 37755, Loss 0.7466517090797424\n",
            "Step 37760, Loss 0.7437630295753479\n",
            "Step 37765, Loss 0.6007609367370605\n",
            "Step 37770, Loss 0.7161500453948975\n",
            "Step 37775, Loss 0.7418556809425354\n",
            "Step 37780, Loss 0.7953653335571289\n",
            "Step 37785, Loss 0.7234416604042053\n",
            "Step 37790, Loss 0.7717347741127014\n",
            "Step 37795, Loss 0.6659581065177917\n",
            "Step 37800, Loss 0.7917511463165283\n",
            "Step 37805, Loss 0.8778679370880127\n",
            "Step 37810, Loss 0.7423973679542542\n",
            "Step 37815, Loss 0.6523800492286682\n",
            "Step 37820, Loss 0.8334070444107056\n",
            "Step 37825, Loss 0.7069768905639648\n",
            "Step 37830, Loss 0.6888656616210938\n",
            "Step 37835, Loss 0.5881826877593994\n",
            "Step 37840, Loss 0.8052260875701904\n",
            "Step 37845, Loss 0.6714239120483398\n",
            "Step 37850, Loss 0.6587134003639221\n",
            "Step 37855, Loss 0.8152209520339966\n",
            "Step 37860, Loss 0.737921953201294\n",
            "Step 37865, Loss 0.7559611797332764\n",
            "Step 37870, Loss 0.7864217758178711\n",
            "Step 37875, Loss 0.825170636177063\n",
            "Step 37880, Loss 0.8815601468086243\n",
            "Step 37885, Loss 0.7023608684539795\n",
            "Step 37890, Loss 0.7833779454231262\n",
            "Step 37895, Loss 0.8136298060417175\n",
            "Step 37900, Loss 0.7011973857879639\n",
            "Step 37905, Loss 0.7701500654220581\n",
            "model not updated\n",
            "Starting epoch 109/160, LR = [0.023875185951594897]\n",
            "Step 37910, Loss 0.6699067950248718\n",
            "Step 37915, Loss 0.7641263008117676\n",
            "Step 37920, Loss 0.7024399638175964\n",
            "Step 37925, Loss 0.4539654850959778\n",
            "Step 37930, Loss 0.638440728187561\n",
            "Step 37935, Loss 0.7299497127532959\n",
            "Step 37940, Loss 0.8461354970932007\n",
            "Step 37945, Loss 0.6626232862472534\n",
            "Step 37950, Loss 0.6215478777885437\n",
            "Step 37955, Loss 0.6108909249305725\n",
            "Step 37960, Loss 0.6495314240455627\n",
            "Step 37965, Loss 0.7261872291564941\n",
            "Step 37970, Loss 0.5455053448677063\n",
            "Step 37975, Loss 0.6435132622718811\n",
            "Step 37980, Loss 0.5676600337028503\n",
            "Step 37985, Loss 0.7232404351234436\n",
            "Step 37990, Loss 0.6835839152336121\n",
            "Step 37995, Loss 0.6626504063606262\n",
            "Step 38000, Loss 0.6787024140357971\n",
            "Step 38005, Loss 0.8593543767929077\n",
            "Step 38010, Loss 0.4714929759502411\n",
            "Step 38015, Loss 0.6454727649688721\n",
            "Step 38020, Loss 0.8122692704200745\n",
            "Step 38025, Loss 0.6038743853569031\n",
            "Step 38030, Loss 0.7487424612045288\n",
            "Step 38035, Loss 0.569284200668335\n",
            "Step 38040, Loss 0.7622607350349426\n",
            "Step 38045, Loss 0.38758108019828796\n",
            "Step 38050, Loss 0.700831413269043\n",
            "Step 38055, Loss 0.7205838561058044\n",
            "Step 38060, Loss 0.8231497406959534\n",
            "Step 38065, Loss 0.4077616035938263\n",
            "Step 38070, Loss 0.65199214220047\n",
            "Step 38075, Loss 0.8000855445861816\n",
            "Step 38080, Loss 0.599364697933197\n",
            "Step 38085, Loss 0.8133116364479065\n",
            "Step 38090, Loss 0.6765621900558472\n",
            "Step 38095, Loss 0.5393018126487732\n",
            "Step 38100, Loss 0.8131763339042664\n",
            "Step 38105, Loss 0.534408450126648\n",
            "Step 38110, Loss 0.7935802936553955\n",
            "Step 38115, Loss 0.7796627283096313\n",
            "Step 38120, Loss 0.6881879568099976\n",
            "Step 38125, Loss 0.7370953559875488\n",
            "Step 38130, Loss 0.7058143615722656\n",
            "Step 38135, Loss 0.637397050857544\n",
            "Step 38140, Loss 0.49231183528900146\n",
            "Step 38145, Loss 0.6879227757453918\n",
            "Step 38150, Loss 0.563567578792572\n",
            "Step 38155, Loss 0.7836053371429443\n",
            "Step 38160, Loss 0.8573461771011353\n",
            "Step 38165, Loss 0.7920729517936707\n",
            "Step 38170, Loss 0.7733024954795837\n",
            "Step 38175, Loss 0.936825692653656\n",
            "Step 38180, Loss 1.1261619329452515\n",
            "Step 38185, Loss 0.8890291452407837\n",
            "Step 38190, Loss 0.6626765727996826\n",
            "Step 38195, Loss 0.6680678129196167\n",
            "Step 38200, Loss 0.9072786569595337\n",
            "Step 38205, Loss 0.6776637434959412\n",
            "Step 38210, Loss 0.7897546887397766\n",
            "Step 38215, Loss 0.6721063256263733\n",
            "Step 38220, Loss 0.5171098709106445\n",
            "Step 38225, Loss 0.6493569016456604\n",
            "Step 38230, Loss 0.7002028226852417\n",
            "Step 38235, Loss 0.770373523235321\n",
            "Step 38240, Loss 0.6144381761550903\n",
            "Step 38245, Loss 0.6163101196289062\n",
            "Step 38250, Loss 0.497699111700058\n",
            "Step 38255, Loss 0.725195586681366\n",
            "model updated\n",
            "Starting epoch 110/160, LR = [0.02304319928982421]\n",
            "Step 38260, Loss 0.7001180052757263\n",
            "Step 38265, Loss 0.6996307969093323\n",
            "Step 38270, Loss 0.5155845880508423\n",
            "Step 38275, Loss 0.8459936380386353\n",
            "Step 38280, Loss 0.6502686142921448\n",
            "Step 38285, Loss 0.8107545971870422\n",
            "Step 38290, Loss 0.8021766543388367\n",
            "Step 38295, Loss 0.6473774313926697\n",
            "Step 38300, Loss 0.8498567342758179\n",
            "Step 38305, Loss 0.6530337929725647\n",
            "Step 38310, Loss 0.49384456872940063\n",
            "Step 38315, Loss 0.9839159250259399\n",
            "Step 38320, Loss 0.7733029723167419\n",
            "Step 38325, Loss 0.7219637036323547\n",
            "Step 38330, Loss 0.621636152267456\n",
            "Step 38335, Loss 0.5727464556694031\n",
            "Step 38340, Loss 0.5569357872009277\n",
            "Step 38345, Loss 0.5528810620307922\n",
            "Step 38350, Loss 0.7188450694084167\n",
            "Step 38355, Loss 0.6333149075508118\n",
            "Step 38360, Loss 0.7362138628959656\n",
            "Step 38365, Loss 0.6938456296920776\n",
            "Step 38370, Loss 0.6849558353424072\n",
            "Step 38375, Loss 0.5303399562835693\n",
            "Step 38380, Loss 0.5463520288467407\n",
            "Step 38385, Loss 0.546356201171875\n",
            "Step 38390, Loss 0.6708413362503052\n",
            "Step 38395, Loss 0.6332110166549683\n",
            "Step 38400, Loss 0.8352293968200684\n",
            "Step 38405, Loss 0.9147914052009583\n",
            "Step 38410, Loss 0.8913068175315857\n",
            "Step 38415, Loss 0.7196958065032959\n",
            "Step 38420, Loss 0.8216949105262756\n",
            "Step 38425, Loss 0.7794511318206787\n",
            "Step 38430, Loss 0.6331247091293335\n",
            "Step 38435, Loss 0.6234768033027649\n",
            "Step 38440, Loss 0.5783483386039734\n",
            "Step 38445, Loss 0.6414034962654114\n",
            "Step 38450, Loss 0.716257631778717\n",
            "Step 38455, Loss 0.6709792613983154\n",
            "Step 38460, Loss 0.5055080056190491\n",
            "Step 38465, Loss 0.8437349796295166\n",
            "Step 38470, Loss 0.8146506547927856\n",
            "Step 38475, Loss 0.7497767806053162\n",
            "Step 38480, Loss 0.8569339513778687\n",
            "Step 38485, Loss 0.6247155070304871\n",
            "Step 38490, Loss 0.8794607520103455\n",
            "Step 38495, Loss 0.6389942169189453\n",
            "Step 38500, Loss 0.6993895769119263\n",
            "Step 38505, Loss 0.5521610975265503\n",
            "Step 38510, Loss 0.825294554233551\n",
            "Step 38515, Loss 0.6568089127540588\n",
            "Step 38520, Loss 0.735397458076477\n",
            "Step 38525, Loss 0.7328882813453674\n",
            "Step 38530, Loss 0.7236297130584717\n",
            "Step 38535, Loss 0.8038145899772644\n",
            "Step 38540, Loss 0.7583054304122925\n",
            "Step 38545, Loss 0.748416543006897\n",
            "Step 38550, Loss 0.6538174748420715\n",
            "Step 38555, Loss 0.6522036194801331\n",
            "Step 38560, Loss 0.6745277643203735\n",
            "Step 38565, Loss 0.6036533117294312\n",
            "Step 38570, Loss 0.6417749524116516\n",
            "Step 38575, Loss 0.7419139742851257\n",
            "Step 38580, Loss 0.7324110865592957\n",
            "Step 38585, Loss 0.6326069235801697\n",
            "Step 38590, Loss 0.6585612297058105\n",
            "Step 38595, Loss 0.8695322275161743\n",
            "Step 38600, Loss 0.707222580909729\n",
            "Step 38605, Loss 0.8403851985931396\n",
            "model not updated\n",
            "Starting epoch 111/160, LR = [0.022221605016787344]\n",
            "Step 38610, Loss 0.7241954207420349\n",
            "Step 38615, Loss 0.6327710151672363\n",
            "Step 38620, Loss 0.7804739475250244\n",
            "Step 38625, Loss 0.7526108026504517\n",
            "Step 38630, Loss 0.7091159820556641\n",
            "Step 38635, Loss 0.614827573299408\n",
            "Step 38640, Loss 0.7223869562149048\n",
            "Step 38645, Loss 0.5041832327842712\n",
            "Step 38650, Loss 0.6584311723709106\n",
            "Step 38655, Loss 0.5905328392982483\n",
            "Step 38660, Loss 0.792729377746582\n",
            "Step 38665, Loss 0.5949773788452148\n",
            "Step 38670, Loss 0.6888784170150757\n",
            "Step 38675, Loss 0.6255428194999695\n",
            "Step 38680, Loss 0.6506550312042236\n",
            "Step 38685, Loss 0.6589841246604919\n",
            "Step 38690, Loss 0.6668709516525269\n",
            "Step 38695, Loss 0.6485386490821838\n",
            "Step 38700, Loss 0.769561231136322\n",
            "Step 38705, Loss 0.7879065871238708\n",
            "Step 38710, Loss 0.7579618096351624\n",
            "Step 38715, Loss 0.6697651147842407\n",
            "Step 38720, Loss 0.7824081182479858\n",
            "Step 38725, Loss 0.6820783019065857\n",
            "Step 38730, Loss 0.7459043264389038\n",
            "Step 38735, Loss 0.5610470771789551\n",
            "Step 38740, Loss 0.5232912302017212\n",
            "Step 38745, Loss 0.5699865818023682\n",
            "Step 38750, Loss 0.7329649925231934\n",
            "Step 38755, Loss 0.5694257020950317\n",
            "Step 38760, Loss 0.660224199295044\n",
            "Step 38765, Loss 0.7107882499694824\n",
            "Step 38770, Loss 0.5066664814949036\n",
            "Step 38775, Loss 0.7765099406242371\n",
            "Step 38780, Loss 0.7610874772071838\n",
            "Step 38785, Loss 0.504864513874054\n",
            "Step 38790, Loss 0.6614838242530823\n",
            "Step 38795, Loss 0.8043087720870972\n",
            "Step 38800, Loss 0.7254045605659485\n",
            "Step 38805, Loss 0.7329481840133667\n",
            "Step 38810, Loss 0.7174906730651855\n",
            "Step 38815, Loss 0.6794009804725647\n",
            "Step 38820, Loss 0.8090986609458923\n",
            "Step 38825, Loss 0.8854900598526001\n",
            "Step 38830, Loss 0.6465349793434143\n",
            "Step 38835, Loss 0.8266419172286987\n",
            "Step 38840, Loss 0.7620256543159485\n",
            "Step 38845, Loss 0.6637355089187622\n",
            "Step 38850, Loss 0.8887766599655151\n",
            "Step 38855, Loss 0.5545345544815063\n",
            "Step 38860, Loss 0.6687051057815552\n",
            "Step 38865, Loss 0.6551224589347839\n",
            "Step 38870, Loss 0.8274438381195068\n",
            "Step 38875, Loss 0.6090418100357056\n",
            "Step 38880, Loss 0.7571137547492981\n",
            "Step 38885, Loss 0.8676370978355408\n",
            "Step 38890, Loss 0.8389695286750793\n",
            "Step 38895, Loss 0.6093903183937073\n",
            "Step 38900, Loss 0.6636656522750854\n",
            "Step 38905, Loss 0.7382354140281677\n",
            "Step 38910, Loss 0.606709897518158\n",
            "Step 38915, Loss 0.7652638554573059\n",
            "Step 38920, Loss 0.9236390590667725\n",
            "Step 38925, Loss 0.7446795701980591\n",
            "Step 38930, Loss 0.7210856676101685\n",
            "Step 38935, Loss 0.5872923731803894\n",
            "Step 38940, Loss 0.7385417819023132\n",
            "Step 38945, Loss 0.5997907519340515\n",
            "Step 38950, Loss 0.6489895582199097\n",
            "Step 38955, Loss 0.8784996867179871\n",
            "Step 38960, Loss 0.7645725011825562\n",
            "model not updated\n",
            "Starting epoch 112/160, LR = [0.021410719872716394]\n",
            "Step 38965, Loss 0.8442249894142151\n",
            "Step 38970, Loss 0.6235209703445435\n",
            "Step 38975, Loss 0.777738630771637\n",
            "Step 38980, Loss 0.6194419860839844\n",
            "Step 38985, Loss 0.6328849792480469\n",
            "Step 38990, Loss 0.5560690760612488\n",
            "Step 38995, Loss 0.731397271156311\n",
            "Step 39000, Loss 0.7154318690299988\n",
            "Step 39005, Loss 0.7394485473632812\n",
            "Step 39010, Loss 0.6110305786132812\n",
            "Step 39015, Loss 0.5786618590354919\n",
            "Step 39020, Loss 0.6860016584396362\n",
            "Step 39025, Loss 0.6781359314918518\n",
            "Step 39030, Loss 0.6420292854309082\n",
            "Step 39035, Loss 0.5208022594451904\n",
            "Step 39040, Loss 0.6988385915756226\n",
            "Step 39045, Loss 0.6653724908828735\n",
            "Step 39050, Loss 0.6315686702728271\n",
            "Step 39055, Loss 0.8379774689674377\n",
            "Step 39060, Loss 0.5908569693565369\n",
            "Step 39065, Loss 0.5340925455093384\n",
            "Step 39070, Loss 0.8066018223762512\n",
            "Step 39075, Loss 0.6075318455696106\n",
            "Step 39080, Loss 0.5594081878662109\n",
            "Step 39085, Loss 0.671593427658081\n",
            "Step 39090, Loss 0.726586103439331\n",
            "Step 39095, Loss 0.5094465613365173\n",
            "Step 39100, Loss 0.8004547953605652\n",
            "Step 39105, Loss 0.6004390120506287\n",
            "Step 39110, Loss 0.8987180590629578\n",
            "Step 39115, Loss 0.8312646150588989\n",
            "Step 39120, Loss 0.6985477805137634\n",
            "Step 39125, Loss 0.4493730068206787\n",
            "Step 39130, Loss 0.6271640062332153\n",
            "Step 39135, Loss 0.592491865158081\n",
            "Step 39140, Loss 0.7375231981277466\n",
            "Step 39145, Loss 0.7870649099349976\n",
            "Step 39150, Loss 0.6064453721046448\n",
            "Step 39155, Loss 0.8840352296829224\n",
            "Step 39160, Loss 0.7930966019630432\n",
            "Step 39165, Loss 0.4819660782814026\n",
            "Step 39170, Loss 0.6187421679496765\n",
            "Step 39175, Loss 0.509365439414978\n",
            "Step 39180, Loss 0.6807005405426025\n",
            "Step 39185, Loss 0.4637092053890228\n",
            "Step 39190, Loss 0.6603193879127502\n",
            "Step 39195, Loss 0.6399256587028503\n",
            "Step 39200, Loss 0.5304960608482361\n",
            "Step 39205, Loss 0.6915432810783386\n",
            "Step 39210, Loss 0.6472043395042419\n",
            "Step 39215, Loss 0.5613829493522644\n",
            "Step 39220, Loss 0.6075046062469482\n",
            "Step 39225, Loss 0.5221393704414368\n",
            "Step 39230, Loss 0.7015575170516968\n",
            "Step 39235, Loss 0.6315097212791443\n",
            "Step 39240, Loss 0.6383885741233826\n",
            "Step 39245, Loss 0.5567588806152344\n",
            "Step 39250, Loss 0.5797690749168396\n",
            "Step 39255, Loss 0.6941549181938171\n",
            "Step 39260, Loss 0.6797816157341003\n",
            "Step 39265, Loss 0.664732813835144\n",
            "Step 39270, Loss 0.6839814782142639\n",
            "Step 39275, Loss 0.6676120162010193\n",
            "Step 39280, Loss 0.6591252088546753\n",
            "Step 39285, Loss 0.5370879173278809\n",
            "Step 39290, Loss 0.6689971685409546\n",
            "Step 39295, Loss 0.6895574927330017\n",
            "Step 39300, Loss 0.814409613609314\n",
            "Step 39305, Loss 0.8178305625915527\n",
            "Step 39310, Loss 0.8332685232162476\n",
            "model not updated\n",
            "Starting epoch 113/160, LR = [0.020610856469270255]\n",
            "Step 39315, Loss 0.5839438438415527\n",
            "Step 39320, Loss 0.6544190049171448\n",
            "Step 39325, Loss 0.6185814738273621\n",
            "Step 39330, Loss 0.6590330600738525\n",
            "Step 39335, Loss 0.5037330389022827\n",
            "Step 39340, Loss 0.48472198843955994\n",
            "Step 39345, Loss 0.7350711822509766\n",
            "Step 39350, Loss 0.7364924550056458\n",
            "Step 39355, Loss 0.5430580377578735\n",
            "Step 39360, Loss 0.6615713238716125\n",
            "Step 39365, Loss 0.6243588328361511\n",
            "Step 39370, Loss 0.5829734206199646\n",
            "Step 39375, Loss 0.5897381901741028\n",
            "Step 39380, Loss 0.7422721982002258\n",
            "Step 39385, Loss 0.6099196672439575\n",
            "Step 39390, Loss 0.5057789087295532\n",
            "Step 39395, Loss 0.799306333065033\n",
            "Step 39400, Loss 0.8406888842582703\n",
            "Step 39405, Loss 0.60445636510849\n",
            "Step 39410, Loss 0.5820108652114868\n",
            "Step 39415, Loss 0.6490289568901062\n",
            "Step 39420, Loss 0.7198976874351501\n",
            "Step 39425, Loss 0.7632334232330322\n",
            "Step 39430, Loss 0.7246259450912476\n",
            "Step 39435, Loss 0.5576497912406921\n",
            "Step 39440, Loss 0.558448314666748\n",
            "Step 39445, Loss 0.7752161026000977\n",
            "Step 39450, Loss 0.632590115070343\n",
            "Step 39455, Loss 0.5963975191116333\n",
            "Step 39460, Loss 0.6317062973976135\n",
            "Step 39465, Loss 0.6503822207450867\n",
            "Step 39470, Loss 0.6287800073623657\n",
            "Step 39475, Loss 0.6371369361877441\n",
            "Step 39480, Loss 0.5812370777130127\n",
            "Step 39485, Loss 0.6694231033325195\n",
            "Step 39490, Loss 0.6031343936920166\n",
            "Step 39495, Loss 0.6392295956611633\n",
            "Step 39500, Loss 0.6856235265731812\n",
            "Step 39505, Loss 0.6174201965332031\n",
            "Step 39510, Loss 0.5934455394744873\n",
            "Step 39515, Loss 0.7513695955276489\n",
            "Step 39520, Loss 0.6268069744110107\n",
            "Step 39525, Loss 0.7394485473632812\n",
            "Step 39530, Loss 0.8303938508033752\n",
            "Step 39535, Loss 0.6582559943199158\n",
            "Step 39540, Loss 0.5888218879699707\n",
            "Step 39545, Loss 0.6395204067230225\n",
            "Step 39550, Loss 0.7150067687034607\n",
            "Step 39555, Loss 0.7460965514183044\n",
            "Step 39560, Loss 0.76528000831604\n",
            "Step 39565, Loss 0.6367667317390442\n",
            "Step 39570, Loss 0.6884132623672485\n",
            "Step 39575, Loss 0.729493260383606\n",
            "Step 39580, Loss 0.877341628074646\n",
            "Step 39585, Loss 0.7052112221717834\n",
            "Step 39590, Loss 0.6812474131584167\n",
            "Step 39595, Loss 0.6541025638580322\n",
            "Step 39600, Loss 0.8472844958305359\n",
            "Step 39605, Loss 0.6483567357063293\n",
            "Step 39610, Loss 0.7136232256889343\n",
            "Step 39615, Loss 0.6980351209640503\n",
            "Step 39620, Loss 0.7791680097579956\n",
            "Step 39625, Loss 0.5724093317985535\n",
            "Step 39630, Loss 0.7383642792701721\n",
            "Step 39635, Loss 0.7481139302253723\n",
            "Step 39640, Loss 0.7089362740516663\n",
            "Step 39645, Loss 0.5397250056266785\n",
            "Step 39650, Loss 0.5895863771438599\n",
            "Step 39655, Loss 0.6839767694473267\n",
            "Step 39660, Loss 0.6076804995536804\n",
            "model not updated\n",
            "Starting epoch 114/160, LR = [0.01982232316901706]\n",
            "Step 39665, Loss 0.6141818761825562\n",
            "Step 39670, Loss 0.6227948665618896\n",
            "Step 39675, Loss 0.6853188276290894\n",
            "Step 39680, Loss 0.6724503636360168\n",
            "Step 39685, Loss 0.6501364707946777\n",
            "Step 39690, Loss 0.8578349947929382\n",
            "Step 39695, Loss 0.5563965439796448\n",
            "Step 39700, Loss 0.7105879187583923\n",
            "Step 39705, Loss 0.6410290598869324\n",
            "Step 39710, Loss 0.6741034388542175\n",
            "Step 39715, Loss 0.5079213380813599\n",
            "Step 39720, Loss 0.4885440170764923\n",
            "Step 39725, Loss 0.5500217080116272\n",
            "Step 39730, Loss 0.6477723121643066\n",
            "Step 39735, Loss 0.7033840417861938\n",
            "Step 39740, Loss 0.740149199962616\n",
            "Step 39745, Loss 0.7379469871520996\n",
            "Step 39750, Loss 0.6676670908927917\n",
            "Step 39755, Loss 0.5151568651199341\n",
            "Step 39760, Loss 0.6683672666549683\n",
            "Step 39765, Loss 0.4755757451057434\n",
            "Step 39770, Loss 0.7282507419586182\n",
            "Step 39775, Loss 0.6929507851600647\n",
            "Step 39780, Loss 0.8718177080154419\n",
            "Step 39785, Loss 0.6814203858375549\n",
            "Step 39790, Loss 0.5012165307998657\n",
            "Step 39795, Loss 0.7884414196014404\n",
            "Step 39800, Loss 0.5816066265106201\n",
            "Step 39805, Loss 0.8076874017715454\n",
            "Step 39810, Loss 0.6274506449699402\n",
            "Step 39815, Loss 0.6322693228721619\n",
            "Step 39820, Loss 0.7442691326141357\n",
            "Step 39825, Loss 0.4800281822681427\n",
            "Step 39830, Loss 0.8211474418640137\n",
            "Step 39835, Loss 0.6773306131362915\n",
            "Step 39840, Loss 0.6493964791297913\n",
            "Step 39845, Loss 0.6757002472877502\n",
            "Step 39850, Loss 0.6630522012710571\n",
            "Step 39855, Loss 0.8004356622695923\n",
            "Step 39860, Loss 0.5041040182113647\n",
            "Step 39865, Loss 0.8382468223571777\n",
            "Step 39870, Loss 0.5635102987289429\n",
            "Step 39875, Loss 0.6180514693260193\n",
            "Step 39880, Loss 0.7534794807434082\n",
            "Step 39885, Loss 0.5481881499290466\n",
            "Step 39890, Loss 0.679561197757721\n",
            "Step 39895, Loss 0.617207407951355\n",
            "Step 39900, Loss 0.5236294269561768\n",
            "Step 39905, Loss 0.5165446996688843\n",
            "Step 39910, Loss 0.6944758892059326\n",
            "Step 39915, Loss 0.5853009223937988\n",
            "Step 39920, Loss 0.43792983889579773\n",
            "Step 39925, Loss 0.6855195760726929\n",
            "Step 39930, Loss 0.7269201278686523\n",
            "Step 39935, Loss 0.7160791158676147\n",
            "Step 39940, Loss 0.6502947807312012\n",
            "Step 39945, Loss 0.4887866973876953\n",
            "Step 39950, Loss 0.6421024799346924\n",
            "Step 39955, Loss 0.6456291079521179\n",
            "Step 39960, Loss 0.4674333930015564\n",
            "Step 39965, Loss 0.563201904296875\n",
            "Step 39970, Loss 0.6349407434463501\n",
            "Step 39975, Loss 0.5895040035247803\n",
            "Step 39980, Loss 0.506777286529541\n",
            "Step 39985, Loss 0.6793819069862366\n",
            "Step 39990, Loss 0.5507708191871643\n",
            "Step 39995, Loss 0.625721275806427\n",
            "Step 40000, Loss 0.6089603900909424\n",
            "Step 40005, Loss 0.5657041668891907\n",
            "Step 40010, Loss 0.5635022521018982\n",
            "model not updated\n",
            "Starting epoch 115/160, LR = [0.019045423966554495]\n",
            "Step 40015, Loss 0.5816376805305481\n",
            "Step 40020, Loss 0.6698448657989502\n",
            "Step 40025, Loss 0.6414961814880371\n",
            "Step 40030, Loss 0.5733571648597717\n",
            "Step 40035, Loss 0.5343456864356995\n",
            "Step 40040, Loss 0.5895131826400757\n",
            "Step 40045, Loss 0.6777633428573608\n",
            "Step 40050, Loss 0.6968346834182739\n",
            "Step 40055, Loss 0.5061104893684387\n",
            "Step 40060, Loss 0.5290352702140808\n",
            "Step 40065, Loss 0.646364152431488\n",
            "Step 40070, Loss 0.6115328669548035\n",
            "Step 40075, Loss 0.6156426668167114\n",
            "Step 40080, Loss 0.6310749650001526\n",
            "Step 40085, Loss 0.6268101334571838\n",
            "Step 40090, Loss 0.6228587627410889\n",
            "Step 40095, Loss 0.5757800340652466\n",
            "Step 40100, Loss 0.5533254742622375\n",
            "Step 40105, Loss 0.6037896871566772\n",
            "Step 40110, Loss 0.515957772731781\n",
            "Step 40115, Loss 0.5339662432670593\n",
            "Step 40120, Loss 0.6685267686843872\n",
            "Step 40125, Loss 0.7059069275856018\n",
            "Step 40130, Loss 0.6040201187133789\n",
            "Step 40135, Loss 0.6015973687171936\n",
            "Step 40140, Loss 0.640031099319458\n",
            "Step 40145, Loss 0.5916816592216492\n",
            "Step 40150, Loss 0.6738327145576477\n",
            "Step 40155, Loss 0.711435079574585\n",
            "Step 40160, Loss 0.6593313217163086\n",
            "Step 40165, Loss 0.6031044125556946\n",
            "Step 40170, Loss 0.5577062368392944\n",
            "Step 40175, Loss 0.5390462279319763\n",
            "Step 40180, Loss 0.648711621761322\n",
            "Step 40185, Loss 0.6797338128089905\n",
            "Step 40190, Loss 0.5601978898048401\n",
            "Step 40195, Loss 0.5495519042015076\n",
            "Step 40200, Loss 0.7103523015975952\n",
            "Step 40205, Loss 0.4315292239189148\n",
            "Step 40210, Loss 0.5665508508682251\n",
            "Step 40215, Loss 0.6476390361785889\n",
            "Step 40220, Loss 0.5652770400047302\n",
            "Step 40225, Loss 0.5421187281608582\n",
            "Step 40230, Loss 0.6273861527442932\n",
            "Step 40235, Loss 0.7969768643379211\n",
            "Step 40240, Loss 0.6428973078727722\n",
            "Step 40245, Loss 0.5582050085067749\n",
            "Step 40250, Loss 0.7271227240562439\n",
            "Step 40255, Loss 0.6217401027679443\n",
            "Step 40260, Loss 0.8409591317176819\n",
            "Step 40265, Loss 0.5053892731666565\n",
            "Step 40270, Loss 0.6591019630432129\n",
            "Step 40275, Loss 0.5674644708633423\n",
            "Step 40280, Loss 0.658708393573761\n",
            "Step 40285, Loss 0.6075349450111389\n",
            "Step 40290, Loss 0.6909685730934143\n",
            "Step 40295, Loss 0.5992858409881592\n",
            "Step 40300, Loss 0.6185989379882812\n",
            "Step 40305, Loss 0.7654949426651001\n",
            "Step 40310, Loss 0.5998360514640808\n",
            "Step 40315, Loss 0.8346559405326843\n",
            "Step 40320, Loss 0.6864563226699829\n",
            "Step 40325, Loss 0.7726922631263733\n",
            "Step 40330, Loss 0.5827106237411499\n",
            "Step 40335, Loss 0.6235133409500122\n",
            "Step 40340, Loss 0.5912207365036011\n",
            "Step 40345, Loss 0.7639523148536682\n",
            "Step 40350, Loss 0.5758923888206482\n",
            "Step 40355, Loss 0.7556422352790833\n",
            "Step 40360, Loss 0.7615907192230225\n",
            "model not updated\n",
            "Starting epoch 116/160, LR = [0.01828045837131401]\n",
            "Step 40365, Loss 0.8706491589546204\n",
            "Step 40370, Loss 0.805835485458374\n",
            "Step 40375, Loss 0.6766815185546875\n",
            "Step 40380, Loss 0.5898156762123108\n",
            "Step 40385, Loss 0.7240008115768433\n",
            "Step 40390, Loss 0.38705745339393616\n",
            "Step 40395, Loss 0.7905197143554688\n",
            "Step 40400, Loss 0.6624207496643066\n",
            "Step 40405, Loss 0.4962603747844696\n",
            "Step 40410, Loss 0.8799492716789246\n",
            "Step 40415, Loss 0.5287840366363525\n",
            "Step 40420, Loss 0.6386865377426147\n",
            "Step 40425, Loss 0.4452430307865143\n",
            "Step 40430, Loss 0.6278579831123352\n",
            "Step 40435, Loss 0.6540660262107849\n",
            "Step 40440, Loss 0.5226075053215027\n",
            "Step 40445, Loss 0.3772786855697632\n",
            "Step 40450, Loss 0.503739058971405\n",
            "Step 40455, Loss 0.7284303307533264\n",
            "Step 40460, Loss 0.5858096480369568\n",
            "Step 40465, Loss 0.6018580198287964\n",
            "Step 40470, Loss 0.6944483518600464\n",
            "Step 40475, Loss 0.5734856128692627\n",
            "Step 40480, Loss 0.6354332566261292\n",
            "Step 40485, Loss 0.6685136556625366\n",
            "Step 40490, Loss 0.5087407827377319\n",
            "Step 40495, Loss 0.5072007179260254\n",
            "Step 40500, Loss 0.7513381242752075\n",
            "Step 40505, Loss 0.6708123683929443\n",
            "Step 40510, Loss 0.5131974816322327\n",
            "Step 40515, Loss 0.6817147731781006\n",
            "Step 40520, Loss 0.5653622150421143\n",
            "Step 40525, Loss 0.8556004762649536\n",
            "Step 40530, Loss 0.5930672287940979\n",
            "Step 40535, Loss 0.6102085113525391\n",
            "Step 40540, Loss 0.7556059956550598\n",
            "Step 40545, Loss 0.5957348942756653\n",
            "Step 40550, Loss 0.5216435194015503\n",
            "Step 40555, Loss 0.6438227295875549\n",
            "Step 40560, Loss 0.6393392086029053\n",
            "Step 40565, Loss 0.5550355315208435\n",
            "Step 40570, Loss 0.6809927225112915\n",
            "Step 40575, Loss 0.5702710151672363\n",
            "Step 40580, Loss 0.7733007669448853\n",
            "Step 40585, Loss 0.7331811785697937\n",
            "Step 40590, Loss 0.6205148696899414\n",
            "Step 40595, Loss 0.369894802570343\n",
            "Step 40600, Loss 0.7962441444396973\n",
            "Step 40605, Loss 0.6834083795547485\n",
            "Step 40610, Loss 0.49247467517852783\n",
            "Step 40615, Loss 0.551376223564148\n",
            "Step 40620, Loss 0.5725817680358887\n",
            "Step 40625, Loss 0.6228474974632263\n",
            "Step 40630, Loss 0.651214063167572\n",
            "Step 40635, Loss 0.7299969792366028\n",
            "Step 40640, Loss 0.5022247433662415\n",
            "Step 40645, Loss 0.586380660533905\n",
            "Step 40650, Loss 0.5651785731315613\n",
            "Step 40655, Loss 0.4992007911205292\n",
            "Step 40660, Loss 0.5532482862472534\n",
            "Step 40665, Loss 0.5456499457359314\n",
            "Step 40670, Loss 0.7093532085418701\n",
            "Step 40675, Loss 0.7558805346488953\n",
            "Step 40680, Loss 0.660317599773407\n",
            "Step 40685, Loss 0.7717019319534302\n",
            "Step 40690, Loss 0.6350293755531311\n",
            "Step 40695, Loss 0.4704905152320862\n",
            "Step 40700, Loss 0.8652738332748413\n",
            "Step 40705, Loss 0.7338736057281494\n",
            "Step 40710, Loss 0.750434935092926\n",
            "Step 40715, Loss 0.5359265804290771\n",
            "model not updated\n",
            "Starting epoch 117/160, LR = [0.01752772129209443]\n",
            "Step 40720, Loss 0.6835236549377441\n",
            "Step 40725, Loss 0.6693788766860962\n",
            "Step 40730, Loss 0.6235018372535706\n",
            "Step 40735, Loss 0.5075011849403381\n",
            "Step 40740, Loss 0.5759692192077637\n",
            "Step 40745, Loss 0.5415975451469421\n",
            "Step 40750, Loss 0.585314154624939\n",
            "Step 40755, Loss 0.5323653221130371\n",
            "Step 40760, Loss 0.5880741477012634\n",
            "Step 40765, Loss 0.5329409837722778\n",
            "Step 40770, Loss 0.5001392364501953\n",
            "Step 40775, Loss 0.6798923015594482\n",
            "Step 40780, Loss 0.5626207590103149\n",
            "Step 40785, Loss 0.6083744168281555\n",
            "Step 40790, Loss 0.583850622177124\n",
            "Step 40795, Loss 0.5747864842414856\n",
            "Step 40800, Loss 0.5336816906929016\n",
            "Step 40805, Loss 0.7548224329948425\n",
            "Step 40810, Loss 0.7710109353065491\n",
            "Step 40815, Loss 0.5942320823669434\n",
            "Step 40820, Loss 0.6852702498435974\n",
            "Step 40825, Loss 0.5526306629180908\n",
            "Step 40830, Loss 0.6884305477142334\n",
            "Step 40835, Loss 0.5840108394622803\n",
            "Step 40840, Loss 0.5538389086723328\n",
            "Step 40845, Loss 0.4914439022541046\n",
            "Step 40850, Loss 0.7176673412322998\n",
            "Step 40855, Loss 0.594093918800354\n",
            "Step 40860, Loss 0.6232092380523682\n",
            "Step 40865, Loss 0.6820970177650452\n",
            "Step 40870, Loss 0.6984338164329529\n",
            "Step 40875, Loss 0.6277640461921692\n",
            "Step 40880, Loss 0.6579416394233704\n",
            "Step 40885, Loss 0.4408564269542694\n",
            "Step 40890, Loss 0.5660851001739502\n",
            "Step 40895, Loss 0.6395891904830933\n",
            "Step 40900, Loss 0.5565399527549744\n",
            "Step 40905, Loss 0.6026734113693237\n",
            "Step 40910, Loss 0.6270286440849304\n",
            "Step 40915, Loss 0.66752028465271\n",
            "Step 40920, Loss 0.5255162119865417\n",
            "Step 40925, Loss 0.6653225421905518\n",
            "Step 40930, Loss 0.6864688396453857\n",
            "Step 40935, Loss 0.5010494589805603\n",
            "Step 40940, Loss 0.8863371014595032\n",
            "Step 40945, Loss 0.6098519563674927\n",
            "Step 40950, Loss 0.6998187303543091\n",
            "Step 40955, Loss 0.6107861399650574\n",
            "Step 40960, Loss 0.5905435681343079\n",
            "Step 40965, Loss 0.5600394010543823\n",
            "Step 40970, Loss 0.6588586568832397\n",
            "Step 40975, Loss 0.7165915966033936\n",
            "Step 40980, Loss 0.5808784365653992\n",
            "Step 40985, Loss 0.653123676776886\n",
            "Step 40990, Loss 0.7703619003295898\n",
            "Step 40995, Loss 0.7438002228736877\n",
            "Step 41000, Loss 0.6530922651290894\n",
            "Step 41005, Loss 0.8585866093635559\n",
            "Step 41010, Loss 0.6421496868133545\n",
            "Step 41015, Loss 0.6734784841537476\n",
            "Step 41020, Loss 0.5018165111541748\n",
            "Step 41025, Loss 0.6437191963195801\n",
            "Step 41030, Loss 0.634997546672821\n",
            "Step 41035, Loss 0.6998047232627869\n",
            "Step 41040, Loss 0.521503746509552\n",
            "Step 41045, Loss 0.7093253135681152\n",
            "Step 41050, Loss 0.6649870276451111\n",
            "Step 41055, Loss 0.5671784281730652\n",
            "Step 41060, Loss 0.5813385844230652\n",
            "Step 41065, Loss 0.722684383392334\n",
            "model not updated\n",
            "Starting epoch 118/160, LR = [0.016787502923368742]\n",
            "Step 41070, Loss 0.6705135703086853\n",
            "Step 41075, Loss 0.657451868057251\n",
            "Step 41080, Loss 0.5350639820098877\n",
            "Step 41085, Loss 0.5498563647270203\n",
            "Step 41090, Loss 0.5307542681694031\n",
            "Step 41095, Loss 0.5161341428756714\n",
            "Step 41100, Loss 0.4110151529312134\n",
            "Step 41105, Loss 0.49695903062820435\n",
            "Step 41110, Loss 0.5589281320571899\n",
            "Step 41115, Loss 0.6191391348838806\n",
            "Step 41120, Loss 0.537881076335907\n",
            "Step 41125, Loss 0.47387561202049255\n",
            "Step 41130, Loss 0.7120774388313293\n",
            "Step 41135, Loss 0.46435680985450745\n",
            "Step 41140, Loss 0.640891432762146\n",
            "Step 41145, Loss 0.7161191701889038\n",
            "Step 41150, Loss 0.5735899806022644\n",
            "Step 41155, Loss 0.5906142592430115\n",
            "Step 41160, Loss 0.6963163614273071\n",
            "Step 41165, Loss 0.45191824436187744\n",
            "Step 41170, Loss 0.6030170321464539\n",
            "Step 41175, Loss 0.6717340350151062\n",
            "Step 41180, Loss 0.5266343951225281\n",
            "Step 41185, Loss 0.4789368808269501\n",
            "Step 41190, Loss 0.60113525390625\n",
            "Step 41195, Loss 0.5018582940101624\n",
            "Step 41200, Loss 0.555092453956604\n",
            "Step 41205, Loss 0.5624986886978149\n",
            "Step 41210, Loss 0.48081061244010925\n",
            "Step 41215, Loss 0.5907762050628662\n",
            "Step 41220, Loss 0.784554660320282\n",
            "Step 41225, Loss 0.47147035598754883\n",
            "Step 41230, Loss 0.6997743248939514\n",
            "Step 41235, Loss 0.5201455354690552\n",
            "Step 41240, Loss 0.4570167660713196\n",
            "Step 41245, Loss 0.5777308940887451\n",
            "Step 41250, Loss 0.5466607809066772\n",
            "Step 41255, Loss 0.6416051387786865\n",
            "Step 41260, Loss 0.7163291573524475\n",
            "Step 41265, Loss 0.7425894737243652\n",
            "Step 41270, Loss 0.5737364888191223\n",
            "Step 41275, Loss 0.7649877667427063\n",
            "Step 41280, Loss 0.6084665060043335\n",
            "Step 41285, Loss 0.5757488012313843\n",
            "Step 41290, Loss 0.42264440655708313\n",
            "Step 41295, Loss 0.6814666390419006\n",
            "Step 41300, Loss 0.5836778879165649\n",
            "Step 41305, Loss 0.5625841021537781\n",
            "Step 41310, Loss 0.4470197856426239\n",
            "Step 41315, Loss 0.48859521746635437\n",
            "Step 41320, Loss 0.5509486198425293\n",
            "Step 41325, Loss 0.8369571566581726\n",
            "Step 41330, Loss 0.5056905150413513\n",
            "Step 41335, Loss 0.5803033113479614\n",
            "Step 41340, Loss 0.6702166795730591\n",
            "Step 41345, Loss 0.6182588934898376\n",
            "Step 41350, Loss 0.5209255814552307\n",
            "Step 41355, Loss 0.6758881211280823\n",
            "Step 41360, Loss 0.6486172080039978\n",
            "Step 41365, Loss 0.5974728465080261\n",
            "Step 41370, Loss 0.5890592932701111\n",
            "Step 41375, Loss 0.627418041229248\n",
            "Step 41380, Loss 0.4146970212459564\n",
            "Step 41385, Loss 0.5928189754486084\n",
            "Step 41390, Loss 0.7328441739082336\n",
            "Step 41395, Loss 0.587259829044342\n",
            "Step 41400, Loss 0.5015387535095215\n",
            "Step 41405, Loss 0.5620311498641968\n",
            "Step 41410, Loss 0.5386101603507996\n",
            "Step 41415, Loss 0.6245232820510864\n",
            "model not updated\n",
            "Starting epoch 119/160, LR = [0.016060088633408822]\n",
            "Step 41420, Loss 0.5356878638267517\n",
            "Step 41425, Loss 0.5244441032409668\n",
            "Step 41430, Loss 0.40774303674697876\n",
            "Step 41435, Loss 0.45629796385765076\n",
            "Step 41440, Loss 0.5914649963378906\n",
            "Step 41445, Loss 0.6187694668769836\n",
            "Step 41450, Loss 0.548152506351471\n",
            "Step 41455, Loss 0.41432783007621765\n",
            "Step 41460, Loss 0.5324283838272095\n",
            "Step 41465, Loss 0.8943619728088379\n",
            "Step 41470, Loss 0.6413646936416626\n",
            "Step 41475, Loss 0.612602949142456\n",
            "Step 41480, Loss 0.46433719992637634\n",
            "Step 41485, Loss 0.6551139950752258\n",
            "Step 41490, Loss 0.47323665022850037\n",
            "Step 41495, Loss 0.4038223624229431\n",
            "Step 41500, Loss 0.5892426371574402\n",
            "Step 41505, Loss 0.7159625887870789\n",
            "Step 41510, Loss 0.5782705545425415\n",
            "Step 41515, Loss 0.5766246318817139\n",
            "Step 41520, Loss 0.6463949084281921\n",
            "Step 41525, Loss 0.613490104675293\n",
            "Step 41530, Loss 0.6197208166122437\n",
            "Step 41535, Loss 0.5308127403259277\n",
            "Step 41540, Loss 0.73780757188797\n",
            "Step 41545, Loss 0.5420181751251221\n",
            "Step 41550, Loss 0.7251733541488647\n",
            "Step 41555, Loss 0.4795793294906616\n",
            "Step 41560, Loss 0.5425081849098206\n",
            "Step 41565, Loss 0.5815180540084839\n",
            "Step 41570, Loss 0.6039430499076843\n",
            "Step 41575, Loss 0.4660443663597107\n",
            "Step 41580, Loss 0.6501820087432861\n",
            "Step 41585, Loss 0.5411684513092041\n",
            "Step 41590, Loss 0.6235236525535583\n",
            "Step 41595, Loss 0.4439801871776581\n",
            "Step 41600, Loss 0.6170898079872131\n",
            "Step 41605, Loss 0.7203777432441711\n",
            "Step 41610, Loss 0.513426661491394\n",
            "Step 41615, Loss 0.6624538898468018\n",
            "Step 41620, Loss 0.43475088477134705\n",
            "Step 41625, Loss 0.6553971171379089\n",
            "Step 41630, Loss 0.47756442427635193\n",
            "Step 41635, Loss 0.5197805166244507\n",
            "Step 41640, Loss 0.6390016674995422\n",
            "Step 41645, Loss 0.3915315568447113\n",
            "Step 41650, Loss 0.6546003818511963\n",
            "Step 41655, Loss 0.495448499917984\n",
            "Step 41660, Loss 0.7925848364830017\n",
            "Step 41665, Loss 0.4384550154209137\n",
            "Step 41670, Loss 0.5781605839729309\n",
            "Step 41675, Loss 0.5044457912445068\n",
            "Step 41680, Loss 0.6462082266807556\n",
            "Step 41685, Loss 0.5807191729545593\n",
            "Step 41690, Loss 0.5059170126914978\n",
            "Step 41695, Loss 0.6218361854553223\n",
            "Step 41700, Loss 0.5243256688117981\n",
            "Step 41705, Loss 0.5634620785713196\n",
            "Step 41710, Loss 0.5201347470283508\n",
            "Step 41715, Loss 0.6040213108062744\n",
            "Step 41720, Loss 0.4468681514263153\n",
            "Step 41725, Loss 0.7476670742034912\n",
            "Step 41730, Loss 0.5899149775505066\n",
            "Step 41735, Loss 0.5873587131500244\n",
            "Step 41740, Loss 0.6375564932823181\n",
            "Step 41745, Loss 0.561260998249054\n",
            "Step 41750, Loss 0.7155169248580933\n",
            "Step 41755, Loss 0.5170567035675049\n",
            "Step 41760, Loss 0.6778619289398193\n",
            "Step 41765, Loss 0.5902904868125916\n",
            "model not updated\n",
            "Starting epoch 120/160, LR = [0.01534575885427039]\n",
            "Step 41770, Loss 0.45699936151504517\n",
            "Step 41775, Loss 0.5687870979309082\n",
            "Step 41780, Loss 0.5361528396606445\n",
            "Step 41785, Loss 0.6080845594406128\n",
            "Step 41790, Loss 0.5408129692077637\n",
            "Step 41795, Loss 0.5722123384475708\n",
            "Step 41800, Loss 0.6250118613243103\n",
            "Step 41805, Loss 0.4343430995941162\n",
            "Step 41810, Loss 0.5674099326133728\n",
            "Step 41815, Loss 0.5270856022834778\n",
            "Step 41820, Loss 0.46218037605285645\n",
            "Step 41825, Loss 0.7109490036964417\n",
            "Step 41830, Loss 0.438565194606781\n",
            "Step 41835, Loss 0.6568578481674194\n",
            "Step 41840, Loss 0.7287287712097168\n",
            "Step 41845, Loss 0.5590738654136658\n",
            "Step 41850, Loss 0.5402234196662903\n",
            "Step 41855, Loss 0.5645232796669006\n",
            "Step 41860, Loss 0.6210118532180786\n",
            "Step 41865, Loss 0.6631922125816345\n",
            "Step 41870, Loss 0.6678080558776855\n",
            "Step 41875, Loss 0.6713243126869202\n",
            "Step 41880, Loss 0.48607900738716125\n",
            "Step 41885, Loss 0.8204706907272339\n",
            "Step 41890, Loss 0.528531014919281\n",
            "Step 41895, Loss 0.6734334826469421\n",
            "Step 41900, Loss 0.5505348443984985\n",
            "Step 41905, Loss 0.48577994108200073\n",
            "Step 41910, Loss 0.6350393891334534\n",
            "Step 41915, Loss 0.5170206427574158\n",
            "Step 41920, Loss 0.51494300365448\n",
            "Step 41925, Loss 0.7054715156555176\n",
            "Step 41930, Loss 0.4874158203601837\n",
            "Step 41935, Loss 0.5341500043869019\n",
            "Step 41940, Loss 0.4471151828765869\n",
            "Step 41945, Loss 0.5064207911491394\n",
            "Step 41950, Loss 0.547678530216217\n",
            "Step 41955, Loss 0.6132075786590576\n",
            "Step 41960, Loss 0.5973690152168274\n",
            "Step 41965, Loss 0.5319724678993225\n",
            "Step 41970, Loss 0.4160374402999878\n",
            "Step 41975, Loss 0.7108196020126343\n",
            "Step 41980, Loss 0.4608460068702698\n",
            "Step 41985, Loss 0.594441294670105\n",
            "Step 41990, Loss 0.85966956615448\n",
            "Step 41995, Loss 0.7785006165504456\n",
            "Step 42000, Loss 0.5658005475997925\n",
            "Step 42005, Loss 0.46354663372039795\n",
            "Step 42010, Loss 0.4825260639190674\n",
            "Step 42015, Loss 0.5846911072731018\n",
            "Step 42020, Loss 0.567584753036499\n",
            "Step 42025, Loss 0.5451403260231018\n",
            "Step 42030, Loss 0.7868150472640991\n",
            "Step 42035, Loss 0.5299743413925171\n",
            "Step 42040, Loss 0.6838262677192688\n",
            "Step 42045, Loss 0.5664790868759155\n",
            "Step 42050, Loss 0.6315523386001587\n",
            "Step 42055, Loss 0.6281746029853821\n",
            "Step 42060, Loss 0.5147320628166199\n",
            "Step 42065, Loss 0.7522124648094177\n",
            "Step 42070, Loss 0.4669080674648285\n",
            "Step 42075, Loss 0.47262123227119446\n",
            "Step 42080, Loss 0.5317339301109314\n",
            "Step 42085, Loss 0.6042611002922058\n",
            "Step 42090, Loss 0.5221977233886719\n",
            "Step 42095, Loss 0.5441341400146484\n",
            "Step 42100, Loss 0.6886739730834961\n",
            "Step 42105, Loss 0.6930268406867981\n",
            "Step 42110, Loss 0.5444926023483276\n",
            "Step 42115, Loss 0.5290833711624146\n",
            "model not updated\n",
            "Starting epoch 121/160, LR = [0.014644788973681202]\n",
            "Step 42120, Loss 0.5419893860816956\n",
            "Step 42125, Loss 0.7560577988624573\n",
            "Step 42130, Loss 0.5096785426139832\n",
            "Step 42135, Loss 0.5650442838668823\n",
            "Step 42140, Loss 0.3440800905227661\n",
            "Step 42145, Loss 0.6260632872581482\n",
            "Step 42150, Loss 0.4752180576324463\n",
            "Step 42155, Loss 0.5285298228263855\n",
            "Step 42160, Loss 0.5663367509841919\n",
            "Step 42165, Loss 0.6046009659767151\n",
            "Step 42170, Loss 0.519374430179596\n",
            "Step 42175, Loss 0.45706042647361755\n",
            "Step 42180, Loss 0.7335138916969299\n",
            "Step 42185, Loss 0.4976080656051636\n",
            "Step 42190, Loss 0.5116979479789734\n",
            "Step 42195, Loss 0.516643226146698\n",
            "Step 42200, Loss 0.47185659408569336\n",
            "Step 42205, Loss 0.6401773691177368\n",
            "Step 42210, Loss 0.5541151762008667\n",
            "Step 42215, Loss 0.7098740339279175\n",
            "Step 42220, Loss 0.6416682600975037\n",
            "Step 42225, Loss 0.6232563257217407\n",
            "Step 42230, Loss 0.5163342356681824\n",
            "Step 42235, Loss 0.4460262954235077\n",
            "Step 42240, Loss 0.5361176133155823\n",
            "Step 42245, Loss 0.502017617225647\n",
            "Step 42250, Loss 0.3763538897037506\n",
            "Step 42255, Loss 0.572214663028717\n",
            "Step 42260, Loss 0.5391351580619812\n",
            "Step 42265, Loss 0.6476038098335266\n",
            "Step 42270, Loss 0.43144774436950684\n",
            "Step 42275, Loss 0.5256207585334778\n",
            "Step 42280, Loss 0.5341589450836182\n",
            "Step 42285, Loss 0.6020313501358032\n",
            "Step 42290, Loss 0.5557941198348999\n",
            "Step 42295, Loss 0.5409466028213501\n",
            "Step 42300, Loss 0.48357927799224854\n",
            "Step 42305, Loss 0.46294644474983215\n",
            "Step 42310, Loss 0.6265920996665955\n",
            "Step 42315, Loss 0.5149238705635071\n",
            "Step 42320, Loss 0.5415428280830383\n",
            "Step 42325, Loss 0.49106720089912415\n",
            "Step 42330, Loss 0.6168971061706543\n",
            "Step 42335, Loss 0.6082866787910461\n",
            "Step 42340, Loss 0.7635015249252319\n",
            "Step 42345, Loss 0.7220034599304199\n",
            "Step 42350, Loss 0.7122001051902771\n",
            "Step 42355, Loss 0.5762932300567627\n",
            "Step 42360, Loss 0.36882010102272034\n",
            "Step 42365, Loss 0.6173728704452515\n",
            "Step 42370, Loss 0.5464892387390137\n",
            "Step 42375, Loss 0.5801651477813721\n",
            "Step 42380, Loss 0.4939608573913574\n",
            "Step 42385, Loss 0.5828191637992859\n",
            "Step 42390, Loss 0.6666273474693298\n",
            "Step 42395, Loss 0.5605623722076416\n",
            "Step 42400, Loss 0.8155983090400696\n",
            "Step 42405, Loss 0.5053514242172241\n",
            "Step 42410, Loss 0.49283716082572937\n",
            "Step 42415, Loss 0.4470246434211731\n",
            "Step 42420, Loss 0.6286373138427734\n",
            "Step 42425, Loss 0.6663947701454163\n",
            "Step 42430, Loss 0.5093023180961609\n",
            "Step 42435, Loss 0.6641212105751038\n",
            "Step 42440, Loss 0.49940991401672363\n",
            "Step 42445, Loss 0.47640177607536316\n",
            "Step 42450, Loss 0.5475115776062012\n",
            "Step 42455, Loss 0.5775836110115051\n",
            "Step 42460, Loss 0.5044986009597778\n",
            "Step 42465, Loss 0.4839598536491394\n",
            "Step 42470, Loss 0.5744796395301819\n",
            "model updated\n",
            "Starting epoch 122/160, LR = [0.013957449228873799]\n",
            "Step 42475, Loss 0.5854284763336182\n",
            "Step 42480, Loss 0.4939728379249573\n",
            "Step 42485, Loss 0.5817487239837646\n",
            "Step 42490, Loss 0.4789140820503235\n",
            "Step 42495, Loss 0.39849749207496643\n",
            "Step 42500, Loss 0.5839265584945679\n",
            "Step 42505, Loss 0.6007363796234131\n",
            "Step 42510, Loss 0.40191221237182617\n",
            "Step 42515, Loss 0.4484281539916992\n",
            "Step 42520, Loss 0.4990170896053314\n",
            "Step 42525, Loss 0.5321899056434631\n",
            "Step 42530, Loss 0.466636598110199\n",
            "Step 42535, Loss 0.43201228976249695\n",
            "Step 42540, Loss 0.6124459505081177\n",
            "Step 42545, Loss 0.6857156753540039\n",
            "Step 42550, Loss 0.6009321808815002\n",
            "Step 42555, Loss 0.49847573041915894\n",
            "Step 42560, Loss 0.4718124568462372\n",
            "Step 42565, Loss 0.7982266545295715\n",
            "Step 42570, Loss 0.5611673593521118\n",
            "Step 42575, Loss 0.5627767443656921\n",
            "Step 42580, Loss 0.4860468804836273\n",
            "Step 42585, Loss 0.6093139052391052\n",
            "Step 42590, Loss 0.4835891127586365\n",
            "Step 42595, Loss 0.607050895690918\n",
            "Step 42600, Loss 0.43707314133644104\n",
            "Step 42605, Loss 0.41891971230506897\n",
            "Step 42610, Loss 0.5851991176605225\n",
            "Step 42615, Loss 0.4119036793708801\n",
            "Step 42620, Loss 0.4948779344558716\n",
            "Step 42625, Loss 0.4956482946872711\n",
            "Step 42630, Loss 0.5834659337997437\n",
            "Step 42635, Loss 0.614551305770874\n",
            "Step 42640, Loss 0.5459065437316895\n",
            "Step 42645, Loss 0.5785266757011414\n",
            "Step 42650, Loss 0.4206295609474182\n",
            "Step 42655, Loss 0.9197970032691956\n",
            "Step 42660, Loss 0.6442409753799438\n",
            "Step 42665, Loss 0.7098016142845154\n",
            "Step 42670, Loss 0.508192777633667\n",
            "Step 42675, Loss 0.7171157002449036\n",
            "Step 42680, Loss 0.5653170943260193\n",
            "Step 42685, Loss 0.5712097883224487\n",
            "Step 42690, Loss 0.7575809955596924\n",
            "Step 42695, Loss 0.43646061420440674\n",
            "Step 42700, Loss 0.551017701625824\n",
            "Step 42705, Loss 0.5670398473739624\n",
            "Step 42710, Loss 0.8206153512001038\n",
            "Step 42715, Loss 0.5774039626121521\n",
            "Step 42720, Loss 0.6380954384803772\n",
            "Step 42725, Loss 0.5045251250267029\n",
            "Step 42730, Loss 0.5281946659088135\n",
            "Step 42735, Loss 0.6433568000793457\n",
            "Step 42740, Loss 0.6672366857528687\n",
            "Step 42745, Loss 0.5427950620651245\n",
            "Step 42750, Loss 0.5674911737442017\n",
            "Step 42755, Loss 0.7595945000648499\n",
            "Step 42760, Loss 0.6981620788574219\n",
            "Step 42765, Loss 0.648398756980896\n",
            "Step 42770, Loss 0.6063472032546997\n",
            "Step 42775, Loss 0.4273488521575928\n",
            "Step 42780, Loss 0.5042073130607605\n",
            "Step 42785, Loss 0.5477453470230103\n",
            "Step 42790, Loss 0.6176414489746094\n",
            "Step 42795, Loss 0.48956534266471863\n",
            "Step 42800, Loss 0.624681293964386\n",
            "Step 42805, Loss 0.7120769619941711\n",
            "Step 42810, Loss 0.7510218620300293\n",
            "Step 42815, Loss 0.4331991374492645\n",
            "Step 42820, Loss 0.5005660653114319\n",
            "model not updated\n",
            "Starting epoch 123/160, LR = [0.013284004602403912]\n",
            "Step 42825, Loss 0.5309972763061523\n",
            "Step 42830, Loss 0.48775553703308105\n",
            "Step 42835, Loss 0.7200055122375488\n",
            "Step 42840, Loss 0.574890673160553\n",
            "Step 42845, Loss 0.6282957196235657\n",
            "Step 42850, Loss 0.5727080702781677\n",
            "Step 42855, Loss 0.5156008005142212\n",
            "Step 42860, Loss 0.428735613822937\n",
            "Step 42865, Loss 0.3883347511291504\n",
            "Step 42870, Loss 0.5415141582489014\n",
            "Step 42875, Loss 0.5885664820671082\n",
            "Step 42880, Loss 0.42851004004478455\n",
            "Step 42885, Loss 0.49693647027015686\n",
            "Step 42890, Loss 0.6696736812591553\n",
            "Step 42895, Loss 0.4968092441558838\n",
            "Step 42900, Loss 0.4582612216472626\n",
            "Step 42905, Loss 0.6116790175437927\n",
            "Step 42910, Loss 0.4003313183784485\n",
            "Step 42915, Loss 0.6768808364868164\n",
            "Step 42920, Loss 0.4540629982948303\n",
            "Step 42925, Loss 0.4997747242450714\n",
            "Step 42930, Loss 0.4997102916240692\n",
            "Step 42935, Loss 0.6410261988639832\n",
            "Step 42940, Loss 0.5695139765739441\n",
            "Step 42945, Loss 0.37286177277565\n",
            "Step 42950, Loss 0.42155662178993225\n",
            "Step 42955, Loss 0.5673797130584717\n",
            "Step 42960, Loss 0.5645673274993896\n",
            "Step 42965, Loss 0.5073987245559692\n",
            "Step 42970, Loss 0.5321985483169556\n",
            "Step 42975, Loss 0.5425803065299988\n",
            "Step 42980, Loss 0.610817015171051\n",
            "Step 42985, Loss 0.5944868922233582\n",
            "Step 42990, Loss 0.5120694041252136\n",
            "Step 42995, Loss 0.5663729310035706\n",
            "Step 43000, Loss 0.600146472454071\n",
            "Step 43005, Loss 0.532690703868866\n",
            "Step 43010, Loss 0.4867491126060486\n",
            "Step 43015, Loss 0.5793367624282837\n",
            "Step 43020, Loss 0.5323882102966309\n",
            "Step 43025, Loss 0.44826406240463257\n",
            "Step 43030, Loss 0.6365254521369934\n",
            "Step 43035, Loss 0.5630489587783813\n",
            "Step 43040, Loss 0.5419839024543762\n",
            "Step 43045, Loss 0.5315691828727722\n",
            "Step 43050, Loss 0.7197386622428894\n",
            "Step 43055, Loss 0.5602782964706421\n",
            "Step 43060, Loss 0.6694835424423218\n",
            "Step 43065, Loss 0.4332004189491272\n",
            "Step 43070, Loss 0.3926394283771515\n",
            "Step 43075, Loss 0.6190668940544128\n",
            "Step 43080, Loss 0.6243460774421692\n",
            "Step 43085, Loss 0.4782722294330597\n",
            "Step 43090, Loss 0.3575381338596344\n",
            "Step 43095, Loss 0.5695805549621582\n",
            "Step 43100, Loss 0.4099835157394409\n",
            "Step 43105, Loss 0.4476451575756073\n",
            "Step 43110, Loss 0.49515509605407715\n",
            "Step 43115, Loss 0.5087692737579346\n",
            "Step 43120, Loss 0.5399603843688965\n",
            "Step 43125, Loss 0.5278550386428833\n",
            "Step 43130, Loss 0.6036855578422546\n",
            "Step 43135, Loss 0.4456367790699005\n",
            "Step 43140, Loss 0.6637550592422485\n",
            "Step 43145, Loss 0.6208328008651733\n",
            "Step 43150, Loss 0.6706157326698303\n",
            "Step 43155, Loss 0.6419678926467896\n",
            "Step 43160, Loss 0.6344950795173645\n",
            "Step 43165, Loss 0.4147856831550598\n",
            "Step 43170, Loss 0.4932844638824463\n",
            "model not updated\n",
            "Starting epoch 124/160, LR = [0.012624714719994672]\n",
            "Step 43175, Loss 0.48305651545524597\n",
            "Step 43180, Loss 0.6608206629753113\n",
            "Step 43185, Loss 0.5863869786262512\n",
            "Step 43190, Loss 0.5666201710700989\n",
            "Step 43195, Loss 0.3296278715133667\n",
            "Step 43200, Loss 0.4755309820175171\n",
            "Step 43205, Loss 0.5067748427391052\n",
            "Step 43210, Loss 0.4066742956638336\n",
            "Step 43215, Loss 0.6509199142456055\n",
            "Step 43220, Loss 0.5844269394874573\n",
            "Step 43225, Loss 0.5298709869384766\n",
            "Step 43230, Loss 0.44884413480758667\n",
            "Step 43235, Loss 0.4173620343208313\n",
            "Step 43240, Loss 0.4285314977169037\n",
            "Step 43245, Loss 0.5490931272506714\n",
            "Step 43250, Loss 0.48876306414604187\n",
            "Step 43255, Loss 0.5217065215110779\n",
            "Step 43260, Loss 0.6065133810043335\n",
            "Step 43265, Loss 0.5027256011962891\n",
            "Step 43270, Loss 0.46281373500823975\n",
            "Step 43275, Loss 0.4849233031272888\n",
            "Step 43280, Loss 0.4338573217391968\n",
            "Step 43285, Loss 0.461932897567749\n",
            "Step 43290, Loss 0.49876871705055237\n",
            "Step 43295, Loss 0.628823459148407\n",
            "Step 43300, Loss 0.4486215114593506\n",
            "Step 43305, Loss 0.5801959037780762\n",
            "Step 43310, Loss 0.6512966156005859\n",
            "Step 43315, Loss 0.5486336946487427\n",
            "Step 43320, Loss 0.5277509689331055\n",
            "Step 43325, Loss 0.5985683798789978\n",
            "Step 43330, Loss 0.5803415179252625\n",
            "Step 43335, Loss 0.3954278230667114\n",
            "Step 43340, Loss 0.4982996881008148\n",
            "Step 43345, Loss 0.40091755986213684\n",
            "Step 43350, Loss 0.3929586410522461\n",
            "Step 43355, Loss 0.6822216510772705\n",
            "Step 43360, Loss 0.3789038062095642\n",
            "Step 43365, Loss 0.5057951807975769\n",
            "Step 43370, Loss 0.6291468143463135\n",
            "Step 43375, Loss 0.47344425320625305\n",
            "Step 43380, Loss 0.48342257738113403\n",
            "Step 43385, Loss 0.6174672245979309\n",
            "Step 43390, Loss 0.5048784017562866\n",
            "Step 43395, Loss 0.4834226667881012\n",
            "Step 43400, Loss 0.5750411748886108\n",
            "Step 43405, Loss 0.3431759476661682\n",
            "Step 43410, Loss 0.4136713147163391\n",
            "Step 43415, Loss 0.5642934441566467\n",
            "Step 43420, Loss 0.6828135848045349\n",
            "Step 43425, Loss 0.5357570648193359\n",
            "Step 43430, Loss 0.4932911992073059\n",
            "Step 43435, Loss 0.40636712312698364\n",
            "Step 43440, Loss 0.5208523273468018\n",
            "Step 43445, Loss 0.5712302327156067\n",
            "Step 43450, Loss 0.40294045209884644\n",
            "Step 43455, Loss 0.43961668014526367\n",
            "Step 43460, Loss 0.47797197103500366\n",
            "Step 43465, Loss 0.6326472163200378\n",
            "Step 43470, Loss 0.5346124768257141\n",
            "Step 43475, Loss 0.558361291885376\n",
            "Step 43480, Loss 0.5853148698806763\n",
            "Step 43485, Loss 0.5417653918266296\n",
            "Step 43490, Loss 0.4427923560142517\n",
            "Step 43495, Loss 0.519064724445343\n",
            "Step 43500, Loss 0.45926281809806824\n",
            "Step 43505, Loss 0.6106507778167725\n",
            "Step 43510, Loss 0.5353952050209045\n",
            "Step 43515, Loss 0.5366548895835876\n",
            "Step 43520, Loss 0.5735276341438293\n",
            "model not updated\n",
            "Starting epoch 125/160, LR = [0.011979833750445862]\n",
            "Step 43525, Loss 0.4353402256965637\n",
            "Step 43530, Loss 0.4038306176662445\n",
            "Step 43535, Loss 0.4056769609451294\n",
            "Step 43540, Loss 0.6022370457649231\n",
            "Step 43545, Loss 0.5054542422294617\n",
            "Step 43550, Loss 0.5493696331977844\n",
            "Step 43555, Loss 0.5405269265174866\n",
            "Step 43560, Loss 0.506101667881012\n",
            "Step 43565, Loss 0.5362205505371094\n",
            "Step 43570, Loss 0.39408770203590393\n",
            "Step 43575, Loss 0.5242071151733398\n",
            "Step 43580, Loss 0.5790852308273315\n",
            "Step 43585, Loss 0.5823788642883301\n",
            "Step 43590, Loss 0.49881958961486816\n",
            "Step 43595, Loss 0.6233794093132019\n",
            "Step 43600, Loss 0.43474531173706055\n",
            "Step 43605, Loss 0.5385640859603882\n",
            "Step 43610, Loss 0.5552800297737122\n",
            "Step 43615, Loss 0.4913627505302429\n",
            "Step 43620, Loss 0.6394769549369812\n",
            "Step 43625, Loss 0.46764829754829407\n",
            "Step 43630, Loss 0.4351937472820282\n",
            "Step 43635, Loss 0.5992906093597412\n",
            "Step 43640, Loss 0.4452948272228241\n",
            "Step 43645, Loss 0.5919342637062073\n",
            "Step 43650, Loss 0.5039742588996887\n",
            "Step 43655, Loss 0.6502067446708679\n",
            "Step 43660, Loss 0.39236435294151306\n",
            "Step 43665, Loss 0.602330207824707\n",
            "Step 43670, Loss 0.4722861051559448\n",
            "Step 43675, Loss 0.40474632382392883\n",
            "Step 43680, Loss 0.5427248477935791\n",
            "Step 43685, Loss 0.585270881652832\n",
            "Step 43690, Loss 0.4104529917240143\n",
            "Step 43695, Loss 0.5900078415870667\n",
            "Step 43700, Loss 0.4237280786037445\n",
            "Step 43705, Loss 0.4972202479839325\n",
            "Step 43710, Loss 0.4844447672367096\n",
            "Step 43715, Loss 0.47878462076187134\n",
            "Step 43720, Loss 0.48347553610801697\n",
            "Step 43725, Loss 0.4474632143974304\n",
            "Step 43730, Loss 0.4550551772117615\n",
            "Step 43735, Loss 0.4939040243625641\n",
            "Step 43740, Loss 0.5486921668052673\n",
            "Step 43745, Loss 0.5520505905151367\n",
            "Step 43750, Loss 0.41100186109542847\n",
            "Step 43755, Loss 0.42404410243034363\n",
            "Step 43760, Loss 0.5589146018028259\n",
            "Step 43765, Loss 0.4967502951622009\n",
            "Step 43770, Loss 0.6557466387748718\n",
            "Step 43775, Loss 0.49547988176345825\n",
            "Step 43780, Loss 0.5514338612556458\n",
            "Step 43785, Loss 0.5132980942726135\n",
            "Step 43790, Loss 0.6391199231147766\n",
            "Step 43795, Loss 0.5010663270950317\n",
            "Step 43800, Loss 0.3124791085720062\n",
            "Step 43805, Loss 0.5132426023483276\n",
            "Step 43810, Loss 0.5198387503623962\n",
            "Step 43815, Loss 0.44081202149391174\n",
            "Step 43820, Loss 0.46551963686943054\n",
            "Step 43825, Loss 0.4951857030391693\n",
            "Step 43830, Loss 0.5007330775260925\n",
            "Step 43835, Loss 0.6469743251800537\n",
            "Step 43840, Loss 0.4076465964317322\n",
            "Step 43845, Loss 0.38075631856918335\n",
            "Step 43850, Loss 0.5276238322257996\n",
            "Step 43855, Loss 0.5513305068016052\n",
            "Step 43860, Loss 0.5866014957427979\n",
            "Step 43865, Loss 0.610988199710846\n",
            "Step 43870, Loss 0.4738479256629944\n",
            "model not updated\n",
            "Starting epoch 126/160, LR = [0.011349610307647159]\n",
            "Step 43875, Loss 0.4484625458717346\n",
            "Step 43880, Loss 0.4397469162940979\n",
            "Step 43885, Loss 0.3821062743663788\n",
            "Step 43890, Loss 0.6126239895820618\n",
            "Step 43895, Loss 0.3762297034263611\n",
            "Step 43900, Loss 0.4378296434879303\n",
            "Step 43905, Loss 0.4204433858394623\n",
            "Step 43910, Loss 0.44122785329818726\n",
            "Step 43915, Loss 0.5020101070404053\n",
            "Step 43920, Loss 0.4056459963321686\n",
            "Step 43925, Loss 0.46697908639907837\n",
            "Step 43930, Loss 0.39121896028518677\n",
            "Step 43935, Loss 0.43678054213523865\n",
            "Step 43940, Loss 0.511657178401947\n",
            "Step 43945, Loss 0.5378681421279907\n",
            "Step 43950, Loss 0.3458792567253113\n",
            "Step 43955, Loss 0.5793508887290955\n",
            "Step 43960, Loss 0.42645207047462463\n",
            "Step 43965, Loss 0.6515156030654907\n",
            "Step 43970, Loss 0.4637240767478943\n",
            "Step 43975, Loss 0.4933006763458252\n",
            "Step 43980, Loss 0.6385656595230103\n",
            "Step 43985, Loss 0.47745534777641296\n",
            "Step 43990, Loss 0.4888542592525482\n",
            "Step 43995, Loss 0.3296053111553192\n",
            "Step 44000, Loss 0.3941654860973358\n",
            "Step 44005, Loss 0.6281512379646301\n",
            "Step 44010, Loss 0.5001047253608704\n",
            "Step 44015, Loss 0.4255410432815552\n",
            "Step 44020, Loss 0.46593570709228516\n",
            "Step 44025, Loss 0.45408809185028076\n",
            "Step 44030, Loss 0.43871134519577026\n",
            "Step 44035, Loss 0.39841267466545105\n",
            "Step 44040, Loss 0.5033127069473267\n",
            "Step 44045, Loss 0.7774252891540527\n",
            "Step 44050, Loss 0.6994971036911011\n",
            "Step 44055, Loss 0.4618223011493683\n",
            "Step 44060, Loss 0.5522424578666687\n",
            "Step 44065, Loss 0.561552882194519\n",
            "Step 44070, Loss 0.5434262156486511\n",
            "Step 44075, Loss 0.440181702375412\n",
            "Step 44080, Loss 0.4958827495574951\n",
            "Step 44085, Loss 0.42024940252304077\n",
            "Step 44090, Loss 0.5459567904472351\n",
            "Step 44095, Loss 0.4681522250175476\n",
            "Step 44100, Loss 0.4769502580165863\n",
            "Step 44105, Loss 0.6136972904205322\n",
            "Step 44110, Loss 0.39444300532341003\n",
            "Step 44115, Loss 0.4058276414871216\n",
            "Step 44120, Loss 0.6305557489395142\n",
            "Step 44125, Loss 0.5463842153549194\n",
            "Step 44130, Loss 0.5626543760299683\n",
            "Step 44135, Loss 0.5786051750183105\n",
            "Step 44140, Loss 0.4738624691963196\n",
            "Step 44145, Loss 0.5813756585121155\n",
            "Step 44150, Loss 0.5686048269271851\n",
            "Step 44155, Loss 0.5195716619491577\n",
            "Step 44160, Loss 0.44846877455711365\n",
            "Step 44165, Loss 0.6553041338920593\n",
            "Step 44170, Loss 0.45758023858070374\n",
            "Step 44175, Loss 0.46381741762161255\n",
            "Step 44180, Loss 0.6362606883049011\n",
            "Step 44185, Loss 0.5343137383460999\n",
            "Step 44190, Loss 0.3909294903278351\n",
            "Step 44195, Loss 0.5872992873191833\n",
            "Step 44200, Loss 0.46138322353363037\n",
            "Step 44205, Loss 0.6554341912269592\n",
            "Step 44210, Loss 0.5328361392021179\n",
            "Step 44215, Loss 0.636858344078064\n",
            "Step 44220, Loss 0.4940238893032074\n",
            "Step 44225, Loss 0.44996434450149536\n",
            "model not updated\n",
            "Starting epoch 127/160, LR = [0.010734287354732554]\n",
            "Step 44230, Loss 0.5537785887718201\n",
            "Step 44235, Loss 0.3836528956890106\n",
            "Step 44240, Loss 0.511684238910675\n",
            "Step 44245, Loss 0.6064826250076294\n",
            "Step 44250, Loss 0.43433523178100586\n",
            "Step 44255, Loss 0.47224125266075134\n",
            "Step 44260, Loss 0.5532474517822266\n",
            "Step 44265, Loss 0.4750604033470154\n",
            "Step 44270, Loss 0.4865070879459381\n",
            "Step 44275, Loss 0.46826639771461487\n",
            "Step 44280, Loss 0.5805248618125916\n",
            "Step 44285, Loss 0.2938438653945923\n",
            "Step 44290, Loss 0.440290629863739\n",
            "Step 44295, Loss 0.4513953924179077\n",
            "Step 44300, Loss 0.4070407450199127\n",
            "Step 44305, Loss 0.38320496678352356\n",
            "Step 44310, Loss 0.6483327150344849\n",
            "Step 44315, Loss 0.48349642753601074\n",
            "Step 44320, Loss 0.5111134052276611\n",
            "Step 44325, Loss 0.5021069049835205\n",
            "Step 44330, Loss 0.3308764398097992\n",
            "Step 44335, Loss 0.39844033122062683\n",
            "Step 44340, Loss 0.6231840252876282\n",
            "Step 44345, Loss 0.4508424699306488\n",
            "Step 44350, Loss 0.5790424942970276\n",
            "Step 44355, Loss 0.5806054472923279\n",
            "Step 44360, Loss 0.5348626375198364\n",
            "Step 44365, Loss 0.47354045510292053\n",
            "Step 44370, Loss 0.45987164974212646\n",
            "Step 44375, Loss 0.46959906816482544\n",
            "Step 44380, Loss 0.40429338812828064\n",
            "Step 44385, Loss 0.30665159225463867\n",
            "Step 44390, Loss 0.4518260061740875\n",
            "Step 44395, Loss 0.39792686700820923\n",
            "Step 44400, Loss 0.48503196239471436\n",
            "Step 44405, Loss 0.7160597443580627\n",
            "Step 44410, Loss 0.34853965044021606\n",
            "Step 44415, Loss 0.5425804257392883\n",
            "Step 44420, Loss 0.4447384476661682\n",
            "Step 44425, Loss 0.46645230054855347\n",
            "Step 44430, Loss 0.4502573311328888\n",
            "Step 44435, Loss 0.47689977288246155\n",
            "Step 44440, Loss 0.4532816410064697\n",
            "Step 44445, Loss 0.6142723560333252\n",
            "Step 44450, Loss 0.4259539842605591\n",
            "Step 44455, Loss 0.4496874511241913\n",
            "Step 44460, Loss 0.6415954232215881\n",
            "Step 44465, Loss 0.5058935880661011\n",
            "Step 44470, Loss 0.4393653869628906\n",
            "Step 44475, Loss 0.47978800535202026\n",
            "Step 44480, Loss 0.36771440505981445\n",
            "Step 44485, Loss 0.5213784575462341\n",
            "Step 44490, Loss 0.5966109037399292\n",
            "Step 44495, Loss 0.5964458584785461\n",
            "Step 44500, Loss 0.5276919603347778\n",
            "Step 44505, Loss 0.5889018774032593\n",
            "Step 44510, Loss 0.5962042212486267\n",
            "Step 44515, Loss 0.4542810022830963\n",
            "Step 44520, Loss 0.510814368724823\n",
            "Step 44525, Loss 0.5332468152046204\n",
            "Step 44530, Loss 0.4439745843410492\n",
            "Step 44535, Loss 0.5492328405380249\n",
            "Step 44540, Loss 0.5380653738975525\n",
            "Step 44545, Loss 0.6946635842323303\n",
            "Step 44550, Loss 0.437093049287796\n",
            "Step 44555, Loss 0.48391425609588623\n",
            "Step 44560, Loss 0.4688090980052948\n",
            "Step 44565, Loss 0.42873066663742065\n",
            "Step 44570, Loss 0.47537434101104736\n",
            "Step 44575, Loss 0.4306965172290802\n",
            "model updated\n",
            "Starting epoch 128/160, LR = [0.010134102110413675]\n",
            "Step 44580, Loss 0.43106260895729065\n",
            "Step 44585, Loss 0.6261940002441406\n",
            "Step 44590, Loss 0.49619925022125244\n",
            "Step 44595, Loss 0.6897990703582764\n",
            "Step 44600, Loss 0.40109050273895264\n",
            "Step 44605, Loss 0.39236196875572205\n",
            "Step 44610, Loss 0.5695819854736328\n",
            "Step 44615, Loss 0.38185301423072815\n",
            "Step 44620, Loss 0.4485664367675781\n",
            "Step 44625, Loss 0.399610310792923\n",
            "Step 44630, Loss 0.4756055474281311\n",
            "Step 44635, Loss 0.3990766704082489\n",
            "Step 44640, Loss 0.2305656522512436\n",
            "Step 44645, Loss 0.4859844744205475\n",
            "Step 44650, Loss 0.4644024074077606\n",
            "Step 44655, Loss 0.43625831604003906\n",
            "Step 44660, Loss 0.49812960624694824\n",
            "Step 44665, Loss 0.4299381375312805\n",
            "Step 44670, Loss 0.4488098621368408\n",
            "Step 44675, Loss 0.5078520774841309\n",
            "Step 44680, Loss 0.34187403321266174\n",
            "Step 44685, Loss 0.364061564207077\n",
            "Step 44690, Loss 0.4268287122249603\n",
            "Step 44695, Loss 0.44826066493988037\n",
            "Step 44700, Loss 0.5485402345657349\n",
            "Step 44705, Loss 0.341759592294693\n",
            "Step 44710, Loss 0.47809478640556335\n",
            "Step 44715, Loss 0.5473393797874451\n",
            "Step 44720, Loss 0.5361027121543884\n",
            "Step 44725, Loss 0.5272182822227478\n",
            "Step 44730, Loss 0.4920201897621155\n",
            "Step 44735, Loss 0.3699389398097992\n",
            "Step 44740, Loss 0.6109833121299744\n",
            "Step 44745, Loss 0.43547987937927246\n",
            "Step 44750, Loss 0.5841020345687866\n",
            "Step 44755, Loss 0.46433892846107483\n",
            "Step 44760, Loss 0.40237662196159363\n",
            "Step 44765, Loss 0.586750328540802\n",
            "Step 44770, Loss 0.6158392429351807\n",
            "Step 44775, Loss 0.41269057989120483\n",
            "Step 44780, Loss 0.539900004863739\n",
            "Step 44785, Loss 0.42630326747894287\n",
            "Step 44790, Loss 0.5102759003639221\n",
            "Step 44795, Loss 0.43441149592399597\n",
            "Step 44800, Loss 0.44423919916152954\n",
            "Step 44805, Loss 0.4063912332057953\n",
            "Step 44810, Loss 0.5030599236488342\n",
            "Step 44815, Loss 0.5161850452423096\n",
            "Step 44820, Loss 0.456326425075531\n",
            "Step 44825, Loss 0.4325304329395294\n",
            "Step 44830, Loss 0.5838268399238586\n",
            "Step 44835, Loss 0.4966900944709778\n",
            "Step 44840, Loss 0.6278378963470459\n",
            "Step 44845, Loss 0.6667070388793945\n",
            "Step 44850, Loss 0.5610528588294983\n",
            "Step 44855, Loss 0.5348230004310608\n",
            "Step 44860, Loss 0.5984484553337097\n",
            "Step 44865, Loss 0.5325881242752075\n",
            "Step 44870, Loss 0.5047875642776489\n",
            "Step 44875, Loss 0.49364808201789856\n",
            "Step 44880, Loss 0.44524553418159485\n",
            "Step 44885, Loss 0.5083250403404236\n",
            "Step 44890, Loss 0.5648481249809265\n",
            "Step 44895, Loss 0.6660093069076538\n",
            "Step 44900, Loss 0.5509451627731323\n",
            "Step 44905, Loss 0.5312357544898987\n",
            "Step 44910, Loss 0.506913423538208\n",
            "Step 44915, Loss 0.4420612156391144\n",
            "Step 44920, Loss 0.40991121530532837\n",
            "Step 44925, Loss 0.6926767230033875\n",
            "model not updated\n",
            "Starting epoch 129/160, LR = [0.009549285957527204]\n",
            "Step 44930, Loss 0.5862119793891907\n",
            "Step 44935, Loss 0.4181155562400818\n",
            "Step 44940, Loss 0.4319785237312317\n",
            "Step 44945, Loss 0.3933505713939667\n",
            "Step 44950, Loss 0.43309327960014343\n",
            "Step 44955, Loss 0.40935149788856506\n",
            "Step 44960, Loss 0.4302147924900055\n",
            "Step 44965, Loss 0.49301081895828247\n",
            "Step 44970, Loss 0.45397651195526123\n",
            "Step 44975, Loss 0.4842793047428131\n",
            "Step 44980, Loss 0.46052515506744385\n",
            "Step 44985, Loss 0.45649054646492004\n",
            "Step 44990, Loss 0.3843041658401489\n",
            "Step 44995, Loss 0.4227636456489563\n",
            "Step 45000, Loss 0.5071114897727966\n",
            "Step 45005, Loss 0.38990089297294617\n",
            "Step 45010, Loss 0.49695858359336853\n",
            "Step 45015, Loss 0.3681095540523529\n",
            "Step 45020, Loss 0.4255179166793823\n",
            "Step 45025, Loss 0.4564860761165619\n",
            "Step 45030, Loss 0.48771244287490845\n",
            "Step 45035, Loss 0.5265505909919739\n",
            "Step 45040, Loss 0.6015412211418152\n",
            "Step 45045, Loss 0.5298938751220703\n",
            "Step 45050, Loss 0.5293465256690979\n",
            "Step 45055, Loss 0.44041377305984497\n",
            "Step 45060, Loss 0.3870113790035248\n",
            "Step 45065, Loss 0.46990975737571716\n",
            "Step 45070, Loss 0.41598156094551086\n",
            "Step 45075, Loss 0.5129396319389343\n",
            "Step 45080, Loss 0.4273819923400879\n",
            "Step 45085, Loss 0.4576385021209717\n",
            "Step 45090, Loss 0.6068033576011658\n",
            "Step 45095, Loss 0.5013998746871948\n",
            "Step 45100, Loss 0.7003591656684875\n",
            "Step 45105, Loss 0.3224996328353882\n",
            "Step 45110, Loss 0.43767911195755005\n",
            "Step 45115, Loss 0.41001155972480774\n",
            "Step 45120, Loss 0.5683789849281311\n",
            "Step 45125, Loss 0.46423667669296265\n",
            "Step 45130, Loss 0.44249558448791504\n",
            "Step 45135, Loss 0.42892587184906006\n",
            "Step 45140, Loss 0.34630176424980164\n",
            "Step 45145, Loss 0.4015233516693115\n",
            "Step 45150, Loss 0.433368980884552\n",
            "Step 45155, Loss 0.429447740316391\n",
            "Step 45160, Loss 0.3578764498233795\n",
            "Step 45165, Loss 0.572422444820404\n",
            "Step 45170, Loss 0.49587196111679077\n",
            "Step 45175, Loss 0.31650784611701965\n",
            "Step 45180, Loss 0.37845441699028015\n",
            "Step 45185, Loss 0.5300412774085999\n",
            "Step 45190, Loss 0.6114445328712463\n",
            "Step 45195, Loss 0.646803617477417\n",
            "Step 45200, Loss 0.6312506794929504\n",
            "Step 45205, Loss 0.49652236700057983\n",
            "Step 45210, Loss 0.4887285530567169\n",
            "Step 45215, Loss 0.5309514403343201\n",
            "Step 45220, Loss 0.4533845782279968\n",
            "Step 45225, Loss 0.4738820493221283\n",
            "Step 45230, Loss 0.4207002818584442\n",
            "Step 45235, Loss 0.5455877780914307\n",
            "Step 45240, Loss 0.6606232523918152\n",
            "Step 45245, Loss 0.5491547584533691\n",
            "Step 45250, Loss 0.4602505564689636\n",
            "Step 45255, Loss 0.5584391951560974\n",
            "Step 45260, Loss 0.6013318300247192\n",
            "Step 45265, Loss 0.41048353910446167\n",
            "Step 45270, Loss 0.4314485788345337\n",
            "Step 45275, Loss 0.5563656091690063\n",
            "model not updated\n",
            "Starting epoch 130/160, LR = [0.008980064353832579]\n",
            "Step 45280, Loss 0.4395729899406433\n",
            "Step 45285, Loss 0.3214119076728821\n",
            "Step 45290, Loss 0.3191712498664856\n",
            "Step 45295, Loss 0.33385178446769714\n",
            "Step 45300, Loss 0.46375203132629395\n",
            "Step 45305, Loss 0.44184669852256775\n",
            "Step 45310, Loss 0.4359144866466522\n",
            "Step 45315, Loss 0.4992442727088928\n",
            "Step 45320, Loss 0.6151556968688965\n",
            "Step 45325, Loss 0.5545585751533508\n",
            "Step 45330, Loss 0.4806433618068695\n",
            "Step 45335, Loss 0.5219755172729492\n",
            "Step 45340, Loss 0.43196040391921997\n",
            "Step 45345, Loss 0.5658334493637085\n",
            "Step 45350, Loss 0.5460757613182068\n",
            "Step 45355, Loss 0.41576120257377625\n",
            "Step 45360, Loss 0.47701162099838257\n",
            "Step 45365, Loss 0.4495150148868561\n",
            "Step 45370, Loss 0.6569827795028687\n",
            "Step 45375, Loss 0.3958660066127777\n",
            "Step 45380, Loss 0.40381452441215515\n",
            "Step 45385, Loss 0.5853661298751831\n",
            "Step 45390, Loss 0.5581589341163635\n",
            "Step 45395, Loss 0.4670695960521698\n",
            "Step 45400, Loss 0.6806755661964417\n",
            "Step 45405, Loss 0.5037828087806702\n",
            "Step 45410, Loss 0.3785873353481293\n",
            "Step 45415, Loss 0.5298190712928772\n",
            "Step 45420, Loss 0.47838062047958374\n",
            "Step 45425, Loss 0.5147103071212769\n",
            "Step 45430, Loss 0.4869847595691681\n",
            "Step 45435, Loss 0.4238622784614563\n",
            "Step 45440, Loss 0.41743525862693787\n",
            "Step 45445, Loss 0.36330297589302063\n",
            "Step 45450, Loss 0.6575751304626465\n",
            "Step 45455, Loss 0.571226179599762\n",
            "Step 45460, Loss 0.4836728274822235\n",
            "Step 45465, Loss 0.49932900071144104\n",
            "Step 45470, Loss 0.5765460729598999\n",
            "Step 45475, Loss 0.5096563696861267\n",
            "Step 45480, Loss 0.5019183158874512\n",
            "Step 45485, Loss 0.4676905870437622\n",
            "Step 45490, Loss 0.583737850189209\n",
            "Step 45495, Loss 0.6061834692955017\n",
            "Step 45500, Loss 0.3492698669433594\n",
            "Step 45505, Loss 0.4981352388858795\n",
            "Step 45510, Loss 0.5011399984359741\n",
            "Step 45515, Loss 0.37514054775238037\n",
            "Step 45520, Loss 0.6546052694320679\n",
            "Step 45525, Loss 0.5566785335540771\n",
            "Step 45530, Loss 0.4881742596626282\n",
            "Step 45535, Loss 0.40590900182724\n",
            "Step 45540, Loss 0.39387357234954834\n",
            "Step 45545, Loss 0.5126352906227112\n",
            "Step 45550, Loss 0.5466440916061401\n",
            "Step 45555, Loss 0.5785802602767944\n",
            "Step 45560, Loss 0.3913571834564209\n",
            "Step 45565, Loss 0.4736889898777008\n",
            "Step 45570, Loss 0.4882622957229614\n",
            "Step 45575, Loss 0.35766154527664185\n",
            "Step 45580, Loss 0.28534558415412903\n",
            "Step 45585, Loss 0.5517537593841553\n",
            "Step 45590, Loss 0.40052881836891174\n",
            "Step 45595, Loss 0.3590276837348938\n",
            "Step 45600, Loss 0.46993789076805115\n",
            "Step 45605, Loss 0.44606977701187134\n",
            "Step 45610, Loss 0.4327496588230133\n",
            "Step 45615, Loss 0.5203402638435364\n",
            "Step 45620, Loss 0.49868258833885193\n",
            "Step 45625, Loss 0.5446466207504272\n",
            "model not updated\n",
            "Starting epoch 131/160, LR = [0.008426656745093665]\n",
            "Step 45630, Loss 0.40324437618255615\n",
            "Step 45635, Loss 0.49999770522117615\n",
            "Step 45640, Loss 0.418474942445755\n",
            "Step 45645, Loss 0.5138120055198669\n",
            "Step 45650, Loss 0.44834256172180176\n",
            "Step 45655, Loss 0.4453432857990265\n",
            "Step 45660, Loss 0.3959164619445801\n",
            "Step 45665, Loss 0.36608484387397766\n",
            "Step 45670, Loss 0.3956397771835327\n",
            "Step 45675, Loss 0.5770689249038696\n",
            "Step 45680, Loss 0.39506369829177856\n",
            "Step 45685, Loss 0.6212630271911621\n",
            "Step 45690, Loss 0.4633015990257263\n",
            "Step 45695, Loss 0.457464337348938\n",
            "Step 45700, Loss 0.4340053200721741\n",
            "Step 45705, Loss 0.5185891389846802\n",
            "Step 45710, Loss 0.4712188243865967\n",
            "Step 45715, Loss 0.5218058228492737\n",
            "Step 45720, Loss 0.3310334086418152\n",
            "Step 45725, Loss 0.3816426694393158\n",
            "Step 45730, Loss 0.442523330450058\n",
            "Step 45735, Loss 0.3438335359096527\n",
            "Step 45740, Loss 0.41361865401268005\n",
            "Step 45745, Loss 0.4713083505630493\n",
            "Step 45750, Loss 0.43625640869140625\n",
            "Step 45755, Loss 0.28441891074180603\n",
            "Step 45760, Loss 0.5183247327804565\n",
            "Step 45765, Loss 0.6019356846809387\n",
            "Step 45770, Loss 0.4095716178417206\n",
            "Step 45775, Loss 0.4104272723197937\n",
            "Step 45780, Loss 0.37237000465393066\n",
            "Step 45785, Loss 0.540366530418396\n",
            "Step 45790, Loss 0.4364655911922455\n",
            "Step 45795, Loss 0.3833140730857849\n",
            "Step 45800, Loss 0.5392410159111023\n",
            "Step 45805, Loss 0.5403345227241516\n",
            "Step 45810, Loss 0.3234904408454895\n",
            "Step 45815, Loss 0.48176077008247375\n",
            "Step 45820, Loss 0.4245299994945526\n",
            "Step 45825, Loss 0.5089441537857056\n",
            "Step 45830, Loss 0.36942151188850403\n",
            "Step 45835, Loss 0.4746369421482086\n",
            "Step 45840, Loss 0.427076131105423\n",
            "Step 45845, Loss 0.5244964361190796\n",
            "Step 45850, Loss 0.555350661277771\n",
            "Step 45855, Loss 0.41470789909362793\n",
            "Step 45860, Loss 0.4460032880306244\n",
            "Step 45865, Loss 0.6157350540161133\n",
            "Step 45870, Loss 0.5138459205627441\n",
            "Step 45875, Loss 0.5155462622642517\n",
            "Step 45880, Loss 0.6346213817596436\n",
            "Step 45885, Loss 0.34504377841949463\n",
            "Step 45890, Loss 0.530850350856781\n",
            "Step 45895, Loss 0.48848941922187805\n",
            "Step 45900, Loss 0.4263940751552582\n",
            "Step 45905, Loss 0.4475420117378235\n",
            "Step 45910, Loss 0.4835882782936096\n",
            "Step 45915, Loss 0.649944543838501\n",
            "Step 45920, Loss 0.5198265314102173\n",
            "Step 45925, Loss 0.53707355260849\n",
            "Step 45930, Loss 0.46239402890205383\n",
            "Step 45935, Loss 0.500203549861908\n",
            "Step 45940, Loss 0.3012482523918152\n",
            "Step 45945, Loss 0.42300620675086975\n",
            "Step 45950, Loss 0.40683409571647644\n",
            "Step 45955, Loss 0.5894793272018433\n",
            "Step 45960, Loss 0.5584079623222351\n",
            "Step 45965, Loss 0.5017284154891968\n",
            "Step 45970, Loss 0.5352123975753784\n",
            "Step 45975, Loss 0.4544956684112549\n",
            "Step 45980, Loss 0.53281170129776\n",
            "model not updated\n",
            "Starting epoch 132/160, LR = [0.007889276480478189]\n",
            "Step 45985, Loss 0.37996262311935425\n",
            "Step 45990, Loss 0.4861384332180023\n",
            "Step 45995, Loss 0.38940855860710144\n",
            "Step 46000, Loss 0.36884504556655884\n",
            "Step 46005, Loss 0.3696315288543701\n",
            "Step 46010, Loss 0.3926295340061188\n",
            "Step 46015, Loss 0.4621514678001404\n",
            "Step 46020, Loss 0.3956514894962311\n",
            "Step 46025, Loss 0.45838743448257446\n",
            "Step 46030, Loss 0.4411749243736267\n",
            "Step 46035, Loss 0.4303883910179138\n",
            "Step 46040, Loss 0.34308916330337524\n",
            "Step 46045, Loss 0.36114493012428284\n",
            "Step 46050, Loss 0.45209792256355286\n",
            "Step 46055, Loss 0.344073623418808\n",
            "Step 46060, Loss 0.3701280951499939\n",
            "Step 46065, Loss 0.48694419860839844\n",
            "Step 46070, Loss 0.3434496223926544\n",
            "Step 46075, Loss 0.4080688953399658\n",
            "Step 46080, Loss 0.5756399035453796\n",
            "Step 46085, Loss 0.3964928388595581\n",
            "Step 46090, Loss 0.4546175003051758\n",
            "Step 46095, Loss 0.5173023343086243\n",
            "Step 46100, Loss 0.45017117261886597\n",
            "Step 46105, Loss 0.4997025728225708\n",
            "Step 46110, Loss 0.4521450400352478\n",
            "Step 46115, Loss 0.3847120404243469\n",
            "Step 46120, Loss 0.47049763798713684\n",
            "Step 46125, Loss 0.45237472653388977\n",
            "Step 46130, Loss 0.578478217124939\n",
            "Step 46135, Loss 0.31627577543258667\n",
            "Step 46140, Loss 0.4013417065143585\n",
            "Step 46145, Loss 0.45079317688941956\n",
            "Step 46150, Loss 0.6030733585357666\n",
            "Step 46155, Loss 0.42685872316360474\n",
            "Step 46160, Loss 0.5251995921134949\n",
            "Step 46165, Loss 0.43799713253974915\n",
            "Step 46170, Loss 0.3661714494228363\n",
            "Step 46175, Loss 0.5810145735740662\n",
            "Step 46180, Loss 0.3870357573032379\n",
            "Step 46185, Loss 0.32995060086250305\n",
            "Step 46190, Loss 0.5240549445152283\n",
            "Step 46195, Loss 0.3788089454174042\n",
            "Step 46200, Loss 0.4066639244556427\n",
            "Step 46205, Loss 0.4885316789150238\n",
            "Step 46210, Loss 0.4332849979400635\n",
            "Step 46215, Loss 0.44844484329223633\n",
            "Step 46220, Loss 0.4764375686645508\n",
            "Step 46225, Loss 0.3678682744503021\n",
            "Step 46230, Loss 0.4163544774055481\n",
            "Step 46235, Loss 0.6225822567939758\n",
            "Step 46240, Loss 0.47028979659080505\n",
            "Step 46245, Loss 0.43036597967147827\n",
            "Step 46250, Loss 0.37651294469833374\n",
            "Step 46255, Loss 0.5035524368286133\n",
            "Step 46260, Loss 0.5100405216217041\n",
            "Step 46265, Loss 0.5066044926643372\n",
            "Step 46270, Loss 0.39717477560043335\n",
            "Step 46275, Loss 0.49521347880363464\n",
            "Step 46280, Loss 0.39855214953422546\n",
            "Step 46285, Loss 0.5659773945808411\n",
            "Step 46290, Loss 0.3978627324104309\n",
            "Step 46295, Loss 0.5185496807098389\n",
            "Step 46300, Loss 0.5416975617408752\n",
            "Step 46305, Loss 0.4841296076774597\n",
            "Step 46310, Loss 0.4501563608646393\n",
            "Step 46315, Loss 0.38554757833480835\n",
            "Step 46320, Loss 0.36837926506996155\n",
            "Step 46325, Loss 0.5558897256851196\n",
            "Step 46330, Loss 0.5020209550857544\n",
            "model not updated\n",
            "Starting epoch 133/160, LR = [0.00736813073030771]\n",
            "Step 46335, Loss 0.4607865512371063\n",
            "Step 46340, Loss 0.3693017363548279\n",
            "Step 46345, Loss 0.43038469552993774\n",
            "Step 46350, Loss 0.46696028113365173\n",
            "Step 46355, Loss 0.4150758981704712\n",
            "Step 46360, Loss 0.49007663130760193\n",
            "Step 46365, Loss 0.4796961545944214\n",
            "Step 46370, Loss 0.43316084146499634\n",
            "Step 46375, Loss 0.4283178746700287\n",
            "Step 46380, Loss 0.48218101263046265\n",
            "Step 46385, Loss 0.5495214462280273\n",
            "Step 46390, Loss 0.4103339612483978\n",
            "Step 46395, Loss 0.406949907541275\n",
            "Step 46400, Loss 0.5131858587265015\n",
            "Step 46405, Loss 0.4240513741970062\n",
            "Step 46410, Loss 0.4523299038410187\n",
            "Step 46415, Loss 0.3037315607070923\n",
            "Step 46420, Loss 0.38590213656425476\n",
            "Step 46425, Loss 0.6172245144844055\n",
            "Step 46430, Loss 0.5280670523643494\n",
            "Step 46435, Loss 0.5792962908744812\n",
            "Step 46440, Loss 0.5890267491340637\n",
            "Step 46445, Loss 0.43421250581741333\n",
            "Step 46450, Loss 0.4999392032623291\n",
            "Step 46455, Loss 0.4537242352962494\n",
            "Step 46460, Loss 0.3943747580051422\n",
            "Step 46465, Loss 0.38061681389808655\n",
            "Step 46470, Loss 0.3308807611465454\n",
            "Step 46475, Loss 0.35579824447631836\n",
            "Step 46480, Loss 0.42847996950149536\n",
            "Step 46485, Loss 0.5206418633460999\n",
            "Step 46490, Loss 0.5378187298774719\n",
            "Step 46495, Loss 0.5938642024993896\n",
            "Step 46500, Loss 0.49636751413345337\n",
            "Step 46505, Loss 0.3558812439441681\n",
            "Step 46510, Loss 0.4781227707862854\n",
            "Step 46515, Loss 0.4908783733844757\n",
            "Step 46520, Loss 0.40032559633255005\n",
            "Step 46525, Loss 0.4192273020744324\n",
            "Step 46530, Loss 0.4670701026916504\n",
            "Step 46535, Loss 0.41225385665893555\n",
            "Step 46540, Loss 0.4037359356880188\n",
            "Step 46545, Loss 0.354583740234375\n",
            "Step 46550, Loss 0.3853967785835266\n",
            "Step 46555, Loss 0.47801312804222107\n",
            "Step 46560, Loss 0.4247089624404907\n",
            "Step 46565, Loss 0.4715576767921448\n",
            "Step 46570, Loss 0.4725973904132843\n",
            "Step 46575, Loss 0.4271850883960724\n",
            "Step 46580, Loss 0.4031009376049042\n",
            "Step 46585, Loss 0.5879303216934204\n",
            "Step 46590, Loss 0.5354393124580383\n",
            "Step 46595, Loss 0.5107764601707458\n",
            "Step 46600, Loss 0.42467522621154785\n",
            "Step 46605, Loss 0.44607168436050415\n",
            "Step 46610, Loss 0.5579197406768799\n",
            "Step 46615, Loss 0.521726131439209\n",
            "Step 46620, Loss 0.4615741968154907\n",
            "Step 46625, Loss 0.41180482506752014\n",
            "Step 46630, Loss 0.3480754494667053\n",
            "Step 46635, Loss 0.410350501537323\n",
            "Step 46640, Loss 0.460286408662796\n",
            "Step 46645, Loss 0.5268408060073853\n",
            "Step 46650, Loss 0.5467334985733032\n",
            "Step 46655, Loss 0.4024236500263214\n",
            "Step 46660, Loss 0.4179777204990387\n",
            "Step 46665, Loss 0.4182235598564148\n",
            "Step 46670, Loss 0.49256300926208496\n",
            "Step 46675, Loss 0.4123370051383972\n",
            "Step 46680, Loss 0.5825495719909668\n",
            "model not updated\n",
            "Starting epoch 134/160, LR = [0.006863420406189349]\n",
            "Step 46685, Loss 0.48631420731544495\n",
            "Step 46690, Loss 0.45475804805755615\n",
            "Step 46695, Loss 0.4300088882446289\n",
            "Step 46700, Loss 0.37492936849594116\n",
            "Step 46705, Loss 0.391418993473053\n",
            "Step 46710, Loss 0.342587947845459\n",
            "Step 46715, Loss 0.426768034696579\n",
            "Step 46720, Loss 0.498466819524765\n",
            "Step 46725, Loss 0.39749372005462646\n",
            "Step 46730, Loss 0.4331592917442322\n",
            "Step 46735, Loss 0.44014281034469604\n",
            "Step 46740, Loss 0.2478466033935547\n",
            "Step 46745, Loss 0.2807477116584778\n",
            "Step 46750, Loss 0.4103708863258362\n",
            "Step 46755, Loss 0.3873752951622009\n",
            "Step 46760, Loss 0.40834319591522217\n",
            "Step 46765, Loss 0.458706259727478\n",
            "Step 46770, Loss 0.38588476181030273\n",
            "Step 46775, Loss 0.311982661485672\n",
            "Step 46780, Loss 0.5058081150054932\n",
            "Step 46785, Loss 0.3576524257659912\n",
            "Step 46790, Loss 0.5932062864303589\n",
            "Step 46795, Loss 0.5424063801765442\n",
            "Step 46800, Loss 0.3605784773826599\n",
            "Step 46805, Loss 0.46680185198783875\n",
            "Step 46810, Loss 0.44940873980522156\n",
            "Step 46815, Loss 0.37784188985824585\n",
            "Step 46820, Loss 0.4041858911514282\n",
            "Step 46825, Loss 0.4011540114879608\n",
            "Step 46830, Loss 0.5931804180145264\n",
            "Step 46835, Loss 0.38865700364112854\n",
            "Step 46840, Loss 0.4229156970977783\n",
            "Step 46845, Loss 0.45545682311058044\n",
            "Step 46850, Loss 0.5986782312393188\n",
            "Step 46855, Loss 0.49840620160102844\n",
            "Step 46860, Loss 0.5319976210594177\n",
            "Step 46865, Loss 0.41357433795928955\n",
            "Step 46870, Loss 0.42005205154418945\n",
            "Step 46875, Loss 0.421994149684906\n",
            "Step 46880, Loss 0.45012542605400085\n",
            "Step 46885, Loss 0.3267748951911926\n",
            "Step 46890, Loss 0.48404768109321594\n",
            "Step 46895, Loss 0.4867832660675049\n",
            "Step 46900, Loss 0.4891534745693207\n",
            "Step 46905, Loss 0.48563647270202637\n",
            "Step 46910, Loss 0.37766000628471375\n",
            "Step 46915, Loss 0.3870806097984314\n",
            "Step 46920, Loss 0.40253350138664246\n",
            "Step 46925, Loss 0.48842090368270874\n",
            "Step 46930, Loss 0.42878812551498413\n",
            "Step 46935, Loss 0.5179201364517212\n",
            "Step 46940, Loss 0.32307618856430054\n",
            "Step 46945, Loss 0.3491317927837372\n",
            "Step 46950, Loss 0.42106518149375916\n",
            "Step 46955, Loss 0.431424617767334\n",
            "Step 46960, Loss 0.3166482150554657\n",
            "Step 46965, Loss 0.5072500705718994\n",
            "Step 46970, Loss 0.506432831287384\n",
            "Step 46975, Loss 0.47703883051872253\n",
            "Step 46980, Loss 0.4259689748287201\n",
            "Step 46985, Loss 0.4271225929260254\n",
            "Step 46990, Loss 0.42339766025543213\n",
            "Step 46995, Loss 0.4722927510738373\n",
            "Step 47000, Loss 0.3427444100379944\n",
            "Step 47005, Loss 0.6400353312492371\n",
            "Step 47010, Loss 0.480669766664505\n",
            "Step 47015, Loss 0.40896573662757874\n",
            "Step 47020, Loss 0.38307344913482666\n",
            "Step 47025, Loss 0.4872698485851288\n",
            "Step 47030, Loss 0.47413694858551025\n",
            "model not updated\n",
            "Starting epoch 135/160, LR = [0.006375340083560676]\n",
            "Step 47035, Loss 0.44980695843696594\n",
            "Step 47040, Loss 0.45946401357650757\n",
            "Step 47045, Loss 0.45340415835380554\n",
            "Step 47050, Loss 0.31932729482650757\n",
            "Step 47055, Loss 0.2723608911037445\n",
            "Step 47060, Loss 0.43031859397888184\n",
            "Step 47065, Loss 0.3990238308906555\n",
            "Step 47070, Loss 0.3990848958492279\n",
            "Step 47075, Loss 0.5325927138328552\n",
            "Step 47080, Loss 0.40645352005958557\n",
            "Step 47085, Loss 0.3963031768798828\n",
            "Step 47090, Loss 0.4520665109157562\n",
            "Step 47095, Loss 0.46953874826431274\n",
            "Step 47100, Loss 0.35496795177459717\n",
            "Step 47105, Loss 0.5079630017280579\n",
            "Step 47110, Loss 0.2841959595680237\n",
            "Step 47115, Loss 0.31734365224838257\n",
            "Step 47120, Loss 0.4040551483631134\n",
            "Step 47125, Loss 0.5117658972740173\n",
            "Step 47130, Loss 0.34418949484825134\n",
            "Step 47135, Loss 0.3845568299293518\n",
            "Step 47140, Loss 0.3589922785758972\n",
            "Step 47145, Loss 0.4428654909133911\n",
            "Step 47150, Loss 0.41710248589515686\n",
            "Step 47155, Loss 0.5261269211769104\n",
            "Step 47160, Loss 0.4392687678337097\n",
            "Step 47165, Loss 0.47334638237953186\n",
            "Step 47170, Loss 0.4005741477012634\n",
            "Step 47175, Loss 0.43135306239128113\n",
            "Step 47180, Loss 0.41827934980392456\n",
            "Step 47185, Loss 0.31319284439086914\n",
            "Step 47190, Loss 0.3793734908103943\n",
            "Step 47195, Loss 0.4055652916431427\n",
            "Step 47200, Loss 0.46078038215637207\n",
            "Step 47205, Loss 0.36794814467430115\n",
            "Step 47210, Loss 0.34273937344551086\n",
            "Step 47215, Loss 0.4718453288078308\n",
            "Step 47220, Loss 0.3204447627067566\n",
            "Step 47225, Loss 0.4526517391204834\n",
            "Step 47230, Loss 0.4229452610015869\n",
            "Step 47235, Loss 0.47904765605926514\n",
            "Step 47240, Loss 0.4168779253959656\n",
            "Step 47245, Loss 0.2885277569293976\n",
            "Step 47250, Loss 0.5175379514694214\n",
            "Step 47255, Loss 0.47502440214157104\n",
            "Step 47260, Loss 0.501274824142456\n",
            "Step 47265, Loss 0.5418308973312378\n",
            "Step 47270, Loss 0.3207823932170868\n",
            "Step 47275, Loss 0.5422799587249756\n",
            "Step 47280, Loss 0.3744622468948364\n",
            "Step 47285, Loss 0.42260581254959106\n",
            "Step 47290, Loss 0.4483756721019745\n",
            "Step 47295, Loss 0.3509211838245392\n",
            "Step 47300, Loss 0.3573283851146698\n",
            "Step 47305, Loss 0.5301806926727295\n",
            "Step 47310, Loss 0.36006203293800354\n",
            "Step 47315, Loss 0.4153735935688019\n",
            "Step 47320, Loss 0.4300495386123657\n",
            "Step 47325, Loss 0.44302842020988464\n",
            "Step 47330, Loss 0.548385739326477\n",
            "Step 47335, Loss 0.46243035793304443\n",
            "Step 47340, Loss 0.49892717599868774\n",
            "Step 47345, Loss 0.37692153453826904\n",
            "Step 47350, Loss 0.526584804058075\n",
            "Step 47355, Loss 0.5248923897743225\n",
            "Step 47360, Loss 0.44192197918891907\n",
            "Step 47365, Loss 0.48266494274139404\n",
            "Step 47370, Loss 0.39193040132522583\n",
            "Step 47375, Loss 0.32996925711631775\n",
            "Step 47380, Loss 0.33724284172058105\n",
            "model not updated\n",
            "Starting epoch 136/160, LR = [0.0059040779266770725]\n",
            "Step 47385, Loss 0.38516196608543396\n",
            "Step 47390, Loss 0.2889291048049927\n",
            "Step 47395, Loss 0.3531167507171631\n",
            "Step 47400, Loss 0.4885219633579254\n",
            "Step 47405, Loss 0.33633679151535034\n",
            "Step 47410, Loss 0.46550214290618896\n",
            "Step 47415, Loss 0.5700708031654358\n",
            "Step 47420, Loss 0.44464242458343506\n",
            "Step 47425, Loss 0.4657359719276428\n",
            "Step 47430, Loss 0.29440149664878845\n",
            "Step 47435, Loss 0.33853957056999207\n",
            "Step 47440, Loss 0.43070048093795776\n",
            "Step 47445, Loss 0.37458178400993347\n",
            "Step 47450, Loss 0.31982719898223877\n",
            "Step 47455, Loss 0.3541485369205475\n",
            "Step 47460, Loss 0.3661569058895111\n",
            "Step 47465, Loss 0.3604007959365845\n",
            "Step 47470, Loss 0.4568231999874115\n",
            "Step 47475, Loss 0.4044489562511444\n",
            "Step 47480, Loss 0.5008231997489929\n",
            "Step 47485, Loss 0.502673864364624\n",
            "Step 47490, Loss 0.42548418045043945\n",
            "Step 47495, Loss 0.4371189773082733\n",
            "Step 47500, Loss 0.3867201805114746\n",
            "Step 47505, Loss 0.37393760681152344\n",
            "Step 47510, Loss 0.36005261540412903\n",
            "Step 47515, Loss 0.4134475290775299\n",
            "Step 47520, Loss 0.4054182469844818\n",
            "Step 47525, Loss 0.4114205241203308\n",
            "Step 47530, Loss 0.3433186709880829\n",
            "Step 47535, Loss 0.4595855474472046\n",
            "Step 47540, Loss 0.3980838656425476\n",
            "Step 47545, Loss 0.328947514295578\n",
            "Step 47550, Loss 0.35433879494667053\n",
            "Step 47555, Loss 0.4885289967060089\n",
            "Step 47560, Loss 0.358837366104126\n",
            "Step 47565, Loss 0.5824605226516724\n",
            "Step 47570, Loss 0.38128817081451416\n",
            "Step 47575, Loss 0.49660056829452515\n",
            "Step 47580, Loss 0.35246801376342773\n",
            "Step 47585, Loss 0.5559892058372498\n",
            "Step 47590, Loss 0.2878427505493164\n",
            "Step 47595, Loss 0.49675247073173523\n",
            "Step 47600, Loss 0.48336881399154663\n",
            "Step 47605, Loss 0.4136802554130554\n",
            "Step 47610, Loss 0.3968527317047119\n",
            "Step 47615, Loss 0.46058735251426697\n",
            "Step 47620, Loss 0.4450790286064148\n",
            "Step 47625, Loss 0.5096760988235474\n",
            "Step 47630, Loss 0.3126251995563507\n",
            "Step 47635, Loss 0.3705413341522217\n",
            "Step 47640, Loss 0.30451878905296326\n",
            "Step 47645, Loss 0.4795387387275696\n",
            "Step 47650, Loss 0.4380328059196472\n",
            "Step 47655, Loss 0.3912668228149414\n",
            "Step 47660, Loss 0.42908820509910583\n",
            "Step 47665, Loss 0.32500287890434265\n",
            "Step 47670, Loss 0.4678415060043335\n",
            "Step 47675, Loss 0.3564658761024475\n",
            "Step 47680, Loss 0.4222847521305084\n",
            "Step 47685, Loss 0.46060389280319214\n",
            "Step 47690, Loss 0.2617539167404175\n",
            "Step 47695, Loss 0.48074662685394287\n",
            "Step 47700, Loss 0.36739665269851685\n",
            "Step 47705, Loss 0.29565659165382385\n",
            "Step 47710, Loss 0.44655323028564453\n",
            "Step 47715, Loss 0.5453468561172485\n",
            "Step 47720, Loss 0.4875015616416931\n",
            "Step 47725, Loss 0.5652770400047302\n",
            "Step 47730, Loss 0.3678240180015564\n",
            "Step 47735, Loss 0.34057891368865967\n",
            "model not updated\n",
            "Starting epoch 137/160, LR = [0.0054498156160709185]\n",
            "Step 47740, Loss 0.36741578578948975\n",
            "Step 47745, Loss 0.42140692472457886\n",
            "Step 47750, Loss 0.46214404702186584\n",
            "Step 47755, Loss 0.35280680656433105\n",
            "Step 47760, Loss 0.4315131604671478\n",
            "Step 47765, Loss 0.4075544476509094\n",
            "Step 47770, Loss 0.4087877571582794\n",
            "Step 47775, Loss 0.36033400893211365\n",
            "Step 47780, Loss 0.373290479183197\n",
            "Step 47785, Loss 0.5058273077011108\n",
            "Step 47790, Loss 0.4283452033996582\n",
            "Step 47795, Loss 0.5879783630371094\n",
            "Step 47800, Loss 0.3192136585712433\n",
            "Step 47805, Loss 0.3704938292503357\n",
            "Step 47810, Loss 0.4092088043689728\n",
            "Step 47815, Loss 0.4138462543487549\n",
            "Step 47820, Loss 0.43535181879997253\n",
            "Step 47825, Loss 0.2992718517780304\n",
            "Step 47830, Loss 0.34193360805511475\n",
            "Step 47835, Loss 0.39616289734840393\n",
            "Step 47840, Loss 0.3295193910598755\n",
            "Step 47845, Loss 0.34038713574409485\n",
            "Step 47850, Loss 0.5293084979057312\n",
            "Step 47855, Loss 0.5199646353721619\n",
            "Step 47860, Loss 0.36231470108032227\n",
            "Step 47865, Loss 0.33756428956985474\n",
            "Step 47870, Loss 0.3316795527935028\n",
            "Step 47875, Loss 0.4312790036201477\n",
            "Step 47880, Loss 0.3699316382408142\n",
            "Step 47885, Loss 0.33363717794418335\n",
            "Step 47890, Loss 0.3633717894554138\n",
            "Step 47895, Loss 0.28565549850463867\n",
            "Step 47900, Loss 0.5477039813995361\n",
            "Step 47905, Loss 0.5691931247711182\n",
            "Step 47910, Loss 0.48862552642822266\n",
            "Step 47915, Loss 0.3074401617050171\n",
            "Step 47920, Loss 0.4505962133407593\n",
            "Step 47925, Loss 0.3832171857357025\n",
            "Step 47930, Loss 0.406060129404068\n",
            "Step 47935, Loss 0.4967319667339325\n",
            "Step 47940, Loss 0.3194991648197174\n",
            "Step 47945, Loss 0.44465017318725586\n",
            "Step 47950, Loss 0.44369491934776306\n",
            "Step 47955, Loss 0.3887532353401184\n",
            "Step 47960, Loss 0.348675400018692\n",
            "Step 47965, Loss 0.48410269618034363\n",
            "Step 47970, Loss 0.5376095175743103\n",
            "Step 47975, Loss 0.3393987715244293\n",
            "Step 47980, Loss 0.538921058177948\n",
            "Step 47985, Loss 0.5129373073577881\n",
            "Step 47990, Loss 0.37435606122016907\n",
            "Step 47995, Loss 0.46786078810691833\n",
            "Step 48000, Loss 0.37377458810806274\n",
            "Step 48005, Loss 0.3948119878768921\n",
            "Step 48010, Loss 0.37258076667785645\n",
            "Step 48015, Loss 0.46183863282203674\n",
            "Step 48020, Loss 0.4343039095401764\n",
            "Step 48025, Loss 0.36607298254966736\n",
            "Step 48030, Loss 0.48709166049957275\n",
            "Step 48035, Loss 0.40516388416290283\n",
            "Step 48040, Loss 0.4934197962284088\n",
            "Step 48045, Loss 0.5021018981933594\n",
            "Step 48050, Loss 0.32083314657211304\n",
            "Step 48055, Loss 0.5254796147346497\n",
            "Step 48060, Loss 0.5469785928726196\n",
            "Step 48065, Loss 0.4335821568965912\n",
            "Step 48070, Loss 0.4359721541404724\n",
            "Step 48075, Loss 0.37566107511520386\n",
            "Step 48080, Loss 0.5535538792610168\n",
            "Step 48085, Loss 0.5887183547019958\n",
            "model not updated\n",
            "Starting epoch 138/160, LR = [0.005012728278510223]\n",
            "Step 48090, Loss 0.4781727194786072\n",
            "Step 48095, Loss 0.3431475758552551\n",
            "Step 48100, Loss 0.3351052701473236\n",
            "Step 48105, Loss 0.38746142387390137\n",
            "Step 48110, Loss 0.3662378191947937\n",
            "Step 48115, Loss 0.29411113262176514\n",
            "Step 48120, Loss 0.5243655443191528\n",
            "Step 48125, Loss 0.2708394527435303\n",
            "Step 48130, Loss 0.3834701478481293\n",
            "Step 48135, Loss 0.4101288616657257\n",
            "Step 48140, Loss 0.39948591589927673\n",
            "Step 48145, Loss 0.5129094123840332\n",
            "Step 48150, Loss 0.33983224630355835\n",
            "Step 48155, Loss 0.29941076040267944\n",
            "Step 48160, Loss 0.3860549330711365\n",
            "Step 48165, Loss 0.37347930669784546\n",
            "Step 48170, Loss 0.3240697681903839\n",
            "Step 48175, Loss 0.4720994830131531\n",
            "Step 48180, Loss 0.47521546483039856\n",
            "Step 48185, Loss 0.2718207538127899\n",
            "Step 48190, Loss 0.34686478972435\n",
            "Step 48195, Loss 0.4058000445365906\n",
            "Step 48200, Loss 0.4022979438304901\n",
            "Step 48205, Loss 0.3824796974658966\n",
            "Step 48210, Loss 0.3462570905685425\n",
            "Step 48215, Loss 0.33512914180755615\n",
            "Step 48220, Loss 0.36619600653648376\n",
            "Step 48225, Loss 0.27414098381996155\n",
            "Step 48230, Loss 0.45610058307647705\n",
            "Step 48235, Loss 0.4730532467365265\n",
            "Step 48240, Loss 0.3816685974597931\n",
            "Step 48245, Loss 0.4790053963661194\n",
            "Step 48250, Loss 0.4642294943332672\n",
            "Step 48255, Loss 0.30762484669685364\n",
            "Step 48260, Loss 0.38942158222198486\n",
            "Step 48265, Loss 0.42113620042800903\n",
            "Step 48270, Loss 0.3301559388637543\n",
            "Step 48275, Loss 0.3357389271259308\n",
            "Step 48280, Loss 0.2999604642391205\n",
            "Step 48285, Loss 0.5020532011985779\n",
            "Step 48290, Loss 0.42524969577789307\n",
            "Step 48295, Loss 0.4635380506515503\n",
            "Step 48300, Loss 0.42962560057640076\n",
            "Step 48305, Loss 0.4072781801223755\n",
            "Step 48310, Loss 0.3646460175514221\n",
            "Step 48315, Loss 0.5432068109512329\n",
            "Step 48320, Loss 0.4968610107898712\n",
            "Step 48325, Loss 0.4500782787799835\n",
            "Step 48330, Loss 0.5004173517227173\n",
            "Step 48335, Loss 0.33705568313598633\n",
            "Step 48340, Loss 0.38952675461769104\n",
            "Step 48345, Loss 0.40388065576553345\n",
            "Step 48350, Loss 0.4328886568546295\n",
            "Step 48355, Loss 0.45996004343032837\n",
            "Step 48360, Loss 0.5418945550918579\n",
            "Step 48365, Loss 0.3904087245464325\n",
            "Step 48370, Loss 0.4300784766674042\n",
            "Step 48375, Loss 0.38382136821746826\n",
            "Step 48380, Loss 0.412066251039505\n",
            "Step 48385, Loss 0.35562047362327576\n",
            "Step 48390, Loss 0.5018401741981506\n",
            "Step 48395, Loss 0.6077896356582642\n",
            "Step 48400, Loss 0.34398505091667175\n",
            "Step 48405, Loss 0.36535632610321045\n",
            "Step 48410, Loss 0.4009530246257782\n",
            "Step 48415, Loss 0.3919932246208191\n",
            "Step 48420, Loss 0.4571654200553894\n",
            "Step 48425, Loss 0.3070060610771179\n",
            "Step 48430, Loss 0.41552603244781494\n",
            "Step 48435, Loss 0.38404983282089233\n",
            "model not updated\n",
            "Starting epoch 139/160, LR = [0.004592984419483964]\n",
            "Step 48440, Loss 0.3608163595199585\n",
            "Step 48445, Loss 0.3925457298755646\n",
            "Step 48450, Loss 0.4035373330116272\n",
            "Step 48455, Loss 0.32295894622802734\n",
            "Step 48460, Loss 0.5003267526626587\n",
            "Step 48465, Loss 0.29598167538642883\n",
            "Step 48470, Loss 0.3522431552410126\n",
            "Step 48475, Loss 0.410374253988266\n",
            "Step 48480, Loss 0.41683754324913025\n",
            "Step 48485, Loss 0.39548736810684204\n",
            "Step 48490, Loss 0.3822580873966217\n",
            "Step 48495, Loss 0.5007138252258301\n",
            "Step 48500, Loss 0.4048674702644348\n",
            "Step 48505, Loss 0.4196001887321472\n",
            "Step 48510, Loss 0.5457378625869751\n",
            "Step 48515, Loss 0.5939182043075562\n",
            "Step 48520, Loss 0.45882636308670044\n",
            "Step 48525, Loss 0.4484997093677521\n",
            "Step 48530, Loss 0.38788819313049316\n",
            "Step 48535, Loss 0.4446987807750702\n",
            "Step 48540, Loss 0.494069367647171\n",
            "Step 48545, Loss 0.4070892632007599\n",
            "Step 48550, Loss 0.4252808690071106\n",
            "Step 48555, Loss 0.4229338467121124\n",
            "Step 48560, Loss 0.35754114389419556\n",
            "Step 48565, Loss 0.3051888942718506\n",
            "Step 48570, Loss 0.39556196331977844\n",
            "Step 48575, Loss 0.32122522592544556\n",
            "Step 48580, Loss 0.3490583002567291\n",
            "Step 48585, Loss 0.5104352831840515\n",
            "Step 48590, Loss 0.3744749128818512\n",
            "Step 48595, Loss 0.31007111072540283\n",
            "Step 48600, Loss 0.4427397847175598\n",
            "Step 48605, Loss 0.39368656277656555\n",
            "Step 48610, Loss 0.5528046488761902\n",
            "Step 48615, Loss 0.3913094401359558\n",
            "Step 48620, Loss 0.3576308786869049\n",
            "Step 48625, Loss 0.4248565137386322\n",
            "Step 48630, Loss 0.332091361284256\n",
            "Step 48635, Loss 0.4438551664352417\n",
            "Step 48640, Loss 0.3969508111476898\n",
            "Step 48645, Loss 0.3740312457084656\n",
            "Step 48650, Loss 0.3772599697113037\n",
            "Step 48655, Loss 0.46516308188438416\n",
            "Step 48660, Loss 0.3372136056423187\n",
            "Step 48665, Loss 0.36689242720603943\n",
            "Step 48670, Loss 0.4010699689388275\n",
            "Step 48675, Loss 0.4510372281074524\n",
            "Step 48680, Loss 0.4758269190788269\n",
            "Step 48685, Loss 0.4275882840156555\n",
            "Step 48690, Loss 0.44433072209358215\n",
            "Step 48695, Loss 0.30168837308883667\n",
            "Step 48700, Loss 0.2600169777870178\n",
            "Step 48705, Loss 0.4592336118221283\n",
            "Step 48710, Loss 0.2626466453075409\n",
            "Step 48715, Loss 0.6185526847839355\n",
            "Step 48720, Loss 0.42984944581985474\n",
            "Step 48725, Loss 0.31315767765045166\n",
            "Step 48730, Loss 0.32846522331237793\n",
            "Step 48735, Loss 0.320803701877594\n",
            "Step 48740, Loss 0.4370949864387512\n",
            "Step 48745, Loss 0.33866679668426514\n",
            "Step 48750, Loss 0.40679144859313965\n",
            "Step 48755, Loss 0.32762664556503296\n",
            "Step 48760, Loss 0.5164728164672852\n",
            "Step 48765, Loss 0.43063703179359436\n",
            "Step 48770, Loss 0.4355415403842926\n",
            "Step 48775, Loss 0.3199729919433594\n",
            "Step 48780, Loss 0.31061673164367676\n",
            "Step 48785, Loss 0.38791030645370483\n",
            "model not updated\n",
            "Starting epoch 140/160, LR = [0.004190745858239986]\n",
            "Step 48790, Loss 0.4521521329879761\n",
            "Step 48795, Loss 0.3324030935764313\n",
            "Step 48800, Loss 0.43980374932289124\n",
            "Step 48805, Loss 0.40656763315200806\n",
            "Step 48810, Loss 0.4169605076313019\n",
            "Step 48815, Loss 0.571060836315155\n",
            "Step 48820, Loss 0.5158692002296448\n",
            "Step 48825, Loss 0.4680007994174957\n",
            "Step 48830, Loss 0.45988911390304565\n",
            "Step 48835, Loss 0.36872079968452454\n",
            "Step 48840, Loss 0.532355785369873\n",
            "Step 48845, Loss 0.4014124274253845\n",
            "Step 48850, Loss 0.27188289165496826\n",
            "Step 48855, Loss 0.2558138072490692\n",
            "Step 48860, Loss 0.3359992504119873\n",
            "Step 48865, Loss 0.3454519212245941\n",
            "Step 48870, Loss 0.5457887649536133\n",
            "Step 48875, Loss 0.462065190076828\n",
            "Step 48880, Loss 0.3517777919769287\n",
            "Step 48885, Loss 0.42031869292259216\n",
            "Step 48890, Loss 0.3152919411659241\n",
            "Step 48895, Loss 0.3838428258895874\n",
            "Step 48900, Loss 0.2644209563732147\n",
            "Step 48905, Loss 0.38884687423706055\n",
            "Step 48910, Loss 0.3322286903858185\n",
            "Step 48915, Loss 0.42965176701545715\n",
            "Step 48920, Loss 0.33614543080329895\n",
            "Step 48925, Loss 0.5276978611946106\n",
            "Step 48930, Loss 0.42616137862205505\n",
            "Step 48935, Loss 0.4324873387813568\n",
            "Step 48940, Loss 0.3779388666152954\n",
            "Step 48945, Loss 0.44278302788734436\n",
            "Step 48950, Loss 0.5165969729423523\n",
            "Step 48955, Loss 0.3282981812953949\n",
            "Step 48960, Loss 0.35407230257987976\n",
            "Step 48965, Loss 0.4843541383743286\n",
            "Step 48970, Loss 0.26892659068107605\n",
            "Step 48975, Loss 0.4366840422153473\n",
            "Step 48980, Loss 0.3794876039028168\n",
            "Step 48985, Loss 0.43681639432907104\n",
            "Step 48990, Loss 0.2725061774253845\n",
            "Step 48995, Loss 0.3547831177711487\n",
            "Step 49000, Loss 0.35509175062179565\n",
            "Step 49005, Loss 0.38109976053237915\n",
            "Step 49010, Loss 0.42057424783706665\n",
            "Step 49015, Loss 0.45366349816322327\n",
            "Step 49020, Loss 0.36063534021377563\n",
            "Step 49025, Loss 0.3338237404823303\n",
            "Step 49030, Loss 0.3865182101726532\n",
            "Step 49035, Loss 0.4255070984363556\n",
            "Step 49040, Loss 0.2714267671108246\n",
            "Step 49045, Loss 0.39490869641304016\n",
            "Step 49050, Loss 0.4474075138568878\n",
            "Step 49055, Loss 0.3754180073738098\n",
            "Step 49060, Loss 0.2705766558647156\n",
            "Step 49065, Loss 0.396098792552948\n",
            "Step 49070, Loss 0.4673546254634857\n",
            "Step 49075, Loss 0.38849714398384094\n",
            "Step 49080, Loss 0.3346344828605652\n",
            "Step 49085, Loss 0.4306560754776001\n",
            "Step 49090, Loss 0.438515841960907\n",
            "Step 49095, Loss 0.3637990355491638\n",
            "Step 49100, Loss 0.40809908509254456\n",
            "Step 49105, Loss 0.4080212414264679\n",
            "Step 49110, Loss 0.3617332875728607\n",
            "Step 49115, Loss 0.2925090193748474\n",
            "Step 49120, Loss 0.3998448848724365\n",
            "Step 49125, Loss 0.33286625146865845\n",
            "Step 49130, Loss 0.3453437089920044\n",
            "Step 49135, Loss 0.4237697124481201\n",
            "model not updated\n",
            "Starting epoch 141/160, LR = [0.003806167665400597]\n",
            "Step 49140, Loss 0.47284775972366333\n",
            "Step 49145, Loss 0.48873355984687805\n",
            "Step 49150, Loss 0.3787870705127716\n",
            "Step 49155, Loss 0.3388931453227997\n",
            "Step 49160, Loss 0.2985662817955017\n",
            "Step 49165, Loss 0.2513331472873688\n",
            "Step 49170, Loss 0.5208513736724854\n",
            "Step 49175, Loss 0.24458998441696167\n",
            "Step 49180, Loss 0.3776673972606659\n",
            "Step 49185, Loss 0.3741534650325775\n",
            "Step 49190, Loss 0.34150293469429016\n",
            "Step 49195, Loss 0.4286836087703705\n",
            "Step 49200, Loss 0.3095671832561493\n",
            "Step 49205, Loss 0.34485238790512085\n",
            "Step 49210, Loss 0.3181675672531128\n",
            "Step 49215, Loss 0.4011988043785095\n",
            "Step 49220, Loss 0.26753005385398865\n",
            "Step 49225, Loss 0.3546519875526428\n",
            "Step 49230, Loss 0.24112321436405182\n",
            "Step 49235, Loss 0.3206063210964203\n",
            "Step 49240, Loss 0.3372545540332794\n",
            "Step 49245, Loss 0.29747289419174194\n",
            "Step 49250, Loss 0.643404483795166\n",
            "Step 49255, Loss 0.3889763653278351\n",
            "Step 49260, Loss 0.26751914620399475\n",
            "Step 49265, Loss 0.4785316586494446\n",
            "Step 49270, Loss 0.28501376509666443\n",
            "Step 49275, Loss 0.3618437945842743\n",
            "Step 49280, Loss 0.414166659116745\n",
            "Step 49285, Loss 0.3644617795944214\n",
            "Step 49290, Loss 0.39877575635910034\n",
            "Step 49295, Loss 0.3785214126110077\n",
            "Step 49300, Loss 0.2770746052265167\n",
            "Step 49305, Loss 0.37954220175743103\n",
            "Step 49310, Loss 0.5064523816108704\n",
            "Step 49315, Loss 0.3665347397327423\n",
            "Step 49320, Loss 0.37940412759780884\n",
            "Step 49325, Loss 0.445613831281662\n",
            "Step 49330, Loss 0.335330069065094\n",
            "Step 49335, Loss 0.5526509284973145\n",
            "Step 49340, Loss 0.3198200762271881\n",
            "Step 49345, Loss 0.4215627908706665\n",
            "Step 49350, Loss 0.39347925782203674\n",
            "Step 49355, Loss 0.39938315749168396\n",
            "Step 49360, Loss 0.48409557342529297\n",
            "Step 49365, Loss 0.34576746821403503\n",
            "Step 49370, Loss 0.3977818489074707\n",
            "Step 49375, Loss 0.28785818815231323\n",
            "Step 49380, Loss 0.5096662640571594\n",
            "Step 49385, Loss 0.4393724203109741\n",
            "Step 49390, Loss 0.28670284152030945\n",
            "Step 49395, Loss 0.44178760051727295\n",
            "Step 49400, Loss 0.346635639667511\n",
            "Step 49405, Loss 0.4020571708679199\n",
            "Step 49410, Loss 0.34815070033073425\n",
            "Step 49415, Loss 0.33261191844940186\n",
            "Step 49420, Loss 0.25930261611938477\n",
            "Step 49425, Loss 0.4113554060459137\n",
            "Step 49430, Loss 0.5588950514793396\n",
            "Step 49435, Loss 0.38647353649139404\n",
            "Step 49440, Loss 0.23557238280773163\n",
            "Step 49445, Loss 0.42439085245132446\n",
            "Step 49450, Loss 0.36052289605140686\n",
            "Step 49455, Loss 0.49890950322151184\n",
            "Step 49460, Loss 0.283925324678421\n",
            "Step 49465, Loss 0.4501410126686096\n",
            "Step 49470, Loss 0.30687373876571655\n",
            "Step 49475, Loss 0.3545357584953308\n",
            "Step 49480, Loss 0.406892329454422\n",
            "Step 49485, Loss 0.4118642210960388\n",
            "Step 49490, Loss 0.47272002696990967\n",
            "model not updated\n",
            "Starting epoch 142/160, LR = [0.0034393981031799357]\n",
            "Step 49495, Loss 0.3710343539714813\n",
            "Step 49500, Loss 0.4154060482978821\n",
            "Step 49505, Loss 0.3453994691371918\n",
            "Step 49510, Loss 0.3505435287952423\n",
            "Step 49515, Loss 0.4045613706111908\n",
            "Step 49520, Loss 0.36803731322288513\n",
            "Step 49525, Loss 0.43707048892974854\n",
            "Step 49530, Loss 0.2743024230003357\n",
            "Step 49535, Loss 0.32562699913978577\n",
            "Step 49540, Loss 0.37404248118400574\n",
            "Step 49545, Loss 0.27766355872154236\n",
            "Step 49550, Loss 0.33617424964904785\n",
            "Step 49555, Loss 0.3265604078769684\n",
            "Step 49560, Loss 0.3099265694618225\n",
            "Step 49565, Loss 0.3827383518218994\n",
            "Step 49570, Loss 0.36438342928886414\n",
            "Step 49575, Loss 0.38095319271087646\n",
            "Step 49580, Loss 0.3739413917064667\n",
            "Step 49585, Loss 0.2763463854789734\n",
            "Step 49590, Loss 0.3056039810180664\n",
            "Step 49595, Loss 0.3423182964324951\n",
            "Step 49600, Loss 0.2951327860355377\n",
            "Step 49605, Loss 0.3653656542301178\n",
            "Step 49610, Loss 0.3341703712940216\n",
            "Step 49615, Loss 0.4748145639896393\n",
            "Step 49620, Loss 0.35729554295539856\n",
            "Step 49625, Loss 0.3411831855773926\n",
            "Step 49630, Loss 0.3520958125591278\n",
            "Step 49635, Loss 0.36895596981048584\n",
            "Step 49640, Loss 0.4117859899997711\n",
            "Step 49645, Loss 0.4388602077960968\n",
            "Step 49650, Loss 0.3830419182777405\n",
            "Step 49655, Loss 0.3649836480617523\n",
            "Step 49660, Loss 0.3029932677745819\n",
            "Step 49665, Loss 0.3612079620361328\n",
            "Step 49670, Loss 0.31850025057792664\n",
            "Step 49675, Loss 0.33721843361854553\n",
            "Step 49680, Loss 0.32950884103775024\n",
            "Step 49685, Loss 0.45480629801750183\n",
            "Step 49690, Loss 0.34188520908355713\n",
            "Step 49695, Loss 0.4004373550415039\n",
            "Step 49700, Loss 0.4552208185195923\n",
            "Step 49705, Loss 0.454883337020874\n",
            "Step 49710, Loss 0.6351690292358398\n",
            "Step 49715, Loss 0.30721455812454224\n",
            "Step 49720, Loss 0.3786546587944031\n",
            "Step 49725, Loss 0.41316840052604675\n",
            "Step 49730, Loss 0.5630568265914917\n",
            "Step 49735, Loss 0.279959112405777\n",
            "Step 49740, Loss 0.4162427484989166\n",
            "Step 49745, Loss 0.35999158024787903\n",
            "Step 49750, Loss 0.6691485047340393\n",
            "Step 49755, Loss 0.3387208580970764\n",
            "Step 49760, Loss 0.4191320240497589\n",
            "Step 49765, Loss 0.4666630029678345\n",
            "Step 49770, Loss 0.4495416283607483\n",
            "Step 49775, Loss 0.35567736625671387\n",
            "Step 49780, Loss 0.49702104926109314\n",
            "Step 49785, Loss 0.30589720606803894\n",
            "Step 49790, Loss 0.25197139382362366\n",
            "Step 49795, Loss 0.3717934787273407\n",
            "Step 49800, Loss 0.3862803280353546\n",
            "Step 49805, Loss 0.2615198791027069\n",
            "Step 49810, Loss 0.47783565521240234\n",
            "Step 49815, Loss 0.43448692560195923\n",
            "Step 49820, Loss 0.38790908455848694\n",
            "Step 49825, Loss 0.4344109892845154\n",
            "Step 49830, Loss 0.34373560547828674\n",
            "Step 49835, Loss 0.515647292137146\n",
            "Step 49840, Loss 0.3347291946411133\n",
            "model not updated\n",
            "Starting epoch 143/160, LR = [0.0030905785682259824]\n",
            "Step 49845, Loss 0.3166370987892151\n",
            "Step 49850, Loss 0.3727256655693054\n",
            "Step 49855, Loss 0.28886598348617554\n",
            "Step 49860, Loss 0.40994539856910706\n",
            "Step 49865, Loss 0.3502511978149414\n",
            "Step 49870, Loss 0.5161980986595154\n",
            "Step 49875, Loss 0.38677269220352173\n",
            "Step 49880, Loss 0.35443946719169617\n",
            "Step 49885, Loss 0.30039718747138977\n",
            "Step 49890, Loss 0.40345579385757446\n",
            "Step 49895, Loss 0.27500060200691223\n",
            "Step 49900, Loss 0.3520132303237915\n",
            "Step 49905, Loss 0.35565516352653503\n",
            "Step 49910, Loss 0.4696901738643646\n",
            "Step 49915, Loss 0.3371155858039856\n",
            "Step 49920, Loss 0.37059321999549866\n",
            "Step 49925, Loss 0.5299162268638611\n",
            "Step 49930, Loss 0.3641047477722168\n",
            "Step 49935, Loss 0.36088865995407104\n",
            "Step 49940, Loss 0.30893656611442566\n",
            "Step 49945, Loss 0.3260747790336609\n",
            "Step 49950, Loss 0.39616864919662476\n",
            "Step 49955, Loss 0.4266689419746399\n",
            "Step 49960, Loss 0.4310030937194824\n",
            "Step 49965, Loss 0.34010615944862366\n",
            "Step 49970, Loss 0.2742905914783478\n",
            "Step 49975, Loss 0.3473471999168396\n",
            "Step 49980, Loss 0.3679254949092865\n",
            "Step 49985, Loss 0.3907441198825836\n",
            "Step 49990, Loss 0.32921531796455383\n",
            "Step 49995, Loss 0.38897988200187683\n",
            "Step 50000, Loss 0.36105144023895264\n",
            "Step 50005, Loss 0.34238845109939575\n",
            "Step 50010, Loss 0.3718057870864868\n",
            "Step 50015, Loss 0.2890285551548004\n",
            "Step 50020, Loss 0.3318711519241333\n",
            "Step 50025, Loss 0.3866654932498932\n",
            "Step 50030, Loss 0.3399440050125122\n",
            "Step 50035, Loss 0.3822043538093567\n",
            "Step 50040, Loss 0.2865420877933502\n",
            "Step 50045, Loss 0.43899932503700256\n",
            "Step 50050, Loss 0.3598509728908539\n",
            "Step 50055, Loss 0.2657102048397064\n",
            "Step 50060, Loss 0.40782785415649414\n",
            "Step 50065, Loss 0.4578072428703308\n",
            "Step 50070, Loss 0.2753768563270569\n",
            "Step 50075, Loss 0.32904043793678284\n",
            "Step 50080, Loss 0.39602139592170715\n",
            "Step 50085, Loss 0.3793245553970337\n",
            "Step 50090, Loss 0.37582600116729736\n",
            "Step 50095, Loss 0.3970789909362793\n",
            "Step 50100, Loss 0.3202798068523407\n",
            "Step 50105, Loss 0.3594249486923218\n",
            "Step 50110, Loss 0.32227516174316406\n",
            "Step 50115, Loss 0.31654495000839233\n",
            "Step 50120, Loss 0.27169373631477356\n",
            "Step 50125, Loss 0.4357615113258362\n",
            "Step 50130, Loss 0.4490428566932678\n",
            "Step 50135, Loss 0.4685150980949402\n",
            "Step 50140, Loss 0.4249719977378845\n",
            "Step 50145, Loss 0.5473516583442688\n",
            "Step 50150, Loss 0.4540732204914093\n",
            "Step 50155, Loss 0.3676906228065491\n",
            "Step 50160, Loss 0.35911324620246887\n",
            "Step 50165, Loss 0.2862198054790497\n",
            "Step 50170, Loss 0.424297571182251\n",
            "Step 50175, Loss 0.46215522289276123\n",
            "Step 50180, Loss 0.313425749540329\n",
            "Step 50185, Loss 0.33901140093803406\n",
            "Step 50190, Loss 0.29575875401496887\n",
            "model not updated\n",
            "Starting epoch 144/160, LR = [0.0027598435371095848]\n",
            "Step 50195, Loss 0.4854927659034729\n",
            "Step 50200, Loss 0.41680556535720825\n",
            "Step 50205, Loss 0.3194398880004883\n",
            "Step 50210, Loss 0.2812533974647522\n",
            "Step 50215, Loss 0.3346201777458191\n",
            "Step 50220, Loss 0.328678697347641\n",
            "Step 50225, Loss 0.3836783468723297\n",
            "Step 50230, Loss 0.36666709184646606\n",
            "Step 50235, Loss 0.23495922982692719\n",
            "Step 50240, Loss 0.26521775126457214\n",
            "Step 50245, Loss 0.3983928859233856\n",
            "Step 50250, Loss 0.3033331334590912\n",
            "Step 50255, Loss 0.3536490797996521\n",
            "Step 50260, Loss 0.3691645860671997\n",
            "Step 50265, Loss 0.25211426615715027\n",
            "Step 50270, Loss 0.41001784801483154\n",
            "Step 50275, Loss 0.3241143226623535\n",
            "Step 50280, Loss 0.4346575438976288\n",
            "Step 50285, Loss 0.47153839468955994\n",
            "Step 50290, Loss 0.3140602111816406\n",
            "Step 50295, Loss 0.3522946834564209\n",
            "Step 50300, Loss 0.3829542100429535\n",
            "Step 50305, Loss 0.31572195887565613\n",
            "Step 50310, Loss 0.3932751417160034\n",
            "Step 50315, Loss 0.47927844524383545\n",
            "Step 50320, Loss 0.3778872489929199\n",
            "Step 50325, Loss 0.40407827496528625\n",
            "Step 50330, Loss 0.27086395025253296\n",
            "Step 50335, Loss 0.4180372655391693\n",
            "Step 50340, Loss 0.43949612975120544\n",
            "Step 50345, Loss 0.3674779236316681\n",
            "Step 50350, Loss 0.3117050230503082\n",
            "Step 50355, Loss 0.43458959460258484\n",
            "Step 50360, Loss 0.49673402309417725\n",
            "Step 50365, Loss 0.455326110124588\n",
            "Step 50370, Loss 0.3260708153247833\n",
            "Step 50375, Loss 0.3720870316028595\n",
            "Step 50380, Loss 0.38413575291633606\n",
            "Step 50385, Loss 0.4116411507129669\n",
            "Step 50390, Loss 0.4406225383281708\n",
            "Step 50395, Loss 0.4403434991836548\n",
            "Step 50400, Loss 0.4052518904209137\n",
            "Step 50405, Loss 0.37151721119880676\n",
            "Step 50410, Loss 0.3130239248275757\n",
            "Step 50415, Loss 0.34741684794425964\n",
            "Step 50420, Loss 0.2913821041584015\n",
            "Step 50425, Loss 0.29696762561798096\n",
            "Step 50430, Loss 0.43511703610420227\n",
            "Step 50435, Loss 0.3279106318950653\n",
            "Step 50440, Loss 0.5115544199943542\n",
            "Step 50445, Loss 0.5468470454216003\n",
            "Step 50450, Loss 0.482921838760376\n",
            "Step 50455, Loss 0.28518879413604736\n",
            "Step 50460, Loss 0.314179003238678\n",
            "Step 50465, Loss 0.40116941928863525\n",
            "Step 50470, Loss 0.3849858343601227\n",
            "Step 50475, Loss 0.4495561122894287\n",
            "Step 50480, Loss 0.2866283059120178\n",
            "Step 50485, Loss 0.3691658079624176\n",
            "Step 50490, Loss 0.4898366630077362\n",
            "Step 50495, Loss 0.5231013894081116\n",
            "Step 50500, Loss 0.37005987763404846\n",
            "Step 50505, Loss 0.39133763313293457\n",
            "Step 50510, Loss 0.33802521228790283\n",
            "Step 50515, Loss 0.4004347026348114\n",
            "Step 50520, Loss 0.3850127160549164\n",
            "Step 50525, Loss 0.34787267446517944\n",
            "Step 50530, Loss 0.40030255913734436\n",
            "Step 50535, Loss 0.29074960947036743\n",
            "Step 50540, Loss 0.42410555481910706\n",
            "model not updated\n",
            "Starting epoch 145/160, LR = [0.0024473205144810426]\n",
            "Step 50545, Loss 0.4032289385795593\n",
            "Step 50550, Loss 0.3048718571662903\n",
            "Step 50555, Loss 0.3309475779533386\n",
            "Step 50560, Loss 0.46445319056510925\n",
            "Step 50565, Loss 0.38515880703926086\n",
            "Step 50570, Loss 0.20438310503959656\n",
            "Step 50575, Loss 0.3022402226924896\n",
            "Step 50580, Loss 0.39641067385673523\n",
            "Step 50585, Loss 0.31127986311912537\n",
            "Step 50590, Loss 0.2901647090911865\n",
            "Step 50595, Loss 0.4396028518676758\n",
            "Step 50600, Loss 0.4481542408466339\n",
            "Step 50605, Loss 0.32084622979164124\n",
            "Step 50610, Loss 0.39072296023368835\n",
            "Step 50615, Loss 0.3917555809020996\n",
            "Step 50620, Loss 0.30255284905433655\n",
            "Step 50625, Loss 0.36396655440330505\n",
            "Step 50630, Loss 0.3119380474090576\n",
            "Step 50635, Loss 0.33174458146095276\n",
            "Step 50640, Loss 0.3875610828399658\n",
            "Step 50645, Loss 0.32690268754959106\n",
            "Step 50650, Loss 0.34580740332603455\n",
            "Step 50655, Loss 0.2559128403663635\n",
            "Step 50660, Loss 0.28934499621391296\n",
            "Step 50665, Loss 0.2726776897907257\n",
            "Step 50670, Loss 0.340568482875824\n",
            "Step 50675, Loss 0.26245933771133423\n",
            "Step 50680, Loss 0.2887958288192749\n",
            "Step 50685, Loss 0.3531504273414612\n",
            "Step 50690, Loss 0.2382577806711197\n",
            "Step 50695, Loss 0.47105714678764343\n",
            "Step 50700, Loss 0.3526332974433899\n",
            "Step 50705, Loss 0.41739046573638916\n",
            "Step 50710, Loss 0.3175244927406311\n",
            "Step 50715, Loss 0.3547501266002655\n",
            "Step 50720, Loss 0.3297743499279022\n",
            "Step 50725, Loss 0.2690020799636841\n",
            "Step 50730, Loss 0.2753339409828186\n",
            "Step 50735, Loss 0.3482731580734253\n",
            "Step 50740, Loss 0.4044741988182068\n",
            "Step 50745, Loss 0.38111603260040283\n",
            "Step 50750, Loss 0.33156442642211914\n",
            "Step 50755, Loss 0.38561680912971497\n",
            "Step 50760, Loss 0.31668737530708313\n",
            "Step 50765, Loss 0.538703203201294\n",
            "Step 50770, Loss 0.33201706409454346\n",
            "Step 50775, Loss 0.35471928119659424\n",
            "Step 50780, Loss 0.30955830216407776\n",
            "Step 50785, Loss 0.437778115272522\n",
            "Step 50790, Loss 0.4713876247406006\n",
            "Step 50795, Loss 0.42359814047813416\n",
            "Step 50800, Loss 0.27465566992759705\n",
            "Step 50805, Loss 0.3707742989063263\n",
            "Step 50810, Loss 0.2965185046195984\n",
            "Step 50815, Loss 0.4243388772010803\n",
            "Step 50820, Loss 0.45672935247421265\n",
            "Step 50825, Loss 0.43492814898490906\n",
            "Step 50830, Loss 0.29199767112731934\n",
            "Step 50835, Loss 0.38075876235961914\n",
            "Step 50840, Loss 0.3280062675476074\n",
            "Step 50845, Loss 0.5122909545898438\n",
            "Step 50850, Loss 0.27783215045928955\n",
            "Step 50855, Loss 0.31605368852615356\n",
            "Step 50860, Loss 0.3604632318019867\n",
            "Step 50865, Loss 0.30674824118614197\n",
            "Step 50870, Loss 0.4704859256744385\n",
            "Step 50875, Loss 0.3940555453300476\n",
            "Step 50880, Loss 0.3509843051433563\n",
            "Step 50885, Loss 0.33084604144096375\n",
            "Step 50890, Loss 0.3917894959449768\n",
            "model not updated\n",
            "Starting epoch 146/160, LR = [0.002153129983914736]\n",
            "Step 50895, Loss 0.3225231170654297\n",
            "Step 50900, Loss 0.5201553702354431\n",
            "Step 50905, Loss 0.36310482025146484\n",
            "Step 50910, Loss 0.4203466773033142\n",
            "Step 50915, Loss 0.37213605642318726\n",
            "Step 50920, Loss 0.47596728801727295\n",
            "Step 50925, Loss 0.2746970057487488\n",
            "Step 50930, Loss 0.29851657152175903\n",
            "Step 50935, Loss 0.31189826130867004\n",
            "Step 50940, Loss 0.43431299924850464\n",
            "Step 50945, Loss 0.25428470969200134\n",
            "Step 50950, Loss 0.30050602555274963\n",
            "Step 50955, Loss 0.29001477360725403\n",
            "Step 50960, Loss 0.30413562059402466\n",
            "Step 50965, Loss 0.4156741201877594\n",
            "Step 50970, Loss 0.2235213965177536\n",
            "Step 50975, Loss 0.39111095666885376\n",
            "Step 50980, Loss 0.4212615191936493\n",
            "Step 50985, Loss 0.35406509041786194\n",
            "Step 50990, Loss 0.33156535029411316\n",
            "Step 50995, Loss 0.3782101571559906\n",
            "Step 51000, Loss 0.4118715524673462\n",
            "Step 51005, Loss 0.2928312122821808\n",
            "Step 51010, Loss 0.415101021528244\n",
            "Step 51015, Loss 0.32958999276161194\n",
            "Step 51020, Loss 0.27693042159080505\n",
            "Step 51025, Loss 0.2952803373336792\n",
            "Step 51030, Loss 0.3334636092185974\n",
            "Step 51035, Loss 0.3188275992870331\n",
            "Step 51040, Loss 0.4404854476451874\n",
            "Step 51045, Loss 0.41494083404541016\n",
            "Step 51050, Loss 0.363938570022583\n",
            "Step 51055, Loss 0.3209587335586548\n",
            "Step 51060, Loss 0.3625490069389343\n",
            "Step 51065, Loss 0.41141068935394287\n",
            "Step 51070, Loss 0.38127532601356506\n",
            "Step 51075, Loss 0.26103466749191284\n",
            "Step 51080, Loss 0.28636500239372253\n",
            "Step 51085, Loss 0.4161945879459381\n",
            "Step 51090, Loss 0.4123390018939972\n",
            "Step 51095, Loss 0.3266303837299347\n",
            "Step 51100, Loss 0.36024707555770874\n",
            "Step 51105, Loss 0.3618137240409851\n",
            "Step 51110, Loss 0.3368643522262573\n",
            "Step 51115, Loss 0.4992575943470001\n",
            "Step 51120, Loss 0.24971966445446014\n",
            "Step 51125, Loss 0.20766395330429077\n",
            "Step 51130, Loss 0.2492026686668396\n",
            "Step 51135, Loss 0.4438125193119049\n",
            "Step 51140, Loss 0.3635127544403076\n",
            "Step 51145, Loss 0.24965877830982208\n",
            "Step 51150, Loss 0.273599237203598\n",
            "Step 51155, Loss 0.3209053575992584\n",
            "Step 51160, Loss 0.36915096640586853\n",
            "Step 51165, Loss 0.3201884627342224\n",
            "Step 51170, Loss 0.3027268350124359\n",
            "Step 51175, Loss 0.45050373673439026\n",
            "Step 51180, Loss 0.3008524179458618\n",
            "Step 51185, Loss 0.4316011965274811\n",
            "Step 51190, Loss 0.3523217439651489\n",
            "Step 51195, Loss 0.42713576555252075\n",
            "Step 51200, Loss 0.3175681531429291\n",
            "Step 51205, Loss 0.2747969925403595\n",
            "Step 51210, Loss 0.4367765486240387\n",
            "Step 51215, Loss 0.43365219235420227\n",
            "Step 51220, Loss 0.43599528074264526\n",
            "Step 51225, Loss 0.3072938323020935\n",
            "Step 51230, Loss 0.4555487036705017\n",
            "Step 51235, Loss 0.4228041172027588\n",
            "Step 51240, Loss 0.32216185331344604\n",
            "Step 51245, Loss 0.3193508982658386\n",
            "model not updated\n",
            "Starting epoch 147/160, LR = [0.001877385361460373]\n",
            "Step 51250, Loss 0.353257954120636\n",
            "Step 51255, Loss 0.4290781021118164\n",
            "Step 51260, Loss 0.40446144342422485\n",
            "Step 51265, Loss 0.30120548605918884\n",
            "Step 51270, Loss 0.23024770617485046\n",
            "Step 51275, Loss 0.38015216588974\n",
            "Step 51280, Loss 0.3756968677043915\n",
            "Step 51285, Loss 0.3086426854133606\n",
            "Step 51290, Loss 0.19982802867889404\n",
            "Step 51295, Loss 0.3879300355911255\n",
            "Step 51300, Loss 0.2714920938014984\n",
            "Step 51305, Loss 0.31490305066108704\n",
            "Step 51310, Loss 0.3331782817840576\n",
            "Step 51315, Loss 0.33692529797554016\n",
            "Step 51320, Loss 0.32502481341362\n",
            "Step 51325, Loss 0.3041156828403473\n",
            "Step 51330, Loss 0.36068248748779297\n",
            "Step 51335, Loss 0.38657861948013306\n",
            "Step 51340, Loss 0.29708367586135864\n",
            "Step 51345, Loss 0.29026517271995544\n",
            "Step 51350, Loss 0.26340457797050476\n",
            "Step 51355, Loss 0.24938350915908813\n",
            "Step 51360, Loss 0.32600486278533936\n",
            "Step 51365, Loss 0.3181067407131195\n",
            "Step 51370, Loss 0.25715020298957825\n",
            "Step 51375, Loss 0.3866063952445984\n",
            "Step 51380, Loss 0.3178393542766571\n",
            "Step 51385, Loss 0.32001277804374695\n",
            "Step 51390, Loss 0.32559558749198914\n",
            "Step 51395, Loss 0.3710923492908478\n",
            "Step 51400, Loss 0.35657721757888794\n",
            "Step 51405, Loss 0.3046438694000244\n",
            "Step 51410, Loss 0.33332201838493347\n",
            "Step 51415, Loss 0.37511348724365234\n",
            "Step 51420, Loss 0.39673909544944763\n",
            "Step 51425, Loss 0.3394618332386017\n",
            "Step 51430, Loss 0.3211266100406647\n",
            "Step 51435, Loss 0.27835091948509216\n",
            "Step 51440, Loss 0.4342188239097595\n",
            "Step 51445, Loss 0.3935695290565491\n",
            "Step 51450, Loss 0.28241318464279175\n",
            "Step 51455, Loss 0.2520425319671631\n",
            "Step 51460, Loss 0.35037001967430115\n",
            "Step 51465, Loss 0.390985369682312\n",
            "Step 51470, Loss 0.3446221947669983\n",
            "Step 51475, Loss 0.25136181712150574\n",
            "Step 51480, Loss 0.360596239566803\n",
            "Step 51485, Loss 0.3951078951358795\n",
            "Step 51490, Loss 0.28837496042251587\n",
            "Step 51495, Loss 0.26429808139801025\n",
            "Step 51500, Loss 0.28293466567993164\n",
            "Step 51505, Loss 0.42470067739486694\n",
            "Step 51510, Loss 0.3960670530796051\n",
            "Step 51515, Loss 0.3401627838611603\n",
            "Step 51520, Loss 0.2496270090341568\n",
            "Step 51525, Loss 0.3019689619541168\n",
            "Step 51530, Loss 0.2314557582139969\n",
            "Step 51535, Loss 0.27241066098213196\n",
            "Step 51540, Loss 0.24981093406677246\n",
            "Step 51545, Loss 0.3281710147857666\n",
            "Step 51550, Loss 0.3465166687965393\n",
            "Step 51555, Loss 0.3244170844554901\n",
            "Step 51560, Loss 0.41896215081214905\n",
            "Step 51565, Loss 0.283897340297699\n",
            "Step 51570, Loss 0.32514896988868713\n",
            "Step 51575, Loss 0.38344046473503113\n",
            "Step 51580, Loss 0.35994136333465576\n",
            "Step 51585, Loss 0.42758995294570923\n",
            "Step 51590, Loss 0.3806534707546234\n",
            "Step 51595, Loss 0.2767298221588135\n",
            "model not updated\n",
            "Starting epoch 148/160, LR = [0.001620192951918931]\n",
            "Step 51600, Loss 0.3538109362125397\n",
            "Step 51605, Loss 0.3154659569263458\n",
            "Step 51610, Loss 0.34512415528297424\n",
            "Step 51615, Loss 0.4046846330165863\n",
            "Step 51620, Loss 0.4776060879230499\n",
            "Step 51625, Loss 0.26892003417015076\n",
            "Step 51630, Loss 0.4520704746246338\n",
            "Step 51635, Loss 0.40849027037620544\n",
            "Step 51640, Loss 0.34093013405799866\n",
            "Step 51645, Loss 0.2554391622543335\n",
            "Step 51650, Loss 0.3462769091129303\n",
            "Step 51655, Loss 0.24589549005031586\n",
            "Step 51660, Loss 0.44562357664108276\n",
            "Step 51665, Loss 0.30845674872398376\n",
            "Step 51670, Loss 0.29674220085144043\n",
            "Step 51675, Loss 0.29932403564453125\n",
            "Step 51680, Loss 0.4375547766685486\n",
            "Step 51685, Loss 0.4300934374332428\n",
            "Step 51690, Loss 0.24480293691158295\n",
            "Step 51695, Loss 0.3358447253704071\n",
            "Step 51700, Loss 0.45619508624076843\n",
            "Step 51705, Loss 0.21976591646671295\n",
            "Step 51710, Loss 0.3713555335998535\n",
            "Step 51715, Loss 0.31536683440208435\n",
            "Step 51720, Loss 0.42455676198005676\n",
            "Step 51725, Loss 0.25785791873931885\n",
            "Step 51730, Loss 0.4230375289916992\n",
            "Step 51735, Loss 0.3614826202392578\n",
            "Step 51740, Loss 0.24747157096862793\n",
            "Step 51745, Loss 0.297141432762146\n",
            "Step 51750, Loss 0.3595867156982422\n",
            "Step 51755, Loss 0.33891570568084717\n",
            "Step 51760, Loss 0.27982670068740845\n",
            "Step 51765, Loss 0.3423025906085968\n",
            "Step 51770, Loss 0.48391637206077576\n",
            "Step 51775, Loss 0.38880839943885803\n",
            "Step 51780, Loss 0.311280757188797\n",
            "Step 51785, Loss 0.3475448787212372\n",
            "Step 51790, Loss 0.33276548981666565\n",
            "Step 51795, Loss 0.368269145488739\n",
            "Step 51800, Loss 0.34024882316589355\n",
            "Step 51805, Loss 0.3366217315196991\n",
            "Step 51810, Loss 0.4095989167690277\n",
            "Step 51815, Loss 0.3036816120147705\n",
            "Step 51820, Loss 0.3554973602294922\n",
            "Step 51825, Loss 0.3040614128112793\n",
            "Step 51830, Loss 0.36750829219818115\n",
            "Step 51835, Loss 0.38405829668045044\n",
            "Step 51840, Loss 0.2577810287475586\n",
            "Step 51845, Loss 0.3188619911670685\n",
            "Step 51850, Loss 0.2188721001148224\n",
            "Step 51855, Loss 0.45048269629478455\n",
            "Step 51860, Loss 0.31471145153045654\n",
            "Step 51865, Loss 0.2904825210571289\n",
            "Step 51870, Loss 0.28953027725219727\n",
            "Step 51875, Loss 0.43612807989120483\n",
            "Step 51880, Loss 0.4974345266819\n",
            "Step 51885, Loss 0.5488870143890381\n",
            "Step 51890, Loss 0.2664947509765625\n",
            "Step 51895, Loss 0.3439358174800873\n",
            "Step 51900, Loss 0.39226841926574707\n",
            "Step 51905, Loss 0.4751029908657074\n",
            "Step 51910, Loss 0.42743295431137085\n",
            "Step 51915, Loss 0.2753032147884369\n",
            "Step 51920, Loss 0.41137489676475525\n",
            "Step 51925, Loss 0.34459301829338074\n",
            "Step 51930, Loss 0.40461695194244385\n",
            "Step 51935, Loss 0.3796231150627136\n",
            "Step 51940, Loss 0.2877543568611145\n",
            "Step 51945, Loss 0.6053521633148193\n",
            "model not updated\n",
            "Starting epoch 149/160, LR = [0.0013816519078601998]\n",
            "Step 51950, Loss 0.37469005584716797\n",
            "Step 51955, Loss 0.3119831085205078\n",
            "Step 51960, Loss 0.33220812678337097\n",
            "Step 51965, Loss 0.39129239320755005\n",
            "Step 51970, Loss 0.2601229250431061\n",
            "Step 51975, Loss 0.3078179359436035\n",
            "Step 51980, Loss 0.3317978084087372\n",
            "Step 51985, Loss 0.35463660955429077\n",
            "Step 51990, Loss 0.3785473704338074\n",
            "Step 51995, Loss 0.3314868211746216\n",
            "Step 52000, Loss 0.3975622355937958\n",
            "Step 52005, Loss 0.4080441892147064\n",
            "Step 52010, Loss 0.3415985703468323\n",
            "Step 52015, Loss 0.38980093598365784\n",
            "Step 52020, Loss 0.39949920773506165\n",
            "Step 52025, Loss 0.32590168714523315\n",
            "Step 52030, Loss 0.24520790576934814\n",
            "Step 52035, Loss 0.24023115634918213\n",
            "Step 52040, Loss 0.4384959638118744\n",
            "Step 52045, Loss 0.23090532422065735\n",
            "Step 52050, Loss 0.4048142731189728\n",
            "Step 52055, Loss 0.3450421094894409\n",
            "Step 52060, Loss 0.27844351530075073\n",
            "Step 52065, Loss 0.2985239028930664\n",
            "Step 52070, Loss 0.3675660192966461\n",
            "Step 52075, Loss 0.42543771862983704\n",
            "Step 52080, Loss 0.43630412220954895\n",
            "Step 52085, Loss 0.436001718044281\n",
            "Step 52090, Loss 0.3234105110168457\n",
            "Step 52095, Loss 0.39730969071388245\n",
            "Step 52100, Loss 0.4001554250717163\n",
            "Step 52105, Loss 0.35505908727645874\n",
            "Step 52110, Loss 0.4434658885002136\n",
            "Step 52115, Loss 0.37394219636917114\n",
            "Step 52120, Loss 0.30545374751091003\n",
            "Step 52125, Loss 0.3519767224788666\n",
            "Step 52130, Loss 0.40548160672187805\n",
            "Step 52135, Loss 0.3761608898639679\n",
            "Step 52140, Loss 0.2682323455810547\n",
            "Step 52145, Loss 0.27876460552215576\n",
            "Step 52150, Loss 0.3582860827445984\n",
            "Step 52155, Loss 0.2837507724761963\n",
            "Step 52160, Loss 0.38684627413749695\n",
            "Step 52165, Loss 0.4299051761627197\n",
            "Step 52170, Loss 0.2684303820133209\n",
            "Step 52175, Loss 0.29131263494491577\n",
            "Step 52180, Loss 0.36348721385002136\n",
            "Step 52185, Loss 0.2959844768047333\n",
            "Step 52190, Loss 0.36387312412261963\n",
            "Step 52195, Loss 0.3853742778301239\n",
            "Step 52200, Loss 0.32067158818244934\n",
            "Step 52205, Loss 0.29700610041618347\n",
            "Step 52210, Loss 0.33552417159080505\n",
            "Step 52215, Loss 0.3971277177333832\n",
            "Step 52220, Loss 0.35220879316329956\n",
            "Step 52225, Loss 0.390049546957016\n",
            "Step 52230, Loss 0.3812740743160248\n",
            "Step 52235, Loss 0.3264073133468628\n",
            "Step 52240, Loss 0.28148260712623596\n",
            "Step 52245, Loss 0.3915545642375946\n",
            "Step 52250, Loss 0.42239946126937866\n",
            "Step 52255, Loss 0.30123502016067505\n",
            "Step 52260, Loss 0.19754727184772491\n",
            "Step 52265, Loss 0.3546544909477234\n",
            "Step 52270, Loss 0.3245983123779297\n",
            "Step 52275, Loss 0.31608954071998596\n",
            "Step 52280, Loss 0.3396156132221222\n",
            "Step 52285, Loss 0.2722547948360443\n",
            "Step 52290, Loss 0.36915647983551025\n",
            "Step 52295, Loss 0.26998206973075867\n",
            "model not updated\n",
            "Starting epoch 150/160, LR = [0.0011618541913974779]\n",
            "Step 52300, Loss 0.3937584459781647\n",
            "Step 52305, Loss 0.3976562023162842\n",
            "Step 52310, Loss 0.330797404050827\n",
            "Step 52315, Loss 0.4185512661933899\n",
            "Step 52320, Loss 0.3667716681957245\n",
            "Step 52325, Loss 0.32472720742225647\n",
            "Step 52330, Loss 0.20435431599617004\n",
            "Step 52335, Loss 0.3626145124435425\n",
            "Step 52340, Loss 0.27964282035827637\n",
            "Step 52345, Loss 0.28016817569732666\n",
            "Step 52350, Loss 0.34472209215164185\n",
            "Step 52355, Loss 0.2832149863243103\n",
            "Step 52360, Loss 0.36038991808891296\n",
            "Step 52365, Loss 0.32891136407852173\n",
            "Step 52370, Loss 0.4529555141925812\n",
            "Step 52375, Loss 0.38047948479652405\n",
            "Step 52380, Loss 0.2600860297679901\n",
            "Step 52385, Loss 0.3972053825855255\n",
            "Step 52390, Loss 0.46345341205596924\n",
            "Step 52395, Loss 0.4344016909599304\n",
            "Step 52400, Loss 0.3677719533443451\n",
            "Step 52405, Loss 0.2999386191368103\n",
            "Step 52410, Loss 0.26475852727890015\n",
            "Step 52415, Loss 0.3140645921230316\n",
            "Step 52420, Loss 0.3554215133190155\n",
            "Step 52425, Loss 0.25940194725990295\n",
            "Step 52430, Loss 0.3282274305820465\n",
            "Step 52435, Loss 0.408613383769989\n",
            "Step 52440, Loss 0.3190222680568695\n",
            "Step 52445, Loss 0.31503286957740784\n",
            "Step 52450, Loss 0.2198520451784134\n",
            "Step 52455, Loss 0.35201504826545715\n",
            "Step 52460, Loss 0.32922735810279846\n",
            "Step 52465, Loss 0.3374674916267395\n",
            "Step 52470, Loss 0.4361252188682556\n",
            "Step 52475, Loss 0.34124940633773804\n",
            "Step 52480, Loss 0.5480635166168213\n",
            "Step 52485, Loss 0.3774232864379883\n",
            "Step 52490, Loss 0.36913061141967773\n",
            "Step 52495, Loss 0.30865880846977234\n",
            "Step 52500, Loss 0.2638271749019623\n",
            "Step 52505, Loss 0.18147560954093933\n",
            "Step 52510, Loss 0.30218610167503357\n",
            "Step 52515, Loss 0.4570191204547882\n",
            "Step 52520, Loss 0.22566325962543488\n",
            "Step 52525, Loss 0.2735033333301544\n",
            "Step 52530, Loss 0.4363100528717041\n",
            "Step 52535, Loss 0.38729098439216614\n",
            "Step 52540, Loss 0.2988385558128357\n",
            "Step 52545, Loss 0.417061448097229\n",
            "Step 52550, Loss 0.41931697726249695\n",
            "Step 52555, Loss 0.30100148916244507\n",
            "Step 52560, Loss 0.3503512144088745\n",
            "Step 52565, Loss 0.3533908426761627\n",
            "Step 52570, Loss 0.35020682215690613\n",
            "Step 52575, Loss 0.3005872666835785\n",
            "Step 52580, Loss 0.3712938129901886\n",
            "Step 52585, Loss 0.34167489409446716\n",
            "Step 52590, Loss 0.3260110318660736\n",
            "Step 52595, Loss 0.4527861177921295\n",
            "Step 52600, Loss 0.3046252727508545\n",
            "Step 52605, Loss 0.33865371346473694\n",
            "Step 52610, Loss 0.3147677481174469\n",
            "Step 52615, Loss 0.3671222925186157\n",
            "Step 52620, Loss 0.33491164445877075\n",
            "Step 52625, Loss 0.2772662937641144\n",
            "Step 52630, Loss 0.4001142978668213\n",
            "Step 52635, Loss 0.2801925837993622\n",
            "Step 52640, Loss 0.3217632472515106\n",
            "Step 52645, Loss 0.20458419620990753\n",
            "model not updated\n",
            "Starting epoch 151/160, LR = [0.0009608845387345072]\n",
            "Step 52650, Loss 0.3821920156478882\n",
            "Step 52655, Loss 0.3190493881702423\n",
            "Step 52660, Loss 0.3387239873409271\n",
            "Step 52665, Loss 0.21598723530769348\n",
            "Step 52670, Loss 0.2619338035583496\n",
            "Step 52675, Loss 0.32214680314064026\n",
            "Step 52680, Loss 0.3518845736980438\n",
            "Step 52685, Loss 0.2875896096229553\n",
            "Step 52690, Loss 0.31934741139411926\n",
            "Step 52695, Loss 0.32868197560310364\n",
            "Step 52700, Loss 0.3894778788089752\n",
            "Step 52705, Loss 0.3918772041797638\n",
            "Step 52710, Loss 0.27690353989601135\n",
            "Step 52715, Loss 0.42109110951423645\n",
            "Step 52720, Loss 0.30130189657211304\n",
            "Step 52725, Loss 0.35978272557258606\n",
            "Step 52730, Loss 0.3576336205005646\n",
            "Step 52735, Loss 0.3974563777446747\n",
            "Step 52740, Loss 0.4064823389053345\n",
            "Step 52745, Loss 0.24755698442459106\n",
            "Step 52750, Loss 0.37655237317085266\n",
            "Step 52755, Loss 0.3842150568962097\n",
            "Step 52760, Loss 0.4564126133918762\n",
            "Step 52765, Loss 0.315414160490036\n",
            "Step 52770, Loss 0.24793288111686707\n",
            "Step 52775, Loss 0.2903496325016022\n",
            "Step 52780, Loss 0.4303429424762726\n",
            "Step 52785, Loss 0.42112916707992554\n",
            "Step 52790, Loss 0.3449099361896515\n",
            "Step 52795, Loss 0.28344982862472534\n",
            "Step 52800, Loss 0.39905592799186707\n",
            "Step 52805, Loss 0.34033674001693726\n",
            "Step 52810, Loss 0.3094886541366577\n",
            "Step 52815, Loss 0.45029038190841675\n",
            "Step 52820, Loss 0.3525753617286682\n",
            "Step 52825, Loss 0.3765445649623871\n",
            "Step 52830, Loss 0.39406490325927734\n",
            "Step 52835, Loss 0.3554025888442993\n",
            "Step 52840, Loss 0.3291666507720947\n",
            "Step 52845, Loss 0.3461151719093323\n",
            "Step 52850, Loss 0.29191380739212036\n",
            "Step 52855, Loss 0.4038434624671936\n",
            "Step 52860, Loss 0.393934041261673\n",
            "Step 52865, Loss 0.2996962070465088\n",
            "Step 52870, Loss 0.39671167731285095\n",
            "Step 52875, Loss 0.3188648223876953\n",
            "Step 52880, Loss 0.32241925597190857\n",
            "Step 52885, Loss 0.243670254945755\n",
            "Step 52890, Loss 0.4025682210922241\n",
            "Step 52895, Loss 0.3049866557121277\n",
            "Step 52900, Loss 0.31971579790115356\n",
            "Step 52905, Loss 0.22612670063972473\n",
            "Step 52910, Loss 0.35994642972946167\n",
            "Step 52915, Loss 0.42968645691871643\n",
            "Step 52920, Loss 0.33490967750549316\n",
            "Step 52925, Loss 0.3220546841621399\n",
            "Step 52930, Loss 0.28853505849838257\n",
            "Step 52935, Loss 0.4881388247013092\n",
            "Step 52940, Loss 0.3573392331600189\n",
            "Step 52945, Loss 0.35000014305114746\n",
            "Step 52950, Loss 0.28644368052482605\n",
            "Step 52955, Loss 0.48912954330444336\n",
            "Step 52960, Loss 0.20931848883628845\n",
            "Step 52965, Loss 0.4793141484260559\n",
            "Step 52970, Loss 0.28183817863464355\n",
            "Step 52975, Loss 0.42992091178894043\n",
            "Step 52980, Loss 0.4070485830307007\n",
            "Step 52985, Loss 0.30424290895462036\n",
            "Step 52990, Loss 0.3329704999923706\n",
            "Step 52995, Loss 0.333956241607666\n",
            "Step 53000, Loss 0.3787965476512909\n",
            "model not updated\n",
            "Starting epoch 152/160, LR = [0.0007788204274980256]\n",
            "Step 53005, Loss 0.42414066195487976\n",
            "Step 53010, Loss 0.23816029727458954\n",
            "Step 53015, Loss 0.38652756810188293\n",
            "Step 53020, Loss 0.41459009051322937\n",
            "Step 53025, Loss 0.4071146249771118\n",
            "Step 53030, Loss 0.267532616853714\n",
            "Step 53035, Loss 0.21693167090415955\n",
            "Step 53040, Loss 0.45359107851982117\n",
            "Step 53045, Loss 0.33971261978149414\n",
            "Step 53050, Loss 0.40084078907966614\n",
            "Step 53055, Loss 0.2709565758705139\n",
            "Step 53060, Loss 0.3390125632286072\n",
            "Step 53065, Loss 0.30290788412094116\n",
            "Step 53070, Loss 0.336112380027771\n",
            "Step 53075, Loss 0.31894657015800476\n",
            "Step 53080, Loss 0.35639917850494385\n",
            "Step 53085, Loss 0.35292914509773254\n",
            "Step 53090, Loss 0.30244848132133484\n",
            "Step 53095, Loss 0.3038689196109772\n",
            "Step 53100, Loss 0.30173665285110474\n",
            "Step 53105, Loss 0.32766181230545044\n",
            "Step 53110, Loss 0.3028208017349243\n",
            "Step 53115, Loss 0.27590087056159973\n",
            "Step 53120, Loss 0.3139807879924774\n",
            "Step 53125, Loss 0.38230493664741516\n",
            "Step 53130, Loss 0.3139653503894806\n",
            "Step 53135, Loss 0.2435041218996048\n",
            "Step 53140, Loss 0.3683214783668518\n",
            "Step 53145, Loss 0.3994419574737549\n",
            "Step 53150, Loss 0.4707229733467102\n",
            "Step 53155, Loss 0.3696841597557068\n",
            "Step 53160, Loss 0.22948506474494934\n",
            "Step 53165, Loss 0.4830894470214844\n",
            "Step 53170, Loss 0.3019799590110779\n",
            "Step 53175, Loss 0.17980355024337769\n",
            "Step 53180, Loss 0.29614847898483276\n",
            "Step 53185, Loss 0.2427801638841629\n",
            "Step 53190, Loss 0.32455968856811523\n",
            "Step 53195, Loss 0.3844777047634125\n",
            "Step 53200, Loss 0.23984621465206146\n",
            "Step 53205, Loss 0.3742646276950836\n",
            "Step 53210, Loss 0.34621235728263855\n",
            "Step 53215, Loss 0.31852757930755615\n",
            "Step 53220, Loss 0.3526802957057953\n",
            "Step 53225, Loss 0.436474084854126\n",
            "Step 53230, Loss 0.3189667761325836\n",
            "Step 53235, Loss 0.3526321053504944\n",
            "Step 53240, Loss 0.4413953721523285\n",
            "Step 53245, Loss 0.23200590908527374\n",
            "Step 53250, Loss 0.24285189807415009\n",
            "Step 53255, Loss 0.4243084192276001\n",
            "Step 53260, Loss 0.33804863691329956\n",
            "Step 53265, Loss 0.3314616084098816\n",
            "Step 53270, Loss 0.33820515871047974\n",
            "Step 53275, Loss 0.206046923995018\n",
            "Step 53280, Loss 0.28840312361717224\n",
            "Step 53285, Loss 0.3680934011936188\n",
            "Step 53290, Loss 0.46639367938041687\n",
            "Step 53295, Loss 0.28950586915016174\n",
            "Step 53300, Loss 0.403321236371994\n",
            "Step 53305, Loss 0.31696510314941406\n",
            "Step 53310, Loss 0.25649547576904297\n",
            "Step 53315, Loss 0.22911904752254486\n",
            "Step 53320, Loss 0.2982749044895172\n",
            "Step 53325, Loss 0.3431643545627594\n",
            "Step 53330, Loss 0.29583656787872314\n",
            "Step 53335, Loss 0.2742294371128082\n",
            "Step 53340, Loss 0.47502273321151733\n",
            "Step 53345, Loss 0.3777245581150055\n",
            "Step 53350, Loss 0.3766803443431854\n",
            "model updated\n",
            "Starting epoch 153/160, LR = [0.0006157320468686607]\n",
            "Step 53355, Loss 0.3572912812232971\n",
            "Step 53360, Loss 0.4171837568283081\n",
            "Step 53365, Loss 0.291445791721344\n",
            "Step 53370, Loss 0.47664982080459595\n",
            "Step 53375, Loss 0.4825280010700226\n",
            "Step 53380, Loss 0.25448060035705566\n",
            "Step 53385, Loss 0.3270793557167053\n",
            "Step 53390, Loss 0.4935806095600128\n",
            "Step 53395, Loss 0.3623923063278198\n",
            "Step 53400, Loss 0.2898576259613037\n",
            "Step 53405, Loss 0.3842688500881195\n",
            "Step 53410, Loss 0.24586760997772217\n",
            "Step 53415, Loss 0.34959515929222107\n",
            "Step 53420, Loss 0.29047808051109314\n",
            "Step 53425, Loss 0.25006720423698425\n",
            "Step 53430, Loss 0.4104389548301697\n",
            "Step 53435, Loss 0.22366707026958466\n",
            "Step 53440, Loss 0.2568480968475342\n",
            "Step 53445, Loss 0.3068181276321411\n",
            "Step 53450, Loss 0.3279423415660858\n",
            "Step 53455, Loss 0.4242379367351532\n",
            "Step 53460, Loss 0.32959508895874023\n",
            "Step 53465, Loss 0.27876704931259155\n",
            "Step 53470, Loss 0.43557554483413696\n",
            "Step 53475, Loss 0.2869681715965271\n",
            "Step 53480, Loss 0.30164024233818054\n",
            "Step 53485, Loss 0.23718105256557465\n",
            "Step 53490, Loss 0.30701130628585815\n",
            "Step 53495, Loss 0.37021371722221375\n",
            "Step 53500, Loss 0.21866396069526672\n",
            "Step 53505, Loss 0.26072049140930176\n",
            "Step 53510, Loss 0.22662046551704407\n",
            "Step 53515, Loss 0.3595837950706482\n",
            "Step 53520, Loss 0.3439860939979553\n",
            "Step 53525, Loss 0.31398534774780273\n",
            "Step 53530, Loss 0.3251069486141205\n",
            "Step 53535, Loss 0.5546820163726807\n",
            "Step 53540, Loss 0.3240402638912201\n",
            "Step 53545, Loss 0.46445155143737793\n",
            "Step 53550, Loss 0.24747291207313538\n",
            "Step 53555, Loss 0.37247997522354126\n",
            "Step 53560, Loss 0.2697850465774536\n",
            "Step 53565, Loss 0.44146275520324707\n",
            "Step 53570, Loss 0.1969495415687561\n",
            "Step 53575, Loss 0.39008817076683044\n",
            "Step 53580, Loss 0.3493958115577698\n",
            "Step 53585, Loss 0.3737492859363556\n",
            "Step 53590, Loss 0.2582230865955353\n",
            "Step 53595, Loss 0.26554903388023376\n",
            "Step 53600, Loss 0.294375479221344\n",
            "Step 53605, Loss 0.30473950505256653\n",
            "Step 53610, Loss 0.5361592769622803\n",
            "Step 53615, Loss 0.30934298038482666\n",
            "Step 53620, Loss 0.2926284372806549\n",
            "Step 53625, Loss 0.3549686074256897\n",
            "Step 53630, Loss 0.3785325288772583\n",
            "Step 53635, Loss 0.27405229210853577\n",
            "Step 53640, Loss 0.2972908914089203\n",
            "Step 53645, Loss 0.38849684596061707\n",
            "Step 53650, Loss 0.301603227853775\n",
            "Step 53655, Loss 0.39442306756973267\n",
            "Step 53660, Loss 0.3328091502189636\n",
            "Step 53665, Loss 0.3096054196357727\n",
            "Step 53670, Loss 0.3776665925979614\n",
            "Step 53675, Loss 0.29718849062919617\n",
            "Step 53680, Loss 0.3207080364227295\n",
            "Step 53685, Loss 0.35550451278686523\n",
            "Step 53690, Loss 0.3669625520706177\n",
            "Step 53695, Loss 0.3578772246837616\n",
            "Step 53700, Loss 0.3649423122406006\n",
            "model not updated\n",
            "Starting epoch 154/160, LR = [0.00047168227052166997]\n",
            "Step 53705, Loss 0.21154144406318665\n",
            "Step 53710, Loss 0.2743625044822693\n",
            "Step 53715, Loss 0.3355005085468292\n",
            "Step 53720, Loss 0.36933180689811707\n",
            "Step 53725, Loss 0.331419438123703\n",
            "Step 53730, Loss 0.25635668635368347\n",
            "Step 53735, Loss 0.30431145429611206\n",
            "Step 53740, Loss 0.2552223801612854\n",
            "Step 53745, Loss 0.34935325384140015\n",
            "Step 53750, Loss 0.3000946640968323\n",
            "Step 53755, Loss 0.3213208019733429\n",
            "Step 53760, Loss 0.26182153820991516\n",
            "Step 53765, Loss 0.22885286808013916\n",
            "Step 53770, Loss 0.34209775924682617\n",
            "Step 53775, Loss 0.20331916213035583\n",
            "Step 53780, Loss 0.3011717200279236\n",
            "Step 53785, Loss 0.3627009689807892\n",
            "Step 53790, Loss 0.24658022820949554\n",
            "Step 53795, Loss 0.39233219623565674\n",
            "Step 53800, Loss 0.3401678800582886\n",
            "Step 53805, Loss 0.3895081877708435\n",
            "Step 53810, Loss 0.37199413776397705\n",
            "Step 53815, Loss 0.38955751061439514\n",
            "Step 53820, Loss 0.3399176299571991\n",
            "Step 53825, Loss 0.32135510444641113\n",
            "Step 53830, Loss 0.28065311908721924\n",
            "Step 53835, Loss 0.2414906620979309\n",
            "Step 53840, Loss 0.441154420375824\n",
            "Step 53845, Loss 0.2238355278968811\n",
            "Step 53850, Loss 0.4492800831794739\n",
            "Step 53855, Loss 0.35947689414024353\n",
            "Step 53860, Loss 0.3378186523914337\n",
            "Step 53865, Loss 0.4629942774772644\n",
            "Step 53870, Loss 0.3240334391593933\n",
            "Step 53875, Loss 0.3460671007633209\n",
            "Step 53880, Loss 0.3749710023403168\n",
            "Step 53885, Loss 0.3913583755493164\n",
            "Step 53890, Loss 0.31206372380256653\n",
            "Step 53895, Loss 0.23341679573059082\n",
            "Step 53900, Loss 0.30158209800720215\n",
            "Step 53905, Loss 0.18612068891525269\n",
            "Step 53910, Loss 0.25704824924468994\n",
            "Step 53915, Loss 0.3749866187572479\n",
            "Step 53920, Loss 0.30487048625946045\n",
            "Step 53925, Loss 0.3902731239795685\n",
            "Step 53930, Loss 0.28667739033699036\n",
            "Step 53935, Loss 0.2692076861858368\n",
            "Step 53940, Loss 0.3186207115650177\n",
            "Step 53945, Loss 0.3827630579471588\n",
            "Step 53950, Loss 0.3322063386440277\n",
            "Step 53955, Loss 0.24105696380138397\n",
            "Step 53960, Loss 0.32118165493011475\n",
            "Step 53965, Loss 0.3706686496734619\n",
            "Step 53970, Loss 0.33538785576820374\n",
            "Step 53975, Loss 0.29706934094429016\n",
            "Step 53980, Loss 0.3600463569164276\n",
            "Step 53985, Loss 0.2465876191854477\n",
            "Step 53990, Loss 0.42890632152557373\n",
            "Step 53995, Loss 0.26199260354042053\n",
            "Step 54000, Loss 0.3990570902824402\n",
            "Step 54005, Loss 0.3311412036418915\n",
            "Step 54010, Loss 0.2761200964450836\n",
            "Step 54015, Loss 0.29021331667900085\n",
            "Step 54020, Loss 0.33044061064720154\n",
            "Step 54025, Loss 0.40789690613746643\n",
            "Step 54030, Loss 0.3167875409126282\n",
            "Step 54035, Loss 0.3443225622177124\n",
            "Step 54040, Loss 0.5913476943969727\n",
            "Step 54045, Loss 0.35404351353645325\n",
            "Step 54050, Loss 0.2453603893518448\n",
            "model not updated\n",
            "Starting epoch 155/160, LR = [0.0003467266323879564]\n",
            "Step 54055, Loss 0.2614552676677704\n",
            "Step 54060, Loss 0.36527392268180847\n",
            "Step 54065, Loss 0.3183295726776123\n",
            "Step 54070, Loss 0.4165167212486267\n",
            "Step 54075, Loss 0.4139079749584198\n",
            "Step 54080, Loss 0.267295777797699\n",
            "Step 54085, Loss 0.48878827691078186\n",
            "Step 54090, Loss 0.31625646352767944\n",
            "Step 54095, Loss 0.4068557322025299\n",
            "Step 54100, Loss 0.4727335274219513\n",
            "Step 54105, Loss 0.4710299074649811\n",
            "Step 54110, Loss 0.28923243284225464\n",
            "Step 54115, Loss 0.3014845550060272\n",
            "Step 54120, Loss 0.3998236060142517\n",
            "Step 54125, Loss 0.24889303743839264\n",
            "Step 54130, Loss 0.29741787910461426\n",
            "Step 54135, Loss 0.3518338203430176\n",
            "Step 54140, Loss 0.28222331404685974\n",
            "Step 54145, Loss 0.28989481925964355\n",
            "Step 54150, Loss 0.3336581587791443\n",
            "Step 54155, Loss 0.3876807987689972\n",
            "Step 54160, Loss 0.3281018137931824\n",
            "Step 54165, Loss 0.4375695288181305\n",
            "Step 54170, Loss 0.39232611656188965\n",
            "Step 54175, Loss 0.3103691339492798\n",
            "Step 54180, Loss 0.48667848110198975\n",
            "Step 54185, Loss 0.36625584959983826\n",
            "Step 54190, Loss 0.3494833707809448\n",
            "Step 54195, Loss 0.3397826850414276\n",
            "Step 54200, Loss 0.32278358936309814\n",
            "Step 54205, Loss 0.3766126036643982\n",
            "Step 54210, Loss 0.4418976604938507\n",
            "Step 54215, Loss 0.3780602812767029\n",
            "Step 54220, Loss 0.4185810983181\n",
            "Step 54225, Loss 0.31467491388320923\n",
            "Step 54230, Loss 0.33273544907569885\n",
            "Step 54235, Loss 0.29091203212738037\n",
            "Step 54240, Loss 0.3143008351325989\n",
            "Step 54245, Loss 0.3951287567615509\n",
            "Step 54250, Loss 0.26651206612586975\n",
            "Step 54255, Loss 0.40729907155036926\n",
            "Step 54260, Loss 0.3902395963668823\n",
            "Step 54265, Loss 0.5824623703956604\n",
            "Step 54270, Loss 0.4385559856891632\n",
            "Step 54275, Loss 0.2725565433502197\n",
            "Step 54280, Loss 0.4530769884586334\n",
            "Step 54285, Loss 0.2685534656047821\n",
            "Step 54290, Loss 0.3769029676914215\n",
            "Step 54295, Loss 0.4296188950538635\n",
            "Step 54300, Loss 0.43248283863067627\n",
            "Step 54305, Loss 0.25486183166503906\n",
            "Step 54310, Loss 0.24991042912006378\n",
            "Step 54315, Loss 0.3891904652118683\n",
            "Step 54320, Loss 0.3230072557926178\n",
            "Step 54325, Loss 0.4163152277469635\n",
            "Step 54330, Loss 0.4524860084056854\n",
            "Step 54335, Loss 0.32240748405456543\n",
            "Step 54340, Loss 0.3438872992992401\n",
            "Step 54345, Loss 0.255282461643219\n",
            "Step 54350, Loss 0.30197930335998535\n",
            "Step 54355, Loss 0.3550734221935272\n",
            "Step 54360, Loss 0.26704299449920654\n",
            "Step 54365, Loss 0.4214298129081726\n",
            "Step 54370, Loss 0.2912967801094055\n",
            "Step 54375, Loss 0.30640751123428345\n",
            "Step 54380, Loss 0.3068244457244873\n",
            "Step 54385, Loss 0.18903793394565582\n",
            "Step 54390, Loss 0.4721880555152893\n",
            "Step 54395, Loss 0.2986818552017212\n",
            "Step 54400, Loss 0.3592616021633148\n",
            "model not updated\n",
            "Starting epoch 156/160, LR = [0.00024091330524465915]\n",
            "Step 54405, Loss 0.2588379681110382\n",
            "Step 54410, Loss 0.3549903333187103\n",
            "Step 54415, Loss 0.30361315608024597\n",
            "Step 54420, Loss 0.25367650389671326\n",
            "Step 54425, Loss 0.32825618982315063\n",
            "Step 54430, Loss 0.357707142829895\n",
            "Step 54435, Loss 0.32283100485801697\n",
            "Step 54440, Loss 0.2743378281593323\n",
            "Step 54445, Loss 0.2578708231449127\n",
            "Step 54450, Loss 0.27155712246894836\n",
            "Step 54455, Loss 0.33822792768478394\n",
            "Step 54460, Loss 0.2840798795223236\n",
            "Step 54465, Loss 0.2551397681236267\n",
            "Step 54470, Loss 0.3139226734638214\n",
            "Step 54475, Loss 0.4259726405143738\n",
            "Step 54480, Loss 0.40879371762275696\n",
            "Step 54485, Loss 0.400768518447876\n",
            "Step 54490, Loss 0.32303449511528015\n",
            "Step 54495, Loss 0.36479151248931885\n",
            "Step 54500, Loss 0.32989516854286194\n",
            "Step 54505, Loss 0.3599209487438202\n",
            "Step 54510, Loss 0.34523963928222656\n",
            "Step 54515, Loss 0.37548261880874634\n",
            "Step 54520, Loss 0.46330690383911133\n",
            "Step 54525, Loss 0.2864466607570648\n",
            "Step 54530, Loss 0.33657193183898926\n",
            "Step 54535, Loss 0.2670886516571045\n",
            "Step 54540, Loss 0.42332735657691956\n",
            "Step 54545, Loss 0.471172958612442\n",
            "Step 54550, Loss 0.31672951579093933\n",
            "Step 54555, Loss 0.40923044085502625\n",
            "Step 54560, Loss 0.25974172353744507\n",
            "Step 54565, Loss 0.3740682303905487\n",
            "Step 54570, Loss 0.2999878525733948\n",
            "Step 54575, Loss 0.19081389904022217\n",
            "Step 54580, Loss 0.31592652201652527\n",
            "Step 54585, Loss 0.3773658573627472\n",
            "Step 54590, Loss 0.27916350960731506\n",
            "Step 54595, Loss 0.39260226488113403\n",
            "Step 54600, Loss 0.31383201479911804\n",
            "Step 54605, Loss 0.34838244318962097\n",
            "Step 54610, Loss 0.2827030420303345\n",
            "Step 54615, Loss 0.3904779255390167\n",
            "Step 54620, Loss 0.2598585784435272\n",
            "Step 54625, Loss 0.3998045027256012\n",
            "Step 54630, Loss 0.36382487416267395\n",
            "Step 54635, Loss 0.23199716210365295\n",
            "Step 54640, Loss 0.26551762223243713\n",
            "Step 54645, Loss 0.30699843168258667\n",
            "Step 54650, Loss 0.3175964951515198\n",
            "Step 54655, Loss 0.5162018537521362\n",
            "Step 54660, Loss 0.2798023521900177\n",
            "Step 54665, Loss 0.3548814058303833\n",
            "Step 54670, Loss 0.367170512676239\n",
            "Step 54675, Loss 0.3262895345687866\n",
            "Step 54680, Loss 0.3228144645690918\n",
            "Step 54685, Loss 0.38409823179244995\n",
            "Step 54690, Loss 0.23969940841197968\n",
            "Step 54695, Loss 0.34812766313552856\n",
            "Step 54700, Loss 0.21225973963737488\n",
            "Step 54705, Loss 0.45234620571136475\n",
            "Step 54710, Loss 0.32034361362457275\n",
            "Step 54715, Loss 0.21133747696876526\n",
            "Step 54720, Loss 0.27925905585289\n",
            "Step 54725, Loss 0.2979468107223511\n",
            "Step 54730, Loss 0.33642885088920593\n",
            "Step 54735, Loss 0.3227939307689667\n",
            "Step 54740, Loss 0.31816035509109497\n",
            "Step 54745, Loss 0.38916894793510437\n",
            "Step 54750, Loss 0.307951420545578\n",
            "Step 54755, Loss 0.4780902862548828\n",
            "model not updated\n",
            "Starting epoch 157/160, LR = [0.00015428308214363154]\n",
            "Step 54760, Loss 0.29590463638305664\n",
            "Step 54765, Loss 0.2781536281108856\n",
            "Step 54770, Loss 0.23437395691871643\n",
            "Step 54775, Loss 0.374588280916214\n",
            "Step 54780, Loss 0.28798556327819824\n",
            "Step 54785, Loss 0.24065212905406952\n",
            "Step 54790, Loss 0.384007066488266\n",
            "Step 54795, Loss 0.23400355875492096\n",
            "Step 54800, Loss 0.2536405026912689\n",
            "Step 54805, Loss 0.32232537865638733\n",
            "Step 54810, Loss 0.32194608449935913\n",
            "Step 54815, Loss 0.37358179688453674\n",
            "Step 54820, Loss 0.2586832046508789\n",
            "Step 54825, Loss 0.2774389982223511\n",
            "Step 54830, Loss 0.2794845998287201\n",
            "Step 54835, Loss 0.4313642084598541\n",
            "Step 54840, Loss 0.2765982747077942\n",
            "Step 54845, Loss 0.26581594347953796\n",
            "Step 54850, Loss 0.2823329567909241\n",
            "Step 54855, Loss 0.25050783157348633\n",
            "Step 54860, Loss 0.33408263325691223\n",
            "Step 54865, Loss 0.29269352555274963\n",
            "Step 54870, Loss 0.35345783829689026\n",
            "Step 54875, Loss 0.37440216541290283\n",
            "Step 54880, Loss 0.2975306510925293\n",
            "Step 54885, Loss 0.3637339770793915\n",
            "Step 54890, Loss 0.3480851948261261\n",
            "Step 54895, Loss 0.40248608589172363\n",
            "Step 54900, Loss 0.2052866369485855\n",
            "Step 54905, Loss 0.26071304082870483\n",
            "Step 54910, Loss 0.4129124879837036\n",
            "Step 54915, Loss 0.3319535255432129\n",
            "Step 54920, Loss 0.38735896348953247\n",
            "Step 54925, Loss 0.289179265499115\n",
            "Step 54930, Loss 0.25774794816970825\n",
            "Step 54935, Loss 0.26487502455711365\n",
            "Step 54940, Loss 0.24070623517036438\n",
            "Step 54945, Loss 0.32417213916778564\n",
            "Step 54950, Loss 0.1976281702518463\n",
            "Step 54955, Loss 0.23701658844947815\n",
            "Step 54960, Loss 0.32796165347099304\n",
            "Step 54965, Loss 0.2521686255931854\n",
            "Step 54970, Loss 0.26743850111961365\n",
            "Step 54975, Loss 0.4176918566226959\n",
            "Step 54980, Loss 0.37163084745407104\n",
            "Step 54985, Loss 0.3045079708099365\n",
            "Step 54990, Loss 0.30942460894584656\n",
            "Step 54995, Loss 0.2549392282962799\n",
            "Step 55000, Loss 0.38704198598861694\n",
            "Step 55005, Loss 0.16889621317386627\n",
            "Step 55010, Loss 0.3819819986820221\n",
            "Step 55015, Loss 0.5996554493904114\n",
            "Step 55020, Loss 0.30603668093681335\n",
            "Step 55025, Loss 0.38283950090408325\n",
            "Step 55030, Loss 0.24156448245048523\n",
            "Step 55035, Loss 0.3256644308567047\n",
            "Step 55040, Loss 0.3033742308616638\n",
            "Step 55045, Loss 0.346122682094574\n",
            "Step 55050, Loss 0.3038449287414551\n",
            "Step 55055, Loss 0.35989266633987427\n",
            "Step 55060, Loss 0.30417805910110474\n",
            "Step 55065, Loss 0.3841695189476013\n",
            "Step 55070, Loss 0.33196642994880676\n",
            "Step 55075, Loss 0.29104694724082947\n",
            "Step 55080, Loss 0.3191087543964386\n",
            "Step 55085, Loss 0.30453070998191833\n",
            "Step 55090, Loss 0.2647349238395691\n",
            "Step 55095, Loss 0.31206247210502625\n",
            "Step 55100, Loss 0.5309168696403503\n",
            "Step 55105, Loss 0.46559378504753113\n",
            "model not updated\n",
            "Starting epoch 158/160, LR = [8.686936068497253e-05]\n",
            "Step 55110, Loss 0.2514253556728363\n",
            "Step 55115, Loss 0.25652068853378296\n",
            "Step 55120, Loss 0.30675017833709717\n",
            "Step 55125, Loss 0.2262822538614273\n",
            "Step 55130, Loss 0.34955641627311707\n",
            "Step 55135, Loss 0.2400856465101242\n",
            "Step 55140, Loss 0.28138819336891174\n",
            "Step 55145, Loss 0.42219337821006775\n",
            "Step 55150, Loss 0.2917758524417877\n",
            "Step 55155, Loss 0.4055081307888031\n",
            "Step 55160, Loss 0.40553897619247437\n",
            "Step 55165, Loss 0.28058338165283203\n",
            "Step 55170, Loss 0.31750354170799255\n",
            "Step 55175, Loss 0.34104132652282715\n",
            "Step 55180, Loss 0.4272269904613495\n",
            "Step 55185, Loss 0.41218411922454834\n",
            "Step 55190, Loss 0.21466457843780518\n",
            "Step 55195, Loss 0.3032829463481903\n",
            "Step 55200, Loss 0.2301924228668213\n",
            "Step 55205, Loss 0.38390713930130005\n",
            "Step 55210, Loss 0.29232510924339294\n",
            "Step 55215, Loss 0.23033687472343445\n",
            "Step 55220, Loss 0.3936229944229126\n",
            "Step 55225, Loss 0.3732037842273712\n",
            "Step 55230, Loss 0.2810128927230835\n",
            "Step 55235, Loss 0.4008147120475769\n",
            "Step 55240, Loss 0.2111813873052597\n",
            "Step 55245, Loss 0.3146255612373352\n",
            "Step 55250, Loss 0.35989484190940857\n",
            "Step 55255, Loss 0.31879159808158875\n",
            "Step 55260, Loss 0.4361533522605896\n",
            "Step 55265, Loss 0.26196882128715515\n",
            "Step 55270, Loss 0.19922254979610443\n",
            "Step 55275, Loss 0.3724209666252136\n",
            "Step 55280, Loss 0.3399517238140106\n",
            "Step 55285, Loss 0.3125531077384949\n",
            "Step 55290, Loss 0.37778815627098083\n",
            "Step 55295, Loss 0.4194766581058502\n",
            "Step 55300, Loss 0.2928979992866516\n",
            "Step 55305, Loss 0.36133790016174316\n",
            "Step 55310, Loss 0.3831666111946106\n",
            "Step 55315, Loss 0.3007969260215759\n",
            "Step 55320, Loss 0.29855668544769287\n",
            "Step 55325, Loss 0.3508632481098175\n",
            "Step 55330, Loss 0.42327573895454407\n",
            "Step 55335, Loss 0.25181278586387634\n",
            "Step 55340, Loss 0.3260584771633148\n",
            "Step 55345, Loss 0.32924899458885193\n",
            "Step 55350, Loss 0.4380863606929779\n",
            "Step 55355, Loss 0.35279205441474915\n",
            "Step 55360, Loss 0.3307454288005829\n",
            "Step 55365, Loss 0.3785821795463562\n",
            "Step 55370, Loss 0.4857040047645569\n",
            "Step 55375, Loss 0.3877585232257843\n",
            "Step 55380, Loss 0.23950228095054626\n",
            "Step 55385, Loss 0.37076836824417114\n",
            "Step 55390, Loss 0.3378761112689972\n",
            "Step 55395, Loss 0.4588407278060913\n",
            "Step 55400, Loss 0.28937095403671265\n",
            "Step 55405, Loss 0.2194122076034546\n",
            "Step 55410, Loss 0.36056408286094666\n",
            "Step 55415, Loss 0.3411863148212433\n",
            "Step 55420, Loss 0.28455066680908203\n",
            "Step 55425, Loss 0.335872083902359\n",
            "Step 55430, Loss 0.29367130994796753\n",
            "Step 55435, Loss 0.3934936225414276\n",
            "Step 55440, Loss 0.36757999658584595\n",
            "Step 55445, Loss 0.4375610947608948\n",
            "Step 55450, Loss 0.23906275629997253\n",
            "Step 55455, Loss 0.4005362391471863\n",
            "model not updated\n",
            "Starting epoch 159/160, LR = [3.869813014157294e-05]\n",
            "Step 55460, Loss 0.3028215765953064\n",
            "Step 55465, Loss 0.3049333989620209\n",
            "Step 55470, Loss 0.3654934763908386\n",
            "Step 55475, Loss 0.28502774238586426\n",
            "Step 55480, Loss 0.39353427290916443\n",
            "Step 55485, Loss 0.27220332622528076\n",
            "Step 55490, Loss 0.3143710196018219\n",
            "Step 55495, Loss 0.2957882881164551\n",
            "Step 55500, Loss 0.23340928554534912\n",
            "Step 55505, Loss 0.28787288069725037\n",
            "Step 55510, Loss 0.289592981338501\n",
            "Step 55515, Loss 0.465501606464386\n",
            "Step 55520, Loss 0.2753468453884125\n",
            "Step 55525, Loss 0.36803770065307617\n",
            "Step 55530, Loss 0.33697912096977234\n",
            "Step 55535, Loss 0.33344000577926636\n",
            "Step 55540, Loss 0.2890152037143707\n",
            "Step 55545, Loss 0.35089465975761414\n",
            "Step 55550, Loss 0.20128823816776276\n",
            "Step 55555, Loss 0.33080101013183594\n",
            "Step 55560, Loss 0.3841959238052368\n",
            "Step 55565, Loss 0.2337220013141632\n",
            "Step 55570, Loss 0.2551231384277344\n",
            "Step 55575, Loss 0.49611467123031616\n",
            "Step 55580, Loss 0.2154485434293747\n",
            "Step 55585, Loss 0.3244326114654541\n",
            "Step 55590, Loss 0.3945102095603943\n",
            "Step 55595, Loss 0.2305002510547638\n",
            "Step 55600, Loss 0.31145885586738586\n",
            "Step 55605, Loss 0.2356015145778656\n",
            "Step 55610, Loss 0.21818408370018005\n",
            "Step 55615, Loss 0.29942405223846436\n",
            "Step 55620, Loss 0.3004922866821289\n",
            "Step 55625, Loss 0.34291204810142517\n",
            "Step 55630, Loss 0.3479703366756439\n",
            "Step 55635, Loss 0.29214492440223694\n",
            "Step 55640, Loss 0.38568466901779175\n",
            "Step 55645, Loss 0.39247071743011475\n",
            "Step 55650, Loss 0.3641515076160431\n",
            "Step 55655, Loss 0.29192057251930237\n",
            "Step 55660, Loss 0.3358822762966156\n",
            "Step 55665, Loss 0.35531771183013916\n",
            "Step 55670, Loss 0.23453959822654724\n",
            "Step 55675, Loss 0.33747321367263794\n",
            "Step 55680, Loss 0.3461211621761322\n",
            "Step 55685, Loss 0.32006359100341797\n",
            "Step 55690, Loss 0.37297743558883667\n",
            "Step 55695, Loss 0.4579584002494812\n",
            "Step 55700, Loss 0.38527777791023254\n",
            "Step 55705, Loss 0.2830623388290405\n",
            "Step 55710, Loss 0.24850568175315857\n",
            "Step 55715, Loss 0.25208109617233276\n",
            "Step 55720, Loss 0.3264623284339905\n",
            "Step 55725, Loss 0.3062587380409241\n",
            "Step 55730, Loss 0.3990499675273895\n",
            "Step 55735, Loss 0.26986730098724365\n",
            "Step 55740, Loss 0.4582369029521942\n",
            "Step 55745, Loss 0.27821657061576843\n",
            "Step 55750, Loss 0.35658714175224304\n",
            "Step 55755, Loss 0.4576256275177002\n",
            "Step 55760, Loss 0.37869715690612793\n",
            "Step 55765, Loss 0.39176714420318604\n",
            "Step 55770, Loss 0.3721189498901367\n",
            "Step 55775, Loss 0.4040795862674713\n",
            "Step 55780, Loss 0.29310184717178345\n",
            "Step 55785, Loss 0.3304619789123535\n",
            "Step 55790, Loss 0.41582801938056946\n",
            "Step 55795, Loss 0.3519032895565033\n",
            "Step 55800, Loss 0.295122355222702\n",
            "Step 55805, Loss 0.2915184497833252\n",
            "model not updated\n",
            "Starting epoch 160/160, LR = [9.787961439795215e-06]\n",
            "Step 55810, Loss 0.2642677128314972\n",
            "Step 55815, Loss 0.49537551403045654\n",
            "Step 55820, Loss 0.22492824494838715\n",
            "Step 55825, Loss 0.36895811557769775\n",
            "Step 55830, Loss 0.32881924510002136\n",
            "Step 55835, Loss 0.2708040773868561\n",
            "Step 55840, Loss 0.35905665159225464\n",
            "Step 55845, Loss 0.3007377088069916\n",
            "Step 55850, Loss 0.35375502705574036\n",
            "Step 55855, Loss 0.35857829451560974\n",
            "Step 55860, Loss 0.19845566153526306\n",
            "Step 55865, Loss 0.3594876527786255\n",
            "Step 55870, Loss 0.26051849126815796\n",
            "Step 55875, Loss 0.24904313683509827\n",
            "Step 55880, Loss 0.3175632655620575\n",
            "Step 55885, Loss 0.3155724108219147\n",
            "Step 55890, Loss 0.3266015648841858\n",
            "Step 55895, Loss 0.40673840045928955\n",
            "Step 55900, Loss 0.34973353147506714\n",
            "Step 55905, Loss 0.30328547954559326\n",
            "Step 55910, Loss 0.2872839570045471\n",
            "Step 55915, Loss 0.43805232644081116\n",
            "Step 55920, Loss 0.3166697919368744\n",
            "Step 55925, Loss 0.2494732141494751\n",
            "Step 55930, Loss 0.3536510169506073\n",
            "Step 55935, Loss 0.3234219253063202\n",
            "Step 55940, Loss 0.38983845710754395\n",
            "Step 55945, Loss 0.3441625237464905\n",
            "Step 55950, Loss 0.32447943091392517\n",
            "Step 55955, Loss 0.30656611919403076\n",
            "Step 55960, Loss 0.4087853729724884\n",
            "Step 55965, Loss 0.4343758821487427\n",
            "Step 55970, Loss 0.2604462504386902\n",
            "Step 55975, Loss 0.2585357129573822\n",
            "Step 55980, Loss 0.2589850127696991\n",
            "Step 55985, Loss 0.4662569463253021\n",
            "Step 55990, Loss 0.4080091118812561\n",
            "Step 55995, Loss 0.2895890474319458\n",
            "Step 56000, Loss 0.2975250780582428\n",
            "Step 56005, Loss 0.36401480436325073\n",
            "Step 56010, Loss 0.312843382358551\n",
            "Step 56015, Loss 0.34407317638397217\n",
            "Step 56020, Loss 0.31844204664230347\n",
            "Step 56025, Loss 0.2840713560581207\n",
            "Step 56030, Loss 0.364864706993103\n",
            "Step 56035, Loss 0.34938114881515503\n",
            "Step 56040, Loss 0.3471410274505615\n",
            "Step 56045, Loss 0.48072323203086853\n",
            "Step 56050, Loss 0.502191424369812\n",
            "Step 56055, Loss 0.3120182156562805\n",
            "Step 56060, Loss 0.4139120876789093\n",
            "Step 56065, Loss 0.3525751829147339\n",
            "Step 56070, Loss 0.317130446434021\n",
            "Step 56075, Loss 0.4074242413043976\n",
            "Step 56080, Loss 0.34992334246635437\n",
            "Step 56085, Loss 0.34421631693840027\n",
            "Step 56090, Loss 0.4646511375904083\n",
            "Step 56095, Loss 0.3442049026489258\n",
            "Step 56100, Loss 0.4007710814476013\n",
            "Step 56105, Loss 0.27078476548194885\n",
            "Step 56110, Loss 0.33367013931274414\n",
            "Step 56115, Loss 0.3055857717990875\n",
            "Step 56120, Loss 0.48134568333625793\n",
            "Step 56125, Loss 0.329034686088562\n",
            "Step 56130, Loss 0.4050528407096863\n",
            "Step 56135, Loss 0.4087534248828888\n",
            "Step 56140, Loss 0.41702526807785034\n",
            "Step 56145, Loss 0.24761177599430084\n",
            "Step 56150, Loss 0.343305379152298\n",
            "Step 56155, Loss 0.26062846183776855\n",
            "model not updated\n"
          ]
        }
      ],
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "train_losses = []\n",
        "last_accuracy = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    \n",
        "    aug_images = []\n",
        "\n",
        "    for image in images:\n",
        "      aug_image = aug_pipeline(image).to(DEVICE)\n",
        "      aug_images.append(aug_image) \n",
        "\n",
        "    aug_images = torch.stack(aug_images)\n",
        "\n",
        "    # Bring data over the device of choice\n",
        "    aug_images = aug_images.to(DEVICE)\n",
        "    \n",
        "\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(aug_images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  train_acc, train_loss, _ = evaluate(net, train_dataloader, last_accuracy, update=False, print_tqdm = False)\n",
        "  train_accuracies.append(train_acc)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "  val_acc, val_loss, last_accuracy = evaluate(net, val_dataloader, last_accuracy, update=True, print_tqdm = False)\n",
        "  val_accuracies.append(val_acc)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lqG2vNpvEiWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "a1d2cee3-b577-4158-b01e-0fe9bd45c0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracies on the validation set:  0.5838\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='epoch'>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Best accuracies on the validation set: \", val_acc)\n",
        "\n",
        "results_df = pd.DataFrame(zip(range(1,NUM_EPOCHS+1), train_accuracies, val_accuracies, train_losses, val_losses), columns = [\"epoch\", \"train_accuracy\", \"val_accuracy\", \"train_loss\", \"val_loss\"])\n",
        "results_df.set_index(\"epoch\")\n",
        "results_df.plot(x = \"epoch\", y = [\"train_accuracy\", \"val_accuracy\"])\n",
        "results_df.plot(x = \"epoch\", y = [\"train_loss\", \"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fSHcUqLB5yWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc10700-b95c-4f14-c1df-c6344d8e72e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 27.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "state_dict = torch.load(f\"{net.__class__.__name__}-gn-cifar100.pth\")\n",
        "net.load_state_dict(state_dict)\n",
        "accuracy = evaluateTest(net, test_dataloader)[0]\n",
        "print('\\nTest Accuracy: {}'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}